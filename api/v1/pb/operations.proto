syntax = "proto3";
package geocube;
option go_package = "./pb;geocube";

import "google/protobuf/timestamp.proto";

import "pb/dataformat.proto";
import "pb/variables.proto";
import "pb/records.proto";

/**
  * Storage class of a container. Depends on the storage
  */
enum StorageClass{
    STANDARD    = 0;
    INFREQUENT  = 1;
    ARCHIVE     = 2;
    DEEPARCHIVE = 3;
}

/**
  * Execution level of a job
  */
enum ExecutionLevel{
    ExecutionSynchronous  = 0; // Job is done synchronously
    ExecutionAsynchronous = 1; // Job is done asynchronously, but without any pause
    StepByStepCritical    = 2; // Job is done asynchronously, step-by-step, pausing at every critical steps
    StepByStepMajor       = 3; // Job is done asynchronously, step-by-step, pausing at every major steps
    StepByStepAll         = 4; // Job is done asynchronously, step-by-step, pausing at every steps
}

/**
  * Define a dataset. A dataset is the metadata to retrieve an image from a file.
  * It is defined by a record and the instance of a variable.
  * 
  * A dataset defines:
  * - Which band(s) are indexed (usually all the bands, but it can be a subset)
  * - How to map the value of its pixels to the dataformat of the variable. In more details:
  *    . the dataformat of the dataset (dformat.[no_data, min, max]) that describes the pixel of the image
  *    . the mapping from each pixel to the data format of the variable (variable.dformat). This mapping is defined as [MinOut, MaxOut, Exponent]. 
  */
message Dataset {
    string         record_id        = 2;
    string         instance_id      = 3;
    string         container_subdir = 6;
    repeated int64 bands            = 7;
    DataFormat     dformat          = 8;  // Internal data format (DType can be Undefined)
    double         real_min_value   = 9;  // Real min value (dformat.min_value maps to real_min_value)
    double         real_max_value   = 10; // Real max value (dformat.max_value maps to real_max_value)
    double         exponent         = 11; // 1: linear scaling (RealMax - RealMin) * pow( (Value - Min) / (Max - Min), Exponent) + RealMin
}

/**
  * Define a container of datasets.
  * Usually a container is a file containing one dataset.
  * But after a consolidation or if the container has several bands, it can contain several datasets.
  */
message Container{
    string           uri           = 1; // URI of the file
    bool             managed       = 2; // True if the Geocube is responsible for the lifecycle of this file
    repeated Dataset datasets      = 3; // List of datasets of the container
}

/**
  * Job to modify datasets (consolidation, deletion, ingestion...)
  * Some lifecycle operations are required to be done cautiously, in order to garantee the integrity of the database.
  * Such operations are defined by a job and are done asynchronously.
  * A job is a state-machine that can be rollbacked anytime during the operation until it ends.
  */
message Job {
    string                    id               = 1;  // Id of the job
    string                    name             = 2;  // Name of the job (must be unique)
    string                    type             = 3;  // Type of the job (consolidation, deletion...)
    string                    state            = 4;  // Current state of the state machine
    google.protobuf.Timestamp creation_time    = 5;  // Time of creation of the job
    google.protobuf.Timestamp last_update_time = 6;  // Time of the last update
    repeated string           logs             = 7;  // Job logs: if logs are too big to fit in a grpc response, logs will only be a subset (by default, the latest)
    int32                     active_tasks     = 8;  // If the job is divided into sub tasks, number of pending tasks
    int32                     failed_tasks     = 9;  // If the job is divided into sub tasks, number of failed tasks
    ExecutionLevel            execution_level  = 10; // Execution level of a job (see ExecutionLevel)
    bool                      waiting          = 11; // If true, the job is waiting for user to continue
}


/**
  * Request info on containers
  */
  message GetContainersRequest {
    repeated string uris = 1;  // List of container uris
}

/**
  * 
  */
message GetContainersResponse {
  repeated Container containers = 1;
}

/**
  * Request to index all the datasets of a container
  */
message IndexDatasetsRequest {
    // TODO Index several containers: repeated ?
    Container container = 1;
}

/**
  * 
  */
message IndexDatasetsResponse {}

/**
  * Parameters of consolidation that are linked to a variable, to define:
  * - how to resample the data during consolidation
  * - how to store the data:
  *   - Compression
  *   - CreationParams (supported: see GDAL drivers: PHOTOMETRIC, COMPRESS, PREDICTOR, ZLEVEL, ZSTDLEVEL, MAX_Z_ERROR, JPEGTABLESMODE and with _OVERVIEW suffix if exists)
  */
message ConsolidationParams{
    enum Compression{
        NO       = 0;
        LOSSLESS = 1;
        LOSSY    = 2;
        CUSTOM   = 3; // configured by creation_params
    }

    DataFormat          dformat          = 1; // dataformat of the data. See exponent for the mapping formula.
    double              exponent         = 2; // 1: linear scaling (RealMax - RealMin) * pow( (Value - Min) / (Max - Min), Exponent) + RealMin
    bool                create_overviews = 3 [deprecated = true]; // Use Layout.overviews_min_size instead
    Resampling          resampling_alg   = 4; // Define how to resample the data during the consolidation (if a reprojection is needed or if the overviews are created)
    Compression         compression      = 5; // Define how the data is compressed at block level
    map<string, string> creation_params  = 8; // map of params:value to configure the creation of the file. See Compression to list the supported params
    bool                bands_interleave = 6 [deprecated = true]; // If the variable is multibands, define whether the bands are interleaved. Use Layout.interlacing_pattern instead
    StorageClass        storage_class    = 7; // Define the storage class of the created file (support only GCS)
}

/**
  * Configure the parameters of the consolidation attached to the variable
  */
message ConfigConsolidationRequest {
    string              variable_id          = 1;
    ConsolidationParams consolidation_params = 2;
}

/**
  * 
  */
message ConfigConsolidationResponse {
}

/**
  * Retrieve the consolidation parameters attached to the given variable
  */
message GetConsolidationParamsRequest {
    string              variable_id          = 1;
}

/**
  * Return consolidation parameters
  */
message GetConsolidationParamsResponse {
    ConsolidationParams consolidation_params = 2;
}

/**
  * Create and start a consolidation job given a list of records and an instance_id to be consolidated on a layout
  * Optionnaly, the job can be done step by step, pausing and waiting for user action, with three levels:
  * - 1: after each critical steps
  * - 2: after each major steps
  * - 3: after all steps
  */
message ConsolidateRequest {
    string         job_name              = 1;
    string         instance_id           = 2;
    string         layout_name           = 7;
    ExecutionLevel execution_level       = 6; // Execution level of a job. A consolidation job cannot be executed synchronously
    string         collapse_on_record_id = 9; // [Optional] Collapse all records on this record (in this case only, original datasets are kept, data is duplicated)

    oneof records_lister{
        RecordIdList  records = 8; // At least one
        RecordFilters filters = 5;
    }
}

/**
  * Return the id of the job created
  */
message ConsolidateResponse{
    string job_id = 1;
}

/**
  * List jobs given a name pattern
  */
message ListJobsRequest{
    string name_like = 1;
    int32  page      = 2;
    int32  limit     = 3;
}

/**
  * Return a list of the job whose name matchs the pattern
  */
message ListJobsResponse {
    repeated Job jobs = 1;
}

/**
  * Retrieve a job given its id
  */
message GetJobRequest{
    string id        = 1;
    int32  log_page  = 2;
    int32  log_limit = 3;
}

/**
  * Return a job with the requested id
  */
message GetJobResponse {
    Job job = 1;
}

/*
 * Clean terminated jobs
 */
message CleanJobsRequest{
    string name_like = 1; // Filter by name (support *, ? and (?i)-suffix for case-insensitivity)
    string state     = 2; // Filter by terminated state (DONE, FAILED)
}

/**
  * Return the number of jobs that have been deleted 
  */
message CleanJobsResponse{
    int32 count = 1;
}

/**
  * Cancel a job (e.g. during consolidation)
  */
message CancelJobRequest {
    string id = 1;
    bool   force_any_state = 2; // Force cancel even when the job is not in a failed state or consolidation step (could corrupt the data)
}

/**
  * 
  */
message CancelJobResponse{
}

/**
  * Retry a job that failed or is stuck (e.g. during consolidation)
  */
message RetryJobRequest {
    string id              = 1;
    bool   force_any_state = 2; // Force retry even when the job is not in a failed state (could corrupt the data)
}

/**
  * 
  */
message RetryJobResponse{
}

/**
  * Proceed the next step of a step-by-step job
  */
message ContinueJobRequest {
    string id = 1;
}

/**
  * 
  */
message ContinueJobResponse{
}

/**
  * Remove the datasets referenced by instances and records without any control
  * The containers (if empty) are not deleted
  */
  message DeleteDatasetsRequest{
    repeated string record_ids       = 2; // Instance id that references the datasets to be deleted
    repeated string instance_ids     = 3; // Record ids that reference the datasets to be deleted
    repeated string dataset_patterns = 6; // Dataset file patterns (support * and ? for all or any characters and trailing (?i) for case-insensitiveness) (or empty to ignore)
    ExecutionLevel  execution_level  = 4; // Execution level (see enum)
    string          job_name         = 5; // Name of the job (if empty, a name will be generated)
}

/**
  * Return the number of deleted datasets
  */
message DeleteDatasetsResponse{
    Job job = 2;
}

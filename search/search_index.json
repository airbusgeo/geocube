{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Geocube-Server Geocube is a geo-spatial imagery database designed as a serverless service to serve processing pipelines of satellite images at large scale. The GeoCube is a multi-sensor spatio-temporal database, fully cloud-native, designed to provide a storage infrastructure that is scalable, distributed, able to ingest pre-processed information from a variety of earth-observation systems or external data, and to deliver consolidated, fully aligned time series at pixel or tile scale. In order to optimize the access time to information stored into the tiles data chunks, a new format \u2018Multi-dataset Cloud Optimized GeoTiff\u2019 (MUCOG), based on the well-known COG and GeoTIFF, has been defined. The \u2018Consolidation\u2019 operation, which consists into reorganizing ingested data for optimal storage and access, is manually triggered depending on the needs of the project. When retrieving the data, the GeoCube provides convenient on-the-fly functions such as automatic resampling, reprojection, mosaicking from the individual tiles and data type change and rescaling. The GeoCube is designed to answer multiple needs, from global applications to more regional/local services, and is basically sensor agnostic. Credits Geocube is a project under development by Airbus DS Geo SA with the support of CNES . Geocube is licensed under the Apache License, Version 2.0. See LICENSE for the full license text.","title":"Home"},{"location":"#welcome-to-geocube-server","text":"Geocube is a geo-spatial imagery database designed as a serverless service to serve processing pipelines of satellite images at large scale. The GeoCube is a multi-sensor spatio-temporal database, fully cloud-native, designed to provide a storage infrastructure that is scalable, distributed, able to ingest pre-processed information from a variety of earth-observation systems or external data, and to deliver consolidated, fully aligned time series at pixel or tile scale. In order to optimize the access time to information stored into the tiles data chunks, a new format \u2018Multi-dataset Cloud Optimized GeoTiff\u2019 (MUCOG), based on the well-known COG and GeoTIFF, has been defined. The \u2018Consolidation\u2019 operation, which consists into reorganizing ingested data for optimal storage and access, is manually triggered depending on the needs of the project. When retrieving the data, the GeoCube provides convenient on-the-fly functions such as automatic resampling, reprojection, mosaicking from the individual tiles and data type change and rescaling. The GeoCube is designed to answer multiple needs, from global applications to more regional/local services, and is basically sensor agnostic.","title":"Welcome to Geocube-Server"},{"location":"#credits","text":"Geocube is a project under development by Airbus DS Geo SA with the support of CNES . Geocube is licensed under the Apache License, Version 2.0. See LICENSE for the full license text.","title":"Credits"},{"location":"quickstart/","text":"Getting started This quickstart requires docker-compose and python3.8 . Clone https://github.com/airbusgeo/geocube.git . Start geocube service with docker-compose, following these instructions . Install geocube-client-python in your prefered environment : pip install git+https://github.com/airbusgeo/geocube-client-python.git Start a python console and type import geocube client = geocube.Client(\"127.0.0.1:8080\") You are connected ! Then, you can do the tutorials to learn how to feed the geocube, access and optimize the data.","title":"Getting started"},{"location":"quickstart/#getting-started","text":"This quickstart requires docker-compose and python3.8 . Clone https://github.com/airbusgeo/geocube.git . Start geocube service with docker-compose, following these instructions . Install geocube-client-python in your prefered environment : pip install git+https://github.com/airbusgeo/geocube-client-python.git Start a python console and type import geocube client = geocube.Client(\"127.0.0.1:8080\") You are connected ! Then, you can do the tutorials to learn how to feed the geocube, access and optimize the data.","title":"Getting started"},{"location":"about/CONTRIBUTING/","text":"How to contribute Thank you for stopping by. Please read these few small guidelines before creating an issue or a pull request. Reporting issues Bugs, feature requests, and development-related questions should be directed to our GitHub issue tracker . If reporting a bug, please try and provide as much context as possible and a reproducible test case. Submitting a patch Patches are to be submitted through pull-requests: https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request Make sure each group of changes be done in distinct branches in order to ensure that a pull request only includes code related to that bug or feature. Always run go fmt on your code before committing it. Do not squash / force-push your commits inside the pull request branch as these tend to mess up the review comments.","title":"Contributing"},{"location":"about/CONTRIBUTING/#how-to-contribute","text":"Thank you for stopping by. Please read these few small guidelines before creating an issue or a pull request.","title":"How to contribute"},{"location":"about/CONTRIBUTING/#reporting-issues","text":"Bugs, feature requests, and development-related questions should be directed to our GitHub issue tracker . If reporting a bug, please try and provide as much context as possible and a reproducible test case.","title":"Reporting issues"},{"location":"about/CONTRIBUTING/#submitting-a-patch","text":"Patches are to be submitted through pull-requests: https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request Make sure each group of changes be done in distinct branches in order to ensure that a pull request only includes code related to that bug or feature. Always run go fmt on your code before committing it. Do not squash / force-push your commits inside the pull request branch as these tend to mess up the review comments.","title":"Submitting a patch"},{"location":"about/help/","text":"Do not hesitate to ask for help or report issues .","title":"Getting help"},{"location":"about/license/","text":"Geocube is licensed under the Apache License, Version 2.0. See LICENSE for the full license text.","title":"License"},{"location":"about/release-notes/","text":"Release notes 1.1.0 Functionalities Consolidater: add --local-download-max-mb to limit the size of the files downloaded by the consolidater (--local-download is deprecated) API Bug fixes Others optimization GetCube: reduce memory usage using StreamableBitmap 1.0.3 Functionalities apiserver/downloader/consolidater: add --gdalNumThreads to change the -wo options of gdal.warp. 1 by default, -1 means ALL_CPUS. gdalNumThreads+workers should be lower than the number of CPUS. downloader: add --chunkSize (1Mbytes by default) gdalwarp uses wm=500 instead of 2047 and -multi option Min/Max to GetXYZTile to scale tile values between min and max. Add index on pg.records.datetime (execute interface/database/pg/update_X.X.X.sql) GCS: automatically retry or mark as temporary some errors API DeleteRecords: add NoFail to delete all the pending records and let the others GetCube: add CompressionLevel=-3 to disable the compression GetCube: add Predownload option to download file before warping to save time. It is efficient when gdal needs the whole image to compute the Cube requested, but its not when a small part of the image is required. Be careful when the data has been consolidated. Consolidation: add collapse_on_record_id: to consolidate by collapsing all datasets on the given record (data is copied) Get information on containers from their uris FindJobs: add page/limit GetXYZTile support filters: records.tags, records.from_date and records.to_date (e.g. ?filters.from_time=YYYY-MM-DD&filters.to_time=YYYY-MM-DD&filters.tags[key1]=value1&filter.tags[key2]=value2...) Records: ? and * are not supported anymore for the record tags Support Int8 datatype DeleteDatasets is moved from Admin to Client (retrocompatibility is ensured for previous clients) ListDatasets(records, instance): to list the datasets indexed with the given records and instances Bug fixes CleanJobs: remove DONEBUTUNTIDY Remove redondant logs maxConnectionAge: default = 15min storage: operations retry when context is cancelled: Add utils.Retriable to test weither an error is retriable. Panic during dataset deletion when status is DeletionNotReady Consolidation used GTIFF_SUBDIR when there was no subdir Consolidation failed because of invalid geometry in ComputeValidShapeFromCell Update postGis to 3.1 to fix a bug with geography intersection (GetCube does not return all datasets) Docker Consolidater use uuidgen instead of ossp-uuid slow FindRecords float32 is compressed with ZSTD instead of LERC_ZSTD Container deletion ignores FileNotFound error Handling of extents that crosses dateline Handling of extents wider than 180\u00b0 of longitude reindex dataset crossing antemeridian Not empty image are returned as empty Consolidation: BuildOverviews fails if GDAL raises a warning Consolidation: NoData=Nan does not work as expected generate with enumer AdminUpdateDataset with RecordIds Better handling of consolidation cancellation CleanJobs does not return all errors Index a dataset with the status INACTIVE or TO_DELETE If nodata != none and lossy compression: use alpha band Others Update golang-mod Use google-cloud-go instead of go-genproto package Memory optimisation GetCube & Consolidater: optimisation of the speed (x3 on large datasets) Refacto MergeDataset, using vrt. Dockerfile uses alpine3.21, golang:alpine3.21 1.0.2 Functionalities Bug fixes countValidPix with gdal >= 3.6.0 Deprecated api cloud.google.com/go/secretmanager/apiv1beta1 => cloud.google.com/go/secretmanager/apiv1 1.0.1 Functionalities Consolidater: add option --local-download (default= true ) to download datasets locally before starting the consolidation. Usually, it's faster to download first, but in some case, it's not (or consume a lot of local storage) API: add GetRecords(List IDs) API: GetCube: add ResamplingAlg (override variable.ResamplingAlg) Bug fixes Cancel consolidation tasks took to much time (due to job being saved at every task) Update mod airbusgeo/cogger to fix a crash with overviews If a deletion task failed, the job must be in \"DONEBUTUNTIDY\" state Dockerfile uses alpine3.17, golang:alpine3.17 GRPC message errors are limited to 3Kb Dataset bands were not taken into account during warping","title":"Release Notes"},{"location":"about/release-notes/#release-notes","text":"","title":"Release notes"},{"location":"about/release-notes/#110","text":"","title":"1.1.0"},{"location":"about/release-notes/#functionalities","text":"Consolidater: add --local-download-max-mb to limit the size of the files downloaded by the consolidater (--local-download is deprecated)","title":"Functionalities"},{"location":"about/release-notes/#api","text":"","title":"API"},{"location":"about/release-notes/#bug-fixes","text":"","title":"Bug fixes"},{"location":"about/release-notes/#others","text":"optimization GetCube: reduce memory usage using StreamableBitmap","title":"Others"},{"location":"about/release-notes/#103","text":"","title":"1.0.3"},{"location":"about/release-notes/#functionalities_1","text":"apiserver/downloader/consolidater: add --gdalNumThreads to change the -wo options of gdal.warp. 1 by default, -1 means ALL_CPUS. gdalNumThreads+workers should be lower than the number of CPUS. downloader: add --chunkSize (1Mbytes by default) gdalwarp uses wm=500 instead of 2047 and -multi option Min/Max to GetXYZTile to scale tile values between min and max. Add index on pg.records.datetime (execute interface/database/pg/update_X.X.X.sql) GCS: automatically retry or mark as temporary some errors","title":"Functionalities"},{"location":"about/release-notes/#api_1","text":"DeleteRecords: add NoFail to delete all the pending records and let the others GetCube: add CompressionLevel=-3 to disable the compression GetCube: add Predownload option to download file before warping to save time. It is efficient when gdal needs the whole image to compute the Cube requested, but its not when a small part of the image is required. Be careful when the data has been consolidated. Consolidation: add collapse_on_record_id: to consolidate by collapsing all datasets on the given record (data is copied) Get information on containers from their uris FindJobs: add page/limit GetXYZTile support filters: records.tags, records.from_date and records.to_date (e.g. ?filters.from_time=YYYY-MM-DD&filters.to_time=YYYY-MM-DD&filters.tags[key1]=value1&filter.tags[key2]=value2...) Records: ? and * are not supported anymore for the record tags Support Int8 datatype DeleteDatasets is moved from Admin to Client (retrocompatibility is ensured for previous clients) ListDatasets(records, instance): to list the datasets indexed with the given records and instances","title":"API"},{"location":"about/release-notes/#bug-fixes_1","text":"CleanJobs: remove DONEBUTUNTIDY Remove redondant logs maxConnectionAge: default = 15min storage: operations retry when context is cancelled: Add utils.Retriable to test weither an error is retriable. Panic during dataset deletion when status is DeletionNotReady Consolidation used GTIFF_SUBDIR when there was no subdir Consolidation failed because of invalid geometry in ComputeValidShapeFromCell Update postGis to 3.1 to fix a bug with geography intersection (GetCube does not return all datasets) Docker Consolidater use uuidgen instead of ossp-uuid slow FindRecords float32 is compressed with ZSTD instead of LERC_ZSTD Container deletion ignores FileNotFound error Handling of extents that crosses dateline Handling of extents wider than 180\u00b0 of longitude reindex dataset crossing antemeridian Not empty image are returned as empty Consolidation: BuildOverviews fails if GDAL raises a warning Consolidation: NoData=Nan does not work as expected generate with enumer AdminUpdateDataset with RecordIds Better handling of consolidation cancellation CleanJobs does not return all errors Index a dataset with the status INACTIVE or TO_DELETE If nodata != none and lossy compression: use alpha band","title":"Bug fixes"},{"location":"about/release-notes/#others_1","text":"Update golang-mod Use google-cloud-go instead of go-genproto package Memory optimisation GetCube & Consolidater: optimisation of the speed (x3 on large datasets) Refacto MergeDataset, using vrt. Dockerfile uses alpine3.21, golang:alpine3.21","title":"Others"},{"location":"about/release-notes/#102","text":"","title":"1.0.2"},{"location":"about/release-notes/#functionalities_2","text":"","title":"Functionalities"},{"location":"about/release-notes/#bug-fixes_2","text":"countValidPix with gdal >= 3.6.0 Deprecated api cloud.google.com/go/secretmanager/apiv1beta1 => cloud.google.com/go/secretmanager/apiv1","title":"Bug fixes"},{"location":"about/release-notes/#101","text":"","title":"1.0.1"},{"location":"about/release-notes/#functionalities_3","text":"Consolidater: add option --local-download (default= true ) to download datasets locally before starting the consolidation. Usually, it's faster to download first, but in some case, it's not (or consume a lot of local storage) API: add GetRecords(List IDs) API: GetCube: add ResamplingAlg (override variable.ResamplingAlg)","title":"Functionalities"},{"location":"about/release-notes/#bug-fixes_3","text":"Cancel consolidation tasks took to much time (due to job being saved at every task) Update mod airbusgeo/cogger to fix a crash with overviews If a deletion task failed, the job must be in \"DONEBUTUNTIDY\" state Dockerfile uses alpine3.17, golang:alpine3.17 GRPC message errors are limited to 3Kb Dataset bands were not taken into account during warping","title":"Bug fixes"},{"location":"architecture/interfaces/","text":"Interfaces Geocube is designed to be customizable and deployable in other environments. Several interfaces are declared and can be implemented to use external tools or services. All the interfaces are declared in the interface folder of the repository. After implementing an interface, it may be necessary to customize the cmd/[...]/main.go files to use the new interface. If you implement a new interface, it will be very welcome to submit a contribution . Storage Interface The interface storage is used to read and write the images that are indexed in the Geocube. It must be accessible in reading by range-request and should be accessible in writing to support the consolidation (optimisation of the data). The interface is available in interface/storage package. To add a storage strategy, the following methods are to be implemented: // Download file content as a slice of byte Download(ctx context.Context, uri string, options ...Option) ([]byte, error) // DownloadTo a local file DownloadTo(ctx context.Context, source string, destination string, options ...Option) error // Upload file content into remote file Upload(ctx context.Context, uri string, data []byte, options ...Option) error // UploadFile into remote file UploadFile(ctx context.Context, uri string, data io.ReadCloser, options ...Option) error // Delete file Delete(ctx context.Context, uri string, options ...Option) error // Exist checks if file exist Exist(ctx context.Context, uri string) (bool, error) // GetAttrs returns file attribute GetAttrs(ctx context.Context, uri string) (Attrs, error) The storage is infered from the prefix of the uri (protocol). The user can add an additionnal storage by implementing the interface and adding it in the interface/storage/uri/ package. Currently supported storages Currently, the geocube code supports three storage systems: AWS-S3, GCS and filesystem. Messaging Interface The messaging interface is available here : interface/messaging/ . It is used to communicate between the ApiServer and the Consolidater, and it can be used as a metric by the Autoscaler to autoscale the ressources for the consolidater. It's a parameter of the constructor of the Service Class and it is configured in the following files: cmd/apiserver/main.go and cmd/consolidater/main.go . Pgqueue implementation A messaging interface based on postgres is implemented using the btubbs/pgq library: interface/messaging/pgqueue . This implementation has autoscaling capabilities. Pubsub implementation Geocube supports PubSub (Google Cloud Platform) messaging broker : interface/messaging/pubsub . Topics and subscriptions are to be created. Topics: - events - events-failed - consolidations - consolidations-failed - consolidations-worker (only if autoscaler is used) Subscriptions: - events - consolidations - consolidations-worker (only if autoscaler is used) These actions could be performed manually or with terraform. For more information, see: https://cloud.google.com/pubsub/docs/overview. You must have the Pub/Sub Admin role on your service account. NB: Topics & Subscriptions must be created before running services. A Pub/Sub emulator is available to use PubSub in a local system (with limited capacities). Please follow the documentation to install and start the emulator. Database Interface The database interface is available here : interface/database/db.go . It is used by the ApiServer as a parameter of the constructor of the service and it is configured in the following file: cmd/apiserver/main.go . PostgreSQL Implementation Geocube currently supports a Postgresql database with the PostGIS extension ( interface/database/pg/ ). Create a database and run the installation SQL script in order to create all tables, schemas and roles. This script is available in Geocube code source in interface/database/pg/create.sql $ psql -h <database_host> -d <database_name> -f interface/database/pg/create.sql For ugrade, see Update PostgreSQL database Autoscaler The autoscaler handles the scale-up or down of the consolidator service. It\u2019s an external service and does not have an interface. The current implementation, using Kubernetes, is available here : interface/autoscaler/ and it is used in the Autoscaler service : cmd/autoscaler/main.go GRPC Clients connect to the Geocube with a GRPC interface. This interface is automatically generated from the protobuf files in api/v1/pb . From the Geocube side, protofiles are generated in the internal/pb folder. A GRPC server is then implemented in internal/grpc to interface the GRPC layer to the Geocube server. From the client side, protofiles must be generated using the same files. It automatically creates a GRPC interface to connect to the Geocube. See Client Python or Client Go for example. Install protoc go get -u github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-grpc-gateway go get -u github.com/golang/protobuf/protoc-gen-go Generate output from protofiles : go generate generate.go","title":"Interfaces"},{"location":"architecture/interfaces/#interfaces","text":"Geocube is designed to be customizable and deployable in other environments. Several interfaces are declared and can be implemented to use external tools or services. All the interfaces are declared in the interface folder of the repository. After implementing an interface, it may be necessary to customize the cmd/[...]/main.go files to use the new interface. If you implement a new interface, it will be very welcome to submit a contribution .","title":"Interfaces"},{"location":"architecture/interfaces/#storage","text":"","title":"Storage"},{"location":"architecture/interfaces/#interface","text":"The interface storage is used to read and write the images that are indexed in the Geocube. It must be accessible in reading by range-request and should be accessible in writing to support the consolidation (optimisation of the data). The interface is available in interface/storage package. To add a storage strategy, the following methods are to be implemented: // Download file content as a slice of byte Download(ctx context.Context, uri string, options ...Option) ([]byte, error) // DownloadTo a local file DownloadTo(ctx context.Context, source string, destination string, options ...Option) error // Upload file content into remote file Upload(ctx context.Context, uri string, data []byte, options ...Option) error // UploadFile into remote file UploadFile(ctx context.Context, uri string, data io.ReadCloser, options ...Option) error // Delete file Delete(ctx context.Context, uri string, options ...Option) error // Exist checks if file exist Exist(ctx context.Context, uri string) (bool, error) // GetAttrs returns file attribute GetAttrs(ctx context.Context, uri string) (Attrs, error) The storage is infered from the prefix of the uri (protocol). The user can add an additionnal storage by implementing the interface and adding it in the interface/storage/uri/ package.","title":"Interface"},{"location":"architecture/interfaces/#currently-supported-storages","text":"Currently, the geocube code supports three storage systems: AWS-S3, GCS and filesystem.","title":"Currently supported storages"},{"location":"architecture/interfaces/#messaging","text":"","title":"Messaging"},{"location":"architecture/interfaces/#interface_1","text":"The messaging interface is available here : interface/messaging/ . It is used to communicate between the ApiServer and the Consolidater, and it can be used as a metric by the Autoscaler to autoscale the ressources for the consolidater. It's a parameter of the constructor of the Service Class and it is configured in the following files: cmd/apiserver/main.go and cmd/consolidater/main.go .","title":"Interface"},{"location":"architecture/interfaces/#pgqueue-implementation","text":"A messaging interface based on postgres is implemented using the btubbs/pgq library: interface/messaging/pgqueue . This implementation has autoscaling capabilities.","title":"Pgqueue implementation"},{"location":"architecture/interfaces/#pubsub-implementation","text":"Geocube supports PubSub (Google Cloud Platform) messaging broker : interface/messaging/pubsub . Topics and subscriptions are to be created. Topics: - events - events-failed - consolidations - consolidations-failed - consolidations-worker (only if autoscaler is used) Subscriptions: - events - consolidations - consolidations-worker (only if autoscaler is used) These actions could be performed manually or with terraform. For more information, see: https://cloud.google.com/pubsub/docs/overview. You must have the Pub/Sub Admin role on your service account. NB: Topics & Subscriptions must be created before running services. A Pub/Sub emulator is available to use PubSub in a local system (with limited capacities). Please follow the documentation to install and start the emulator.","title":"Pubsub implementation"},{"location":"architecture/interfaces/#database","text":"","title":"Database"},{"location":"architecture/interfaces/#interface_2","text":"The database interface is available here : interface/database/db.go . It is used by the ApiServer as a parameter of the constructor of the service and it is configured in the following file: cmd/apiserver/main.go .","title":"Interface"},{"location":"architecture/interfaces/#postgresql-implementation","text":"Geocube currently supports a Postgresql database with the PostGIS extension ( interface/database/pg/ ). Create a database and run the installation SQL script in order to create all tables, schemas and roles. This script is available in Geocube code source in interface/database/pg/create.sql $ psql -h <database_host> -d <database_name> -f interface/database/pg/create.sql For ugrade, see Update PostgreSQL database","title":"PostgreSQL Implementation"},{"location":"architecture/interfaces/#autoscaler","text":"The autoscaler handles the scale-up or down of the consolidator service. It\u2019s an external service and does not have an interface. The current implementation, using Kubernetes, is available here : interface/autoscaler/ and it is used in the Autoscaler service : cmd/autoscaler/main.go","title":"Autoscaler"},{"location":"architecture/interfaces/#grpc","text":"Clients connect to the Geocube with a GRPC interface. This interface is automatically generated from the protobuf files in api/v1/pb . From the Geocube side, protofiles are generated in the internal/pb folder. A GRPC server is then implemented in internal/grpc to interface the GRPC layer to the Geocube server. From the client side, protofiles must be generated using the same files. It automatically creates a GRPC interface to connect to the Geocube. See Client Python or Client Go for example. Install protoc go get -u github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-grpc-gateway go get -u github.com/golang/protobuf/protoc-gen-go Generate output from protofiles : go generate generate.go","title":"GRPC"},{"location":"architecture/mucog/","text":"Multidataset COG (MuCOG) The Geocube can refactor the data to store several images in an optimized version of the COG. The COG is already an optimized Geotiff for the cloud, thanks to its internal organization of the file. The data is split into tiles and the index are grouped together. The Multidataset COG, the MuCOG, goes one step further. It interlaces the full resolution blocks of several cogs, enabling a fast access to the whole timeserie of a given block. As in the COG, this interlacing preserves sparsity. Overviews can be stored spatially contiguously or interlaced, depending on the use case. This MuCOG respects the GeoTIFF standard that makes it readable by GDAL. Performance The MuCOG is not mandatory, but it gives very good performance when it comes to reading a timeseries on a cloud storage. The MuCOG minimizes the latency, by drastically decreasing the number of calls to the storage, it reduces bandwidth usage, by decreasing the volume of data read and it results in a reduction of costs, with a lower computing requirement. With a MuCOG, you can retrieve a block of one hundred images in less than a second, which is 10 to 20 times faster than with COGs. Read a timeseries on a cloud storage \u2013 100 datasets, 256x256 pixels Test condition : Virtual machine on GCP (16CPU/64Gb RAM) Storage on GCS Request of 100 blocks (256px*256px) Consolidation Consolidation is the process (called job ) consisting in translating, warping, splitting, merging multiple files into COGs or MuCOGs, given a grid layout. It's a long process (from a couple of minutes to an hour) that requires a rigorous management of the life-cycle of the files. It can fail, be restarted or cancelled. The consolidation steps are handled by a state machine and as far as possible, the states can be retried, cancelled or revert. More information on how to consolidate here . The API Server handles the life-cycle of the job but the creation of the MuCOGs is done by the Consolidation workers in parallel, autoscaled by the Autoscaler . The consolidation worker first creates a COG for each record from the images linked to the record being processed, then it creates a MuCOG, merging the COGs together in a unique file. At the end of the consolidation, original files are removed from the index and deleted by another state machine: More information on deletion here .","title":"MuCOG"},{"location":"architecture/mucog/#_1","text":"","title":""},{"location":"architecture/mucog/#multidataset-cog-mucog","text":"The Geocube can refactor the data to store several images in an optimized version of the COG. The COG is already an optimized Geotiff for the cloud, thanks to its internal organization of the file. The data is split into tiles and the index are grouped together. The Multidataset COG, the MuCOG, goes one step further. It interlaces the full resolution blocks of several cogs, enabling a fast access to the whole timeserie of a given block. As in the COG, this interlacing preserves sparsity. Overviews can be stored spatially contiguously or interlaced, depending on the use case. This MuCOG respects the GeoTIFF standard that makes it readable by GDAL.","title":"Multidataset COG (MuCOG)"},{"location":"architecture/mucog/#performance","text":"The MuCOG is not mandatory, but it gives very good performance when it comes to reading a timeseries on a cloud storage. The MuCOG minimizes the latency, by drastically decreasing the number of calls to the storage, it reduces bandwidth usage, by decreasing the volume of data read and it results in a reduction of costs, with a lower computing requirement. With a MuCOG, you can retrieve a block of one hundred images in less than a second, which is 10 to 20 times faster than with COGs. Read a timeseries on a cloud storage \u2013 100 datasets, 256x256 pixels Test condition : Virtual machine on GCP (16CPU/64Gb RAM) Storage on GCS Request of 100 blocks (256px*256px)","title":"Performance"},{"location":"architecture/mucog/#consolidation","text":"Consolidation is the process (called job ) consisting in translating, warping, splitting, merging multiple files into COGs or MuCOGs, given a grid layout. It's a long process (from a couple of minutes to an hour) that requires a rigorous management of the life-cycle of the files. It can fail, be restarted or cancelled. The consolidation steps are handled by a state machine and as far as possible, the states can be retried, cancelled or revert. More information on how to consolidate here . The API Server handles the life-cycle of the job but the creation of the MuCOGs is done by the Consolidation workers in parallel, autoscaled by the Autoscaler . The consolidation worker first creates a COG for each record from the images linked to the record being processed, then it creates a MuCOG, merging the COGs together in a unique file. At the end of the consolidation, original files are removed from the index and deleted by another state machine: More information on deletion here .","title":"Consolidation"},{"location":"architecture/services/","text":"Geocube services The Geocube Server is composed of the ApiServer, the Consolidater, the Autoscaler and an interface layer to integrate into the user environment. To integrate into the environment of deployment, the geocube has an interface layer. Some implementations of this layer are available and the user is free to implement others depending on its own environment. The interfaces are available in the interface folder. API Server The API server is the core of the Geocube. It is connected to a PostGIS database that contains the metadata on the indexed images and all the entities describing the images or the way to interact with them. API Server: is responsible for the life-cycle of all the entities (it's the only service that have access to the database) through (more or less) CRUDL operations. can be responsible for the life-cycle of the indexed images, through consolidation or deletion. gathers metadata required to reply to an image or timseries request. creates a cube of data, by reprojecting, rescaling, casting and mosaicing images stored in an object storage. delivers timeseries of data. Consolidater The consolidater service is called when a consolidation job is created to optimize the data. It takes several images, an extent and a layout and it creates a unique file (COG or MUCOG) that contains the data intersecting the extent. Autoscaler The autoscaler is a background service that can be used to automatically deploy consolidater workers when consolidation orders are emitted. Downloader Virtual machines on which the API server run are not necessary sized to process a large number of image requests. In that case, it's possible to create more machines or more powerful ones using an other autoscaling service (e.g. Cloud Run). Another way not to overload the API server is to request the metadatas that contain all that is necessary to create a cube of data (but not the data) and consume them in a dedicated service, called downloader , that is responsible for retrieving the data and create the cube. The retrieval is strictly identical ( downloader is actually a sub-service of API Server ), but it can be run on the machine of the client (provided it has the appropriate rights to get the data). Thus, in a large processing flow where many workers are deployed, geocube-client and downloader will be scaled up in the same way. See Geocube Python Client SDK1 for more information on the downloader usage.","title":"Geocube services"},{"location":"architecture/services/#geocube-services","text":"The Geocube Server is composed of the ApiServer, the Consolidater, the Autoscaler and an interface layer to integrate into the user environment. To integrate into the environment of deployment, the geocube has an interface layer. Some implementations of this layer are available and the user is free to implement others depending on its own environment. The interfaces are available in the interface folder.","title":"Geocube services"},{"location":"architecture/services/#api-server","text":"The API server is the core of the Geocube. It is connected to a PostGIS database that contains the metadata on the indexed images and all the entities describing the images or the way to interact with them. API Server: is responsible for the life-cycle of all the entities (it's the only service that have access to the database) through (more or less) CRUDL operations. can be responsible for the life-cycle of the indexed images, through consolidation or deletion. gathers metadata required to reply to an image or timseries request. creates a cube of data, by reprojecting, rescaling, casting and mosaicing images stored in an object storage. delivers timeseries of data.","title":"API Server"},{"location":"architecture/services/#consolidater","text":"The consolidater service is called when a consolidation job is created to optimize the data. It takes several images, an extent and a layout and it creates a unique file (COG or MUCOG) that contains the data intersecting the extent.","title":"Consolidater"},{"location":"architecture/services/#autoscaler","text":"The autoscaler is a background service that can be used to automatically deploy consolidater workers when consolidation orders are emitted.","title":"Autoscaler"},{"location":"architecture/services/#downloader","text":"Virtual machines on which the API server run are not necessary sized to process a large number of image requests. In that case, it's possible to create more machines or more powerful ones using an other autoscaling service (e.g. Cloud Run). Another way not to overload the API server is to request the metadatas that contain all that is necessary to create a cube of data (but not the data) and consume them in a dedicated service, called downloader , that is responsible for retrieving the data and create the cube. The retrieval is strictly identical ( downloader is actually a sub-service of API Server ), but it can be run on the machine of the client (provided it has the appropriate rights to get the data). Thus, in a large processing flow where many workers are deployed, geocube-client and downloader will be scaled up in the same way. See Geocube Python Client SDK1 for more information on the downloader usage.","title":"Downloader"},{"location":"architecture/solution/","text":"Geocube Ecosystem The Geocube is composed of three parts: Geocube Server : a scalable service using GDAL as image processing library to access and manage the geo data. Images are stored in an object storage, like GCS, AWS or local file system, and are indexed in a PostGIS Database. Geocube Ingester : a service responsible for massive ingestion of ARD images from any public, commercial or private catalogues. The ingesters are capable of preprocess images, using predefined or custom scripts and index them in the Geocube. An example of an ingester is available on Github . Geocube Client: an API to search images, optimize the database, ingest new images and deliver aligned timeseries for massive and parallel workflows in standard languages, such as Python or Golang and parallel computing libraries such as Dask.. A python client and a go client are available as example. Clients (for another language) can be created from the protobuf files using the automatically generated GRPC interface (see Tutorials ).","title":"Ecosystem"},{"location":"architecture/solution/#geocube-ecosystem","text":"The Geocube is composed of three parts: Geocube Server : a scalable service using GDAL as image processing library to access and manage the geo data. Images are stored in an object storage, like GCS, AWS or local file system, and are indexed in a PostGIS Database. Geocube Ingester : a service responsible for massive ingestion of ARD images from any public, commercial or private catalogues. The ingesters are capable of preprocess images, using predefined or custom scripts and index them in the Geocube. An example of an ingester is available on Github . Geocube Client: an API to search images, optimize the database, ingest new images and deliver aligned timeseries for massive and parallel workflows in standard languages, such as Python or Golang and parallel computing libraries such as Dask.. A python client and a go client are available as example. Clients (for another language) can be created from the protobuf files using the automatically generated GRPC interface (see Tutorials ).","title":"Geocube Ecosystem"},{"location":"installation/docker-install/","text":"Installation - Docker Clone repository Clone https://github.com/airbusgeo/geocube.git repository. All dockerfile are available in the docker folder. Base-image The Dockerfiles of the other services depend on a base-image : $ docker build -f docker/Dockerfile.base-image -t geocube-base-image . [...] Successfully built 62eb9e6d2c0e Server, Consolidater and Downloader Then, the BASE_IMAGE must be passed as a parameter in order to build server, consolidater or downloader dockerfile: $ docker build -f docker/Dockerfile.server -t geocube . --build-arg BASE_IMAGE=geocube-base-image $ docker build -f docker/Dockerfile.consolidater -t geocube-consolidater . --build-arg BASE_IMAGE=geocube-base-image $ docker build -f docker/Dockerfile.downloader -t geocube-downloader . --build-arg BASE_IMAGE=geocube-base-image You can run \u201cdocker run\u201d command in order to start the application. Run server - examples: export STORAGE=/geocube-datasets docker run --rm --network=host -e PUBSUB_EMULATOR_HOST=localhost:8085 -v $STORAGE:$STORAGE geocube -project geocube-emulator -ingestionStorage=$STORAGE -dbConnection=postgresql://user:password@localhost:5432/geocube -eventsQueue events -consolidationsQueue consolidations -cancelledJobs $STORAGE/cancelled-jobs export STORAGE=/geocube-datasets docker run --rm --network=host -e PUBSUB_EMULATOR_HOST=localhost:8085 -v $STORAGE:$STORAGE geocube-consolidater /consolidater -psProject geocube-emulator -workdir=/tmp -eventsQueue events -consolidationsQueue consolidations -cancelledJobs $STORAGE/cancelled-jobs With GCS support (authentication with application_default_credentials.json): export STORAGE=/geocube-datasets docker run --rm -v ~/.config/gcloud:/root/.config/gcloud geocube -with-gcs [...] For more information concerning running option, see: https://docs.docker.com/engine/reference/commandline/run/ Run downloader - examples: Basic example: docker run --rm -p 127.0.0.1:8081:8081/tcp geocube-downloader -port 8081 -workers 4 Example with GCS support: With GOOGLE_APPLICATION_CREDENTIALS: docker run --rm -e GOOGLE_APPLICATION_CREDENTIALS=/account/geocube_server.json -p 127.0.0.1:8081:8081/tcp --mount type=bind,src=~/Documents/account/geocube,dst=/account 65cddc550e9a geocube-downloader -port 8081 -with-gcs -workers 4 -gdalBlockSize 2Mb With application_default_credentials.json: docker run --rm -v ~/.config/gcloud:/root/.config/gcloud -p 127.0.0.1:8081:8081/tcp geocube-downloader -port 8081 -with-gcs -workers 4 -gdalBlockSize 2Mb Additionnal information here Messaging Broker cf Local environment - Messaging Broker Docker-compose A docker-compose file is provided as example. It's a minimal example, so feel free to edit it to take advantage of the full power of the Geocube. Copy the ./docker/.env.example to ./docker/.env Edit ./docker/.env to set the STORAGE_URI (it will be mount as a volume to access and store images). Build the base image cd docker and docker-compose up","title":"Docker"},{"location":"installation/docker-install/#installation-docker","text":"","title":"Installation - Docker"},{"location":"installation/docker-install/#clone-repository","text":"Clone https://github.com/airbusgeo/geocube.git repository. All dockerfile are available in the docker folder.","title":"Clone repository"},{"location":"installation/docker-install/#base-image","text":"The Dockerfiles of the other services depend on a base-image : $ docker build -f docker/Dockerfile.base-image -t geocube-base-image . [...] Successfully built 62eb9e6d2c0e","title":"Base-image"},{"location":"installation/docker-install/#server-consolidater-and-downloader","text":"Then, the BASE_IMAGE must be passed as a parameter in order to build server, consolidater or downloader dockerfile: $ docker build -f docker/Dockerfile.server -t geocube . --build-arg BASE_IMAGE=geocube-base-image $ docker build -f docker/Dockerfile.consolidater -t geocube-consolidater . --build-arg BASE_IMAGE=geocube-base-image $ docker build -f docker/Dockerfile.downloader -t geocube-downloader . --build-arg BASE_IMAGE=geocube-base-image You can run \u201cdocker run\u201d command in order to start the application.","title":"Server, Consolidater and Downloader"},{"location":"installation/docker-install/#run-server-examples","text":"export STORAGE=/geocube-datasets docker run --rm --network=host -e PUBSUB_EMULATOR_HOST=localhost:8085 -v $STORAGE:$STORAGE geocube -project geocube-emulator -ingestionStorage=$STORAGE -dbConnection=postgresql://user:password@localhost:5432/geocube -eventsQueue events -consolidationsQueue consolidations -cancelledJobs $STORAGE/cancelled-jobs export STORAGE=/geocube-datasets docker run --rm --network=host -e PUBSUB_EMULATOR_HOST=localhost:8085 -v $STORAGE:$STORAGE geocube-consolidater /consolidater -psProject geocube-emulator -workdir=/tmp -eventsQueue events -consolidationsQueue consolidations -cancelledJobs $STORAGE/cancelled-jobs With GCS support (authentication with application_default_credentials.json): export STORAGE=/geocube-datasets docker run --rm -v ~/.config/gcloud:/root/.config/gcloud geocube -with-gcs [...] For more information concerning running option, see: https://docs.docker.com/engine/reference/commandline/run/","title":"Run server - examples:"},{"location":"installation/docker-install/#run-downloader-examples","text":"Basic example: docker run --rm -p 127.0.0.1:8081:8081/tcp geocube-downloader -port 8081 -workers 4 Example with GCS support: With GOOGLE_APPLICATION_CREDENTIALS: docker run --rm -e GOOGLE_APPLICATION_CREDENTIALS=/account/geocube_server.json -p 127.0.0.1:8081:8081/tcp --mount type=bind,src=~/Documents/account/geocube,dst=/account 65cddc550e9a geocube-downloader -port 8081 -with-gcs -workers 4 -gdalBlockSize 2Mb With application_default_credentials.json: docker run --rm -v ~/.config/gcloud:/root/.config/gcloud -p 127.0.0.1:8081:8081/tcp geocube-downloader -port 8081 -with-gcs -workers 4 -gdalBlockSize 2Mb Additionnal information here","title":"Run downloader - examples:"},{"location":"installation/docker-install/#messaging-broker","text":"cf Local environment - Messaging Broker","title":"Messaging Broker"},{"location":"installation/docker-install/#docker-compose","text":"A docker-compose file is provided as example. It's a minimal example, so feel free to edit it to take advantage of the full power of the Geocube. Copy the ./docker/.env.example to ./docker/.env Edit ./docker/.env to set the STORAGE_URI (it will be mount as a volume to access and store images). Build the base image cd docker and docker-compose up","title":"Docker-compose"},{"location":"installation/k8s-install/","text":"Installation - Kubernetes Cluster IAM & Security All the notions of security and service account are not covered in this document. It is the responsibility of the installers. The files presented below are available as examples/templates. They do not present any notions of security. Container Registry You can create your own registry server: https://docs.docker.com/registry/deploying/ Docker Hub In case the images are stored on https://hub.docker.com, you can define them as follows in your kubernetes configuration files (postgresql example: image: postgres:11 ): apiVersion: networking.k8s.io/v1 kind: Deployment metadata: name: postgresql spec: replicas: 1 template: spec: containers: - name: postgresql image: postgres:11 In this example, postgres:11 image will be loaded. Private Registry You can configure your kubernetes deployment files with private docker registry. For more information, see: Pull an Image from a Private Registry imagePullSecrets is defined in your kubernetes configuration files and image name is specified as follow ex: image: geocube-private-image:tag Database Geocube server must have sufficient rights in order to read and write into database. For more information, see: Client Authentication . Geocube required that max_connections must be configured as 1024 . For more information, see: Server configuration . Kubernetes example configuration files are available in deploy/k8s/database in order to deploy minimal postgresql Database. All the parameters between {{}} are mandatory: {{POSTGRES_USER}} : user name {{POSTGRES_PASSWORD}} : user password $ kubectl apply -f deploy/k8s/database/database.yaml Pubsub Emulator Kubernetes configuration files are available in deploy/k8s/pubSubEmulator in order to deploy minimal pubSub emulator. {{PUBSUB_EMULATOR_IMAGE}} is to be defined (eg: <container_registry>/pubsub-emulator:<tag> ) $ kubectl apply -f deploy/k8s/pubSubEmulator/pubSub.yaml You have to configure the access between PubSub and geocube\u2019s components. Apiserver Apiserver must have the necessary access to communicate with the database, the messaging service as well as the rights to read and write to the storage. Create apiserver service account ApiServer must have suffisant rights in order to manage object storage and secrets access. $ kubectl apply -f deploy/k8s/apiserver/service-account.yaml Create apiserver service $ kubectl apply -f deploy/k8s/apiserver/service.yaml Create apiserver deployment In order to start ApiServer, all the parameters between {{}} are to be defined in file deploy/k8s/apiserver/deployment.yaml : {{GEOCUBE_SERVER_IMAGE}} : Geocube ApiServer Docker Image (eg. <container_registry>/geocube-go-server:<tag> ) Connection to the database {{BD_HOST}} , {{DB_USER}} and {{DB_PASSWD}} {{INGESTION_STORAGE}} : uri to store ingested datasets (local and gcs uris are supported) {{PUBSUB_EMULATOR_HOST}} environment variable can be added with pubSub emulator service IP (only if emulator is used) {{CANCELLED_JOBS_STORAGE}} : uri to store cancelled jobs (local and gcs uris are supported) Ex: containers: - args: - -dbName=geocube - -dbUser=apiserver - -dbPassword=mydbPassword - -dbHost=localhost:5432 - -eventsQueue=events - -consolidationsQueue=consolidations - -ingestionStorage=/geocube-datasets or gs://my-bucket/geocube-datasets - -maxConnectionAge=3600 - -workers=1 - -cancelledJobs=/geocube-cancelled-jobs or gs://my-bucket/geocube-cancelled-jobs env: - name: PUBSUB_EMULATOR_HOST value: 0.0.0.0:8085 image: registry/project/geocube-go-server:v1 $ kubectl apply -f deploy/k8s/apiserver/deployment.yaml Consolidater Consolidater must have the necessary access to communicate with the messaging service as well as the rights to read and write to the storage. Create Consolidater RoleBinding $ kubectl apply -f deploy/k8s/consolidater/role-binding.yaml Create Consolidater Role (CRUD on pods & list on ReplicationControllers) $ kubectl apply -f deploy/k8s/consolidater/role.yaml Create Autoscaler service account $ kubectl apply -f deploy/k8s/consolidater/autoscaler-service-account.yaml Create Autoscaler replication controller In order to start Autoscaler replication controller, you have to define some parameters in file deploy/k8s/consolidater/replication-controller.yaml : {{CONSOLIDATER_IMAGE}} : Consolidater Docker Image (eg. <container_registry>/consolidater:<tag> ). {{PUBSUB_EMULATOR_HOST}} environment variable could be added with pubSub emulator service IP (only if emulator is used). {{CANCELLED_JOBS_STORAGE}} : uri to store cancelled jobs (local and gcs uris are supported) Ex: containers: - name: consolidater image: registry/project/consolidater:v1 imagePullPolicy: \"Always\" ports: - containerPort: 9000 protocol: TCP env: - name: PUBSUB_EMULATOR_HOST value: 0.0.0.0:8085 [...] args: - | UUID=`uuidgen`; WORKDIR=/local-ssd/$UUID; mkdir -p $WORKDIR; /consolidater -eventsQueue events -consolidationsQueue consolidations -workdir $WORKDIR -cancelledJobs=/geocube-cancelled-jobs or gs://my-bucket/geocube-cancelled-jobs || true; exitcode=$?; rm -rf $WORKDIR; exit $exitcode; $ kubectl apply -f deploy/k8s/consolidater/replication-controller.yaml Create autoscaler deployment Define Autoscaler Docker Image {{AUTOSCALER_IMAGE}} (eg. <container_registry>/autoscaler:<tag> ) in file deploy/k8s/consolidater/autoscaler-deployment.yaml Ex: containers: - name: autoscaler image: registry/project/autoscaler:v1 imagePullPolicy: Always args: - -update=30s - -queue=consolidations - -rc=consolidater - -ns=default - -ratio=1 - -minratio=1 - -step=16 - -max=256 - -min=0 - -pod.cost.path=/termination_cost - -pod.cost.port=9000 $ kubectl apply -f deploy/k8s/consolidater/autoscaler-deployment.yaml Reference Kubernetes Deployment describes a desired state of pod: https://kubernetes.io/docs/concepts/workloads/controllers/deployment Pods is a group of one or more containers: https://kubernetes.io/docs/concepts/workloads/pods Secrets lets you store and manage sensitive information, such as passwords, OAuth tokens, and ssh keys: https://kubernetes.io/docs/concepts/configuration/secret Service is an abstract way to expose an application running on a set of Pods as a network service: https://kubernetes.io/fr/docs/concepts/services-networking/service Replication controller ensures that a specified number of pod replicas are running at any one time: https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller RoleBinding and Role : https://kubernetes.io/docs/reference/access-authn-authz/rbac/","title":"Deploying with Kubernetes"},{"location":"installation/k8s-install/#installation-kubernetes-cluster","text":"","title":"Installation - Kubernetes Cluster"},{"location":"installation/k8s-install/#iam-security","text":"All the notions of security and service account are not covered in this document. It is the responsibility of the installers. The files presented below are available as examples/templates. They do not present any notions of security.","title":"IAM &amp; Security"},{"location":"installation/k8s-install/#container-registry","text":"You can create your own registry server: https://docs.docker.com/registry/deploying/","title":"Container Registry"},{"location":"installation/k8s-install/#docker-hub","text":"In case the images are stored on https://hub.docker.com, you can define them as follows in your kubernetes configuration files (postgresql example: image: postgres:11 ): apiVersion: networking.k8s.io/v1 kind: Deployment metadata: name: postgresql spec: replicas: 1 template: spec: containers: - name: postgresql image: postgres:11 In this example, postgres:11 image will be loaded.","title":"Docker Hub"},{"location":"installation/k8s-install/#private-registry","text":"You can configure your kubernetes deployment files with private docker registry. For more information, see: Pull an Image from a Private Registry imagePullSecrets is defined in your kubernetes configuration files and image name is specified as follow ex: image: geocube-private-image:tag","title":"Private Registry"},{"location":"installation/k8s-install/#database","text":"Geocube server must have sufficient rights in order to read and write into database. For more information, see: Client Authentication . Geocube required that max_connections must be configured as 1024 . For more information, see: Server configuration . Kubernetes example configuration files are available in deploy/k8s/database in order to deploy minimal postgresql Database. All the parameters between {{}} are mandatory: {{POSTGRES_USER}} : user name {{POSTGRES_PASSWORD}} : user password $ kubectl apply -f deploy/k8s/database/database.yaml","title":"Database"},{"location":"installation/k8s-install/#pubsub-emulator","text":"Kubernetes configuration files are available in deploy/k8s/pubSubEmulator in order to deploy minimal pubSub emulator. {{PUBSUB_EMULATOR_IMAGE}} is to be defined (eg: <container_registry>/pubsub-emulator:<tag> ) $ kubectl apply -f deploy/k8s/pubSubEmulator/pubSub.yaml You have to configure the access between PubSub and geocube\u2019s components.","title":"Pubsub Emulator"},{"location":"installation/k8s-install/#apiserver","text":"Apiserver must have the necessary access to communicate with the database, the messaging service as well as the rights to read and write to the storage. Create apiserver service account ApiServer must have suffisant rights in order to manage object storage and secrets access. $ kubectl apply -f deploy/k8s/apiserver/service-account.yaml Create apiserver service $ kubectl apply -f deploy/k8s/apiserver/service.yaml Create apiserver deployment In order to start ApiServer, all the parameters between {{}} are to be defined in file deploy/k8s/apiserver/deployment.yaml : {{GEOCUBE_SERVER_IMAGE}} : Geocube ApiServer Docker Image (eg. <container_registry>/geocube-go-server:<tag> ) Connection to the database {{BD_HOST}} , {{DB_USER}} and {{DB_PASSWD}} {{INGESTION_STORAGE}} : uri to store ingested datasets (local and gcs uris are supported) {{PUBSUB_EMULATOR_HOST}} environment variable can be added with pubSub emulator service IP (only if emulator is used) {{CANCELLED_JOBS_STORAGE}} : uri to store cancelled jobs (local and gcs uris are supported) Ex: containers: - args: - -dbName=geocube - -dbUser=apiserver - -dbPassword=mydbPassword - -dbHost=localhost:5432 - -eventsQueue=events - -consolidationsQueue=consolidations - -ingestionStorage=/geocube-datasets or gs://my-bucket/geocube-datasets - -maxConnectionAge=3600 - -workers=1 - -cancelledJobs=/geocube-cancelled-jobs or gs://my-bucket/geocube-cancelled-jobs env: - name: PUBSUB_EMULATOR_HOST value: 0.0.0.0:8085 image: registry/project/geocube-go-server:v1 $ kubectl apply -f deploy/k8s/apiserver/deployment.yaml","title":"Apiserver"},{"location":"installation/k8s-install/#consolidater","text":"Consolidater must have the necessary access to communicate with the messaging service as well as the rights to read and write to the storage. Create Consolidater RoleBinding $ kubectl apply -f deploy/k8s/consolidater/role-binding.yaml Create Consolidater Role (CRUD on pods & list on ReplicationControllers) $ kubectl apply -f deploy/k8s/consolidater/role.yaml Create Autoscaler service account $ kubectl apply -f deploy/k8s/consolidater/autoscaler-service-account.yaml Create Autoscaler replication controller In order to start Autoscaler replication controller, you have to define some parameters in file deploy/k8s/consolidater/replication-controller.yaml : {{CONSOLIDATER_IMAGE}} : Consolidater Docker Image (eg. <container_registry>/consolidater:<tag> ). {{PUBSUB_EMULATOR_HOST}} environment variable could be added with pubSub emulator service IP (only if emulator is used). {{CANCELLED_JOBS_STORAGE}} : uri to store cancelled jobs (local and gcs uris are supported) Ex: containers: - name: consolidater image: registry/project/consolidater:v1 imagePullPolicy: \"Always\" ports: - containerPort: 9000 protocol: TCP env: - name: PUBSUB_EMULATOR_HOST value: 0.0.0.0:8085 [...] args: - | UUID=`uuidgen`; WORKDIR=/local-ssd/$UUID; mkdir -p $WORKDIR; /consolidater -eventsQueue events -consolidationsQueue consolidations -workdir $WORKDIR -cancelledJobs=/geocube-cancelled-jobs or gs://my-bucket/geocube-cancelled-jobs || true; exitcode=$?; rm -rf $WORKDIR; exit $exitcode; $ kubectl apply -f deploy/k8s/consolidater/replication-controller.yaml Create autoscaler deployment Define Autoscaler Docker Image {{AUTOSCALER_IMAGE}} (eg. <container_registry>/autoscaler:<tag> ) in file deploy/k8s/consolidater/autoscaler-deployment.yaml Ex: containers: - name: autoscaler image: registry/project/autoscaler:v1 imagePullPolicy: Always args: - -update=30s - -queue=consolidations - -rc=consolidater - -ns=default - -ratio=1 - -minratio=1 - -step=16 - -max=256 - -min=0 - -pod.cost.path=/termination_cost - -pod.cost.port=9000 $ kubectl apply -f deploy/k8s/consolidater/autoscaler-deployment.yaml","title":"Consolidater"},{"location":"installation/k8s-install/#reference","text":"","title":"Reference"},{"location":"installation/k8s-install/#kubernetes","text":"Deployment describes a desired state of pod: https://kubernetes.io/docs/concepts/workloads/controllers/deployment Pods is a group of one or more containers: https://kubernetes.io/docs/concepts/workloads/pods Secrets lets you store and manage sensitive information, such as passwords, OAuth tokens, and ssh keys: https://kubernetes.io/docs/concepts/configuration/secret Service is an abstract way to expose an application running on a set of Pods as a network service: https://kubernetes.io/fr/docs/concepts/services-networking/service Replication controller ensures that a specified number of pod replicas are running at any one time: https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller RoleBinding and Role : https://kubernetes.io/docs/reference/access-authn-authz/rbac/","title":"Kubernetes"},{"location":"installation/local-install/","text":"Installation - Local environment Environment of development Name Version link Golang >= 1.16 https://golang.org/doc/install GDAL >= 3.3 https://gdal.org Python >= 3.7 https://www.python.org/downloads/ PostgreSQL >= 11 https://www.postgresql.org/download/ PostGIS >= 3.1 https://postgis.net/install/ Docker NC https://docs.docker.com/engine/install/ Clone repository Clone https://github.com/airbusgeo/geocube.git repository. Build and run Apiserver For more information concerning build and run go application, see: Build and run Go Application https://golang.org/cmd/go/#hdr-Compile_packages_and_dependencies In the geocube repository, cmd/server , run go build command in order to generate executable file: $ cd cmd/server && go build It creates the executable server . Some arguments are required in order to start the server. $ ./server --help Usage of ./server: -aws-endpoint string define aws_endpoint for GDAL to use s3 storage (--with-s3) -aws-region string define aws_region for GDAL to use s3 storage (--with-s3) -aws-shared-credentials-file string define aws_shared_credentials_file for GDAL to use s3 storage (--with-s3) -baSecretName string name of the secret that stores the bearer authentication (admin & user) (gcp only) -cancelledJobs string storage where cancelled jobs are referenced. Must be reachable by the Consolidation Workers and the Geocube with read/write permissions -consolidationsQueue string name of the pgqueue or the pubsub topic to send the consolidation orders -dbConnection string database connection (ex: postgresql://user:password@localhost:5432/geocube) -dbHost string database host (see dbName) -dbName string database name (to connect with User, Host & Password) -dbPassword string database password (see dbName) -dbSecretName string name of the secret that stores credentials to connect to the database (gcp only) -dbUser string database user (see dbName) -eventsQueue string name of the pgqueue or the pubsub topic to send the asynchronous job events -gdalBlockSize string gdal blocksize value (default 1Mb) (default \"1Mb\") -gdalNumCachedBlocks int gdal blockcache value (default 500) (default 500) -gdalStorageDebug enable storage debug to use custom gdal storage strategy -ingestionStorage string path to the storage where ingested and consolidated datasets will be stored. Must be reachable with read/write/delete permissions. (local/gs) -maxConnectionAge int grpc max age connection -pgqConnection string url of the postgres database to enable pgqueue messaging system (pgqueue only) -port string geocube port to use (default \"8080\") -project string project name (gcp only/not required in local usage) -tls enable TLS protocol (certificate and key must be /tls/tls.crt and /tls/tls.key) -with-gcs configure GDAL to use gcs storage (may need authentication) -with-s3 configure GDAL to use s3 storage (may need authentication) -workers int number of parallel workers per catalog request (default 1) Example (run): $ ./server -ingestionStorage=/geocube-datasets -dbConnection=postgresql://user:password@localhost:5432/geocube -eventsQueue events -consolidationsQueue consolidations -cancelledJobs /tmp {\"severity\":\"info\",\"timestamp\":\"2021-05-24T15:10:57.621+0200\",\"message\":\"Geocube v0.3.0\"} Consolidater In the geocube repository, cmd/consolidater , run go build command in order to generate executable file: $ cd cmd/consolidater && go build It creates the executable consolidater . Some arguments are required in order to start a consolidation worker. $ ./consolidater --help Usage of ./consolidater: -aws-endpoint string define aws_endpoint for GDAL to use s3 storage (--with-s3) -aws-region string define aws_region for GDAL to use s3 storage (--with-s3) -aws-shared-credentials-file string define aws_shared_credentials_file for GDAL to use s3 storage (--with-s3) -cancelledJobs string storage where cancelled jobs are referenced -consolidationsQueue string name of the messaging queue for consolidation jobs (pgqueue or pubsub subscription) -eventsQueue string name of the messaging queue for job events (pgquue or pubsub topic) -gdalBlockSize string gdal blocksize value (default 1Mb) (default \"1Mb\") -gdalNumCachedBlocks int gdal blockcache value (default 500) (default 500) -gdalStorageDebug enable storage debug to use custom gdal storage strategy -local-download locally download the datasets before starting the consolidation (generally faster than letting GDAL to download them tile by tile) (default true) -pgqConnection string url of the postgres database to enable pgqueue messaging system (pgqueue only) -psProject string subscription project (gcp pubSub only) -retryCount int number of retries when consolidation job failed with a temporary error (default 1) -with-gcs configure GDAL to use gcs storage (may need authentication) -with-s3 configure GDAL to use s3 storage (may need authentication) -workdir string scratch work directory -workers int number of workers for parallel tasks (default 1) Example (run): $ ./consolidater -workdir=/tmp -psProject geocube-emulator -eventsQueue events -consolidationsQueue consolidations -cancelledJobs /tmp Downloader Downloader service is useful if the server runs in a distant environment, and the local environment has an efficient access to the storage. In the geocube repository, cmd/downloader , run go build command in order to generate executable file: $ cd cmd/downloader && go build It creates the executable downloader . Some arguments are required in order to start a downloader service. Downloader available options: $ ./downloader --help Usage of ./downloader: -aws-endpoint string define aws_endpoint for GDAL to use s3 storage (--with-s3) -aws-region string define aws_region for GDAL to use s3 storage (--with-s3) -aws-shared-credentials-file string define aws_shared_credentials_file for GDAL to use s3 storage (--with-s3) -gdalBlockSize string gdal blocksize value (default 1Mb) (default \"1Mb\") -gdalNumCachedBlocks int gdal blockcache value (default 500) (default 500) -gdalStorageDebug enable storage debug to use custom gdal storage strategy -maxConnectionAge int grpc max age connection -port string geocube downloader port to use (default \"8080\") -tls enable TLS protocol -with-gcs configure GDAL to use gcs storage (may need authentication) -with-s3 configure GDAL to use s3 storage (may need authentication) -workers int number of parallel workers per catalog request (default 1) Basic example: ./downloader -port 8081 -workers 4 Example with GCS support: ./downloader -port 8081 -with-gcs -workers 4 -gdalBlockSize 2Mb Storage Debug (GCP only): It's possible to monitor storage metrics with --gdalStorageDebug argument. You will retrieve storage metrics into logs as: {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/2/-89/myLayout/myFile.tif - 2 calls - 2097152 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/4/-90/myLayout/myFile.tif - 2 calls - 2097152 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/5/-91/myLayout/myFile.tif - 2 calls - 2097152 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/4/-91/myLayout/myFile.tif - 3 calls - 3145728 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/5/-90/myLayout/myFile.tif - 2 calls - 2097152 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/3/-89/myLayout/myFile.tif - 2 calls - 3145728 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/2/-90/myLayout/myFile.tif - 2 calls - 2097152 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/3/-90/myLayout/myFile.tif - 2 calls - 3145728 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/6/-90/myLayout/myFile.tif - 2 calls - 2097152 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/6/-91/myLayout/myFile.tif - 2 calls - 2097152 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/3/-91/myLayout/myFile.tif - 3 calls - 3145728 octets\"} Logs: https://docs.docker.com/engine/reference/commandline/logs/#examples Autoscaler Autoscaling is not available in a local environment. See K8S - Autoscaler for more information. Messaging Broker PGQueue To use this messaging broker, create the pgq_jobs table in your postgres database using the following script interface/messaging/pgqueue/create_table.sql . $ psql -h <database_host> -d <database_name> -f interface/messaging/pgqueue/create_table.sql Then, start the apiserver and the consolidater with the corresponding arguments: - --pgqConnection : connection uri to the postgres database (e.g. postgresql://user:password@localhost:5432/geocube ) - --consolidationQueue consolidations - --eventsQueue events And the Autoscaler, with: - --pgq-connection : connection uri to the postgres database (e.g. postgresql://user:password@localhost:5432/geocube ) - --queue consolidations Pub/Sub (Emulator) For more information, see: https://cloud.google.com/pubsub/docs/emulator You can launch a local emulator with this command: $ gcloud beta emulators pubsub start --project=geocube-emulator Executing: /usr/lib/google-cloud-sdk/platform/pubsub-emulator/bin/cloud-pubsub-emulator --host=localhost --port=8085 [pubsub] This is the Google Pub/Sub fake. [pubsub] Implementation may be incomplete or differ from the real system. [pubsub] Jun 30, 2021 3:04:05 PM com.google.cloud.pubsub.testing.v1.Main main [pubsub] INFO: IAM integration is disabled. IAM policy methods and ACL checks are not supported [pubsub] SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\". [pubsub] SLF4J: Defaulting to no-operation (NOP) logger implementation [pubsub] SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. [pubsub] Jun 30, 2021 3:04:06 PM com.google.cloud.pubsub.testing.v1.Main main [pubsub] INFO: Server started, listening on 8085 Topics and subscription which are necessary for the proper functioning of the geocube, can be created by running the following script (replace $GEOCUBE_SERVER by the appropriate value): $ go run tools/pubsub_emulator/main.go --project-id geocube-emulator --geocube-server https://$GEOCUBE_SERVER 2021/06/30 14:56:48 New client for project-id geocube-emulator 2021/06/30 14:56:48 Create Topic : consolidations 2021/06/30 14:56:48 Create Topic : consolidations-worker 2021/06/30 14:56:48 Create Topic : events 2021/06/30 14:56:48 Create Subscription : consolidations 2021/06/30 14:56:48 Create Subscription : consolidations-worker 2021/06/30 14:56:48 Create Subscription : events pushing to https://$GEOCUBE_SERVER/push 2021/06/30 14:56:48 Done! In order to run geocube with the PubSub emulator, you must define the PUBSUB_EMULATOR_HOST environment variable (by default localhost:8085 ) before starting services.","title":"Local environment"},{"location":"installation/local-install/#installation-local-environment","text":"","title":"Installation - Local environment"},{"location":"installation/local-install/#environment-of-development","text":"Name Version link Golang >= 1.16 https://golang.org/doc/install GDAL >= 3.3 https://gdal.org Python >= 3.7 https://www.python.org/downloads/ PostgreSQL >= 11 https://www.postgresql.org/download/ PostGIS >= 3.1 https://postgis.net/install/ Docker NC https://docs.docker.com/engine/install/","title":"Environment of development"},{"location":"installation/local-install/#clone-repository","text":"Clone https://github.com/airbusgeo/geocube.git repository.","title":"Clone repository"},{"location":"installation/local-install/#build-and-run","text":"","title":"Build and run"},{"location":"installation/local-install/#apiserver","text":"For more information concerning build and run go application, see: Build and run Go Application https://golang.org/cmd/go/#hdr-Compile_packages_and_dependencies In the geocube repository, cmd/server , run go build command in order to generate executable file: $ cd cmd/server && go build It creates the executable server . Some arguments are required in order to start the server. $ ./server --help Usage of ./server: -aws-endpoint string define aws_endpoint for GDAL to use s3 storage (--with-s3) -aws-region string define aws_region for GDAL to use s3 storage (--with-s3) -aws-shared-credentials-file string define aws_shared_credentials_file for GDAL to use s3 storage (--with-s3) -baSecretName string name of the secret that stores the bearer authentication (admin & user) (gcp only) -cancelledJobs string storage where cancelled jobs are referenced. Must be reachable by the Consolidation Workers and the Geocube with read/write permissions -consolidationsQueue string name of the pgqueue or the pubsub topic to send the consolidation orders -dbConnection string database connection (ex: postgresql://user:password@localhost:5432/geocube) -dbHost string database host (see dbName) -dbName string database name (to connect with User, Host & Password) -dbPassword string database password (see dbName) -dbSecretName string name of the secret that stores credentials to connect to the database (gcp only) -dbUser string database user (see dbName) -eventsQueue string name of the pgqueue or the pubsub topic to send the asynchronous job events -gdalBlockSize string gdal blocksize value (default 1Mb) (default \"1Mb\") -gdalNumCachedBlocks int gdal blockcache value (default 500) (default 500) -gdalStorageDebug enable storage debug to use custom gdal storage strategy -ingestionStorage string path to the storage where ingested and consolidated datasets will be stored. Must be reachable with read/write/delete permissions. (local/gs) -maxConnectionAge int grpc max age connection -pgqConnection string url of the postgres database to enable pgqueue messaging system (pgqueue only) -port string geocube port to use (default \"8080\") -project string project name (gcp only/not required in local usage) -tls enable TLS protocol (certificate and key must be /tls/tls.crt and /tls/tls.key) -with-gcs configure GDAL to use gcs storage (may need authentication) -with-s3 configure GDAL to use s3 storage (may need authentication) -workers int number of parallel workers per catalog request (default 1) Example (run): $ ./server -ingestionStorage=/geocube-datasets -dbConnection=postgresql://user:password@localhost:5432/geocube -eventsQueue events -consolidationsQueue consolidations -cancelledJobs /tmp {\"severity\":\"info\",\"timestamp\":\"2021-05-24T15:10:57.621+0200\",\"message\":\"Geocube v0.3.0\"}","title":"Apiserver"},{"location":"installation/local-install/#consolidater","text":"In the geocube repository, cmd/consolidater , run go build command in order to generate executable file: $ cd cmd/consolidater && go build It creates the executable consolidater . Some arguments are required in order to start a consolidation worker. $ ./consolidater --help Usage of ./consolidater: -aws-endpoint string define aws_endpoint for GDAL to use s3 storage (--with-s3) -aws-region string define aws_region for GDAL to use s3 storage (--with-s3) -aws-shared-credentials-file string define aws_shared_credentials_file for GDAL to use s3 storage (--with-s3) -cancelledJobs string storage where cancelled jobs are referenced -consolidationsQueue string name of the messaging queue for consolidation jobs (pgqueue or pubsub subscription) -eventsQueue string name of the messaging queue for job events (pgquue or pubsub topic) -gdalBlockSize string gdal blocksize value (default 1Mb) (default \"1Mb\") -gdalNumCachedBlocks int gdal blockcache value (default 500) (default 500) -gdalStorageDebug enable storage debug to use custom gdal storage strategy -local-download locally download the datasets before starting the consolidation (generally faster than letting GDAL to download them tile by tile) (default true) -pgqConnection string url of the postgres database to enable pgqueue messaging system (pgqueue only) -psProject string subscription project (gcp pubSub only) -retryCount int number of retries when consolidation job failed with a temporary error (default 1) -with-gcs configure GDAL to use gcs storage (may need authentication) -with-s3 configure GDAL to use s3 storage (may need authentication) -workdir string scratch work directory -workers int number of workers for parallel tasks (default 1) Example (run): $ ./consolidater -workdir=/tmp -psProject geocube-emulator -eventsQueue events -consolidationsQueue consolidations -cancelledJobs /tmp","title":"Consolidater"},{"location":"installation/local-install/#downloader","text":"Downloader service is useful if the server runs in a distant environment, and the local environment has an efficient access to the storage. In the geocube repository, cmd/downloader , run go build command in order to generate executable file: $ cd cmd/downloader && go build It creates the executable downloader . Some arguments are required in order to start a downloader service. Downloader available options: $ ./downloader --help Usage of ./downloader: -aws-endpoint string define aws_endpoint for GDAL to use s3 storage (--with-s3) -aws-region string define aws_region for GDAL to use s3 storage (--with-s3) -aws-shared-credentials-file string define aws_shared_credentials_file for GDAL to use s3 storage (--with-s3) -gdalBlockSize string gdal blocksize value (default 1Mb) (default \"1Mb\") -gdalNumCachedBlocks int gdal blockcache value (default 500) (default 500) -gdalStorageDebug enable storage debug to use custom gdal storage strategy -maxConnectionAge int grpc max age connection -port string geocube downloader port to use (default \"8080\") -tls enable TLS protocol -with-gcs configure GDAL to use gcs storage (may need authentication) -with-s3 configure GDAL to use s3 storage (may need authentication) -workers int number of parallel workers per catalog request (default 1) Basic example: ./downloader -port 8081 -workers 4 Example with GCS support: ./downloader -port 8081 -with-gcs -workers 4 -gdalBlockSize 2Mb Storage Debug (GCP only): It's possible to monitor storage metrics with --gdalStorageDebug argument. You will retrieve storage metrics into logs as: {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/2/-89/myLayout/myFile.tif - 2 calls - 2097152 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/4/-90/myLayout/myFile.tif - 2 calls - 2097152 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/5/-91/myLayout/myFile.tif - 2 calls - 2097152 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/4/-91/myLayout/myFile.tif - 3 calls - 3145728 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/5/-90/myLayout/myFile.tif - 2 calls - 2097152 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/3/-89/myLayout/myFile.tif - 2 calls - 3145728 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/2/-90/myLayout/myFile.tif - 2 calls - 2097152 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/3/-90/myLayout/myFile.tif - 2 calls - 3145728 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/6/-90/myLayout/myFile.tif - 2 calls - 2097152 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/6/-91/myLayout/myFile.tif - 2 calls - 2097152 octets\"} {\"severity\":\"debug\",\"timestamp\":\"2022-01-14T11:29:15.618+0100\",\"message\":\"GCS Metrics: gs://myBucket/32523_20m/3/-91/myLayout/myFile.tif - 3 calls - 3145728 octets\"} Logs: https://docs.docker.com/engine/reference/commandline/logs/#examples","title":"Downloader"},{"location":"installation/local-install/#autoscaler","text":"Autoscaling is not available in a local environment. See K8S - Autoscaler for more information.","title":"Autoscaler"},{"location":"installation/local-install/#messaging-broker","text":"","title":"Messaging Broker"},{"location":"installation/local-install/#pgqueue","text":"To use this messaging broker, create the pgq_jobs table in your postgres database using the following script interface/messaging/pgqueue/create_table.sql . $ psql -h <database_host> -d <database_name> -f interface/messaging/pgqueue/create_table.sql Then, start the apiserver and the consolidater with the corresponding arguments: - --pgqConnection : connection uri to the postgres database (e.g. postgresql://user:password@localhost:5432/geocube ) - --consolidationQueue consolidations - --eventsQueue events And the Autoscaler, with: - --pgq-connection : connection uri to the postgres database (e.g. postgresql://user:password@localhost:5432/geocube ) - --queue consolidations","title":"PGQueue"},{"location":"installation/local-install/#pubsub-emulator","text":"For more information, see: https://cloud.google.com/pubsub/docs/emulator You can launch a local emulator with this command: $ gcloud beta emulators pubsub start --project=geocube-emulator Executing: /usr/lib/google-cloud-sdk/platform/pubsub-emulator/bin/cloud-pubsub-emulator --host=localhost --port=8085 [pubsub] This is the Google Pub/Sub fake. [pubsub] Implementation may be incomplete or differ from the real system. [pubsub] Jun 30, 2021 3:04:05 PM com.google.cloud.pubsub.testing.v1.Main main [pubsub] INFO: IAM integration is disabled. IAM policy methods and ACL checks are not supported [pubsub] SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\". [pubsub] SLF4J: Defaulting to no-operation (NOP) logger implementation [pubsub] SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. [pubsub] Jun 30, 2021 3:04:06 PM com.google.cloud.pubsub.testing.v1.Main main [pubsub] INFO: Server started, listening on 8085 Topics and subscription which are necessary for the proper functioning of the geocube, can be created by running the following script (replace $GEOCUBE_SERVER by the appropriate value): $ go run tools/pubsub_emulator/main.go --project-id geocube-emulator --geocube-server https://$GEOCUBE_SERVER 2021/06/30 14:56:48 New client for project-id geocube-emulator 2021/06/30 14:56:48 Create Topic : consolidations 2021/06/30 14:56:48 Create Topic : consolidations-worker 2021/06/30 14:56:48 Create Topic : events 2021/06/30 14:56:48 Create Subscription : consolidations 2021/06/30 14:56:48 Create Subscription : consolidations-worker 2021/06/30 14:56:48 Create Subscription : events pushing to https://$GEOCUBE_SERVER/push 2021/06/30 14:56:48 Done! In order to run geocube with the PubSub emulator, you must define the PUBSUB_EMULATOR_HOST environment variable (by default localhost:8085 ) before starting services.","title":"Pub/Sub (Emulator)"},{"location":"installation/others/","text":"Other environments Geocube is designed to be customizable and deployable in other environments. But it may be necessary to implemente new interfaces. More information on interfaces here","title":"Customize environment"},{"location":"installation/others/#other-environments","text":"Geocube is designed to be customizable and deployable in other environments. But it may be necessary to implemente new interfaces. More information on interfaces here","title":"Other environments"},{"location":"installation/prerequisite/","text":"Prerequisites The Geocube needs: a Geospatial database (currently supported : Postgresql with PostGIS) a Messaging System to exchange messages between applications (currently supported: Pub/Sub and Postgresql using pgqueue ) an Object Storage, writable and readable by range-request (currently supported: local storage, Google Storage , AWS-S3 ) (Optional) a Scaling Platform to automatically scale the ressources (currently supported: Kubernetes ) The Geocube can be run locally , as dockers or deployed in a cluster .","title":"Prerequisite"},{"location":"installation/prerequisite/#prerequisites","text":"The Geocube needs: a Geospatial database (currently supported : Postgresql with PostGIS) a Messaging System to exchange messages between applications (currently supported: Pub/Sub and Postgresql using pgqueue ) an Object Storage, writable and readable by range-request (currently supported: local storage, Google Storage , AWS-S3 ) (Optional) a Scaling Platform to automatically scale the ressources (currently supported: Kubernetes ) The Geocube can be run locally , as dockers or deployed in a cluster .","title":"Prerequisites"},{"location":"installation/upgrade/","text":"Geocube Upgrade PostgreSQL database After upgrading the Geocube server, the database schema might need an update. Apply incrementally each interface/database/pg/update_X.X.X.sql with X.X.X corresponding to a Geocube Server version from your previous version to the current version. $ psql -h <database_host> -d <database_name> -f interface/database/pg/update_X.X.X.sql","title":"Upgrade"},{"location":"installation/upgrade/#geocube-upgrade","text":"","title":"Geocube Upgrade"},{"location":"installation/upgrade/#postgresql-database","text":"After upgrading the Geocube server, the database schema might need an update. Apply incrementally each interface/database/pg/update_X.X.X.sql with X.X.X corresponding to a Geocube Server version from your previous version to the current version. $ psql -h <database_host> -d <database_name> -f interface/database/pg/update_X.X.X.sql","title":"PostgreSQL database"},{"location":"user-guide/access/","text":"Access Get images To retrieve data from the Geocube, the datasets or the containers are not explicitly named. The Geocube will find the datasets depending on user-provided criteria (see GetCubeRequest ): geography : using either the aoi of the record and the shape of the dataset semantic : using the tags of the record time : using the datetime of the record kind of data : using the instance These criteria can return several datasets. The Geocube will automatically do a mosaic of the results. The geocube only returned 2D rectangular arrays. Once the datasets are filtered by records and instance, the Geocube will extract a rectangular extent using the warp command of GDAL, that automatically deals with reprojection, rescaling and mosaicing . If necessary, a casting of the value is done (see Dataformat mapping ). Finally, the Geocube streams the results through the GRPC interface. If the resulting image is only no data (because there is no dataset matching the criteria or the extent does not contain data), the image is skipped and the Geocube returns nothing. Get a cube A cube of data is just a list of 2D arrays from the same instance , with the same extent in the same crs . Getting an image or getting a cube is exactly the same except that, in the latter case, the request defines a collection of records . If the Geocube has been optimized to serve the timeseries as efficiently as possible (see Consolidation : - It is faster to request a timeseries than each image one by one. - It is better to request a large area (bigger or equal to the block size) than a lot of small areas. Get mosaics from several records If several dataset are linked to the same record, the Geocube returns a mosaic of them. It's possible to request the Geocube to merge datasets from different records. By passing a GroupedRecords , the Geocube automatically groups the dataset as if they belong to the same records, except that the order is preserved (the datasets belonging to the last record of the group will be on the top of the mosaic). NB : To be used carefully : the edges are not blended and it may result in visible seams. To request a cube of grouped records, the request will contain a GroupedRecordsList (actually a list of list of records). Get metadata only Instead of returning the images, the Geocube can return the metadata that defined how to build the Cube, using the field headers_only of the GetCube() function. In particular, Metadata contains: URI of datasets, subdir and subbands Dataformat of the datasets Metadata can be useful to understand which datasets are retrieved and it can be passed to a Downloader service , that will download and build the cube as if the cube request is to the Geocube Server. Using Cloud-Optimized File format GDAL only reads the part of the image it needs. It results in many small reads, but not all the file is read. To optimize the access to files stored in the Cloud, the Geocube uses a LRU cache and range-request to optimize the read of images. To make the most of this capacity, the files should be in a file format that allow efficient range request, like tilled GeoTIFF, COG (Cloud-Optimize Geotiff) or MuCOG . Some file format should really be avoided, such as JPEG2000 which requires a full download and decompression to access any part of the image. If the indexed images are not in an appropriate format, they can be Consolidate .","title":"Access"},{"location":"user-guide/access/#access","text":"","title":"Access"},{"location":"user-guide/access/#get-images","text":"To retrieve data from the Geocube, the datasets or the containers are not explicitly named. The Geocube will find the datasets depending on user-provided criteria (see GetCubeRequest ): geography : using either the aoi of the record and the shape of the dataset semantic : using the tags of the record time : using the datetime of the record kind of data : using the instance These criteria can return several datasets. The Geocube will automatically do a mosaic of the results. The geocube only returned 2D rectangular arrays. Once the datasets are filtered by records and instance, the Geocube will extract a rectangular extent using the warp command of GDAL, that automatically deals with reprojection, rescaling and mosaicing . If necessary, a casting of the value is done (see Dataformat mapping ). Finally, the Geocube streams the results through the GRPC interface. If the resulting image is only no data (because there is no dataset matching the criteria or the extent does not contain data), the image is skipped and the Geocube returns nothing.","title":"Get images"},{"location":"user-guide/access/#get-a-cube","text":"A cube of data is just a list of 2D arrays from the same instance , with the same extent in the same crs . Getting an image or getting a cube is exactly the same except that, in the latter case, the request defines a collection of records . If the Geocube has been optimized to serve the timeseries as efficiently as possible (see Consolidation : - It is faster to request a timeseries than each image one by one. - It is better to request a large area (bigger or equal to the block size) than a lot of small areas.","title":"Get a cube"},{"location":"user-guide/access/#get-mosaics-from-several-records","text":"If several dataset are linked to the same record, the Geocube returns a mosaic of them. It's possible to request the Geocube to merge datasets from different records. By passing a GroupedRecords , the Geocube automatically groups the dataset as if they belong to the same records, except that the order is preserved (the datasets belonging to the last record of the group will be on the top of the mosaic). NB : To be used carefully : the edges are not blended and it may result in visible seams. To request a cube of grouped records, the request will contain a GroupedRecordsList (actually a list of list of records).","title":"Get mosaics from several records"},{"location":"user-guide/access/#get-metadata-only","text":"Instead of returning the images, the Geocube can return the metadata that defined how to build the Cube, using the field headers_only of the GetCube() function. In particular, Metadata contains: URI of datasets, subdir and subbands Dataformat of the datasets Metadata can be useful to understand which datasets are retrieved and it can be passed to a Downloader service , that will download and build the cube as if the cube request is to the Geocube Server.","title":"Get metadata only"},{"location":"user-guide/access/#using-cloud-optimized-file-format","text":"GDAL only reads the part of the image it needs. It results in many small reads, but not all the file is read. To optimize the access to files stored in the Cloud, the Geocube uses a LRU cache and range-request to optimize the read of images. To make the most of this capacity, the files should be in a file format that allow efficient range request, like tilled GeoTIFF, COG (Cloud-Optimize Geotiff) or MuCOG . Some file format should really be avoided, such as JPEG2000 which requires a full download and decompression to access any part of the image. If the indexed images are not in an appropriate format, they can be Consolidate .","title":"Using Cloud-Optimized File format"},{"location":"user-guide/consolidation/","text":"Consolidation Why ? Consolidation is the process of optimizing the data format, the projection and the tiling of the datasets to fit with the needs of the project. Depending on the depth of the timeseries that is usually needed, the size of the tiles requested, the memory requirements, etc, the datasets can be optimized to improve the speed of access to the data or the memory impact. Consolidation is not mandatory , but some applications, especially those requiring massive data and deep timeseries, may suffer from poorly formatted images. For instance, consolidation may be a game changer in the following cases (all the more, if the images are retrieved more than once - reprocessing, visualisation): if the image format is not cloud-optimized (jpeg2000, GeoTiff, ...) if the images are retrieved as timeseries if the storage has a high latency per object. if the datasets are not in the right projection or resolution if the images are retrieved with low resolutions (creation of overviews) if the processing requires small tiles (e.g. deep learning) During the consolidation, the datasets will be tiled, reprojected, casted and merged into files optimized for the needs of the user. Consolidation parameters The consolidation parameters that describe the data format of the optimized datasets are linked to a variable: Internal Dataformat Exponent for the mapping between internal dataformat and variable.dformat (see formula below) Resampling algorithm used for reprojection and overviews Compression of the data NB: regarding the mapping of the dataformat , for the consolidation process, the MinOut/MaxOut are the Min/Max of the variable. The consolidation parameters of a variable are configured with ConfigConsolidation() . A call to ConfigConsolidation() will update the consolidation parameters of the variable and it will only affect the future consolidations. Layout The datasets will be tiled, reprojected and stacked on a grid defined by a Layout . The layout has external parameters that define the grid: - grid_parameters : dict of parameters, containing at least a grid type (actually only singlecell and regular are available) - grid_flags : list of flags and internal parameters that define the internal tiling and the depth of the stacking: - block_shape - max_records per file - Creation of Overviews - interlacing_pattern that describes how to interlace the Records, the Bands, the Zooms level/overview and the Tiles (geotiff blocks). See Layout() The layout must be carefully defined depending on the performance expected in terms of access. The size of the cell of the grid multiplied by the maximum number of the records and the datatype will give the maximum size of the final files. The grid can be a single-cell grid : at the beginning of the consolidation, the aoi of all the datasets will be projected and merged in the given crs. The bounds of this aoi give the size of the cell. Single-cell grid can be used to consolidate a bunch of already aligned records, like Sentinel-2 granules. Be careful with Single-cell Layout as the merged aoi may be very large and caused memory errors. Consolidation As a GetCube request, the Consolidation request requires a list of records and an instance of a variable to select the datasets that will be consolidated. Then, the consolidater defines the containers that will be created from the layout : each cell of the grid of the layout intersecting at least one dataset will result in a new file/container. Finally, the Consolidation Parameters that are linked to the variable is used to define how to create the container from the datasets (dataformat, overviews...). The Consolidate() function will create an asynchronous consolidation job . Below are described all the state of the consolidation. NEW During this step, the datasets are selected, regarding the records and the instance , and locked to prevent an other job from modifying them. The job is created in the database. Action Effect NewStatus ForceRetry NEW Cancel Only if job is waiting (STEP_BY_STEP_ALL) ABORTED ForceCancel Rollback ABORTED CREATED For each cell of the grid of the layout, the list of the datasets intersecting the cell is retrieved. This list is inspected to determine whether or not a consolidation is required (the dataset might already be consolidated on the same layout). If a consolidation is needed, a consolidation task is created. Action Effect NewStatus ForceRetry CREATED Cancel If job is waiting (STEP_BY_STEP_ALL) ABORTED ForceCancel Rollback ABORTED CONSOLIDATION IN PROGRESS The consolidation tasks are sent to the consolidation workers, that get the cube of data intersecting the cell and format it in a new MuCOG. Action Effect NewStatus Retry Retry the FAILED tasks (the tasks that finish during the retrying may be ignored. Use with cautious. CONSOLIDATIONRETRYING ForceRetry Retry the FAILED and the PENDING tasks (should only be used if the processing is stuck and consolidation workers are deleted) CONSOLIDATIONFORCERETRYING Cancel, ForceCancel Cancel the PENDING tasks and the job (the workers will automatically stop as soon as possible) CONSOLIDATIONCANCELLING CONSOLIDATION DONE The new datasets are indexed in the Geocube, but there are still inactive (they cannot be retrieved by the user) Action Effect NewStatus ForceRetry CONSOLIDATIONDONE Cancel If job is waiting ABORTED ForceCancel Rollback ABORTED CONSOLIDATION INDEXED The old datasets are inactivated and the new datasets are activated (they can be retrieved by the user with no loss of service). Action Effect NewStatus ForceRetry CONSOLIDATIONINDEXED Cancel If job is waiting ABORTED ForceCancel Rollback ABORTED CONSOLIDATION EFFECTIVE The old datasets are removed from the database and if necessary, a deletion job is created to physically delete the containers. Action Effect NewStatus ForceRetry CONSOLIDATIONEFFECTIVE DONE Consolidation is finished with success. INITIALISATION FAILED Something goes wrong with the initialisation. The job is aborted Action Effect NewStatus Retry, ForceRetry CREATED Cancel, ForceCancel Rollback ABORTED CONSOLIDATION FAILED At least one consolidation tasks failed. The job can be retried or cancelled. Action Effect NewStatus Retry, ForceRetry Restart FAILED tasks CONSOLIDATINRETRYING Cancel, ForceCancel Rollback ABORTED","title":"Consolidation"},{"location":"user-guide/consolidation/#consolidation","text":"","title":"Consolidation"},{"location":"user-guide/consolidation/#why","text":"Consolidation is the process of optimizing the data format, the projection and the tiling of the datasets to fit with the needs of the project. Depending on the depth of the timeseries that is usually needed, the size of the tiles requested, the memory requirements, etc, the datasets can be optimized to improve the speed of access to the data or the memory impact. Consolidation is not mandatory , but some applications, especially those requiring massive data and deep timeseries, may suffer from poorly formatted images. For instance, consolidation may be a game changer in the following cases (all the more, if the images are retrieved more than once - reprocessing, visualisation): if the image format is not cloud-optimized (jpeg2000, GeoTiff, ...) if the images are retrieved as timeseries if the storage has a high latency per object. if the datasets are not in the right projection or resolution if the images are retrieved with low resolutions (creation of overviews) if the processing requires small tiles (e.g. deep learning) During the consolidation, the datasets will be tiled, reprojected, casted and merged into files optimized for the needs of the user.","title":"Why ?"},{"location":"user-guide/consolidation/#consolidation-parameters","text":"The consolidation parameters that describe the data format of the optimized datasets are linked to a variable: Internal Dataformat Exponent for the mapping between internal dataformat and variable.dformat (see formula below) Resampling algorithm used for reprojection and overviews Compression of the data NB: regarding the mapping of the dataformat , for the consolidation process, the MinOut/MaxOut are the Min/Max of the variable. The consolidation parameters of a variable are configured with ConfigConsolidation() . A call to ConfigConsolidation() will update the consolidation parameters of the variable and it will only affect the future consolidations.","title":"Consolidation parameters"},{"location":"user-guide/consolidation/#layout","text":"The datasets will be tiled, reprojected and stacked on a grid defined by a Layout . The layout has external parameters that define the grid: - grid_parameters : dict of parameters, containing at least a grid type (actually only singlecell and regular are available) - grid_flags : list of flags and internal parameters that define the internal tiling and the depth of the stacking: - block_shape - max_records per file - Creation of Overviews - interlacing_pattern that describes how to interlace the Records, the Bands, the Zooms level/overview and the Tiles (geotiff blocks). See Layout() The layout must be carefully defined depending on the performance expected in terms of access. The size of the cell of the grid multiplied by the maximum number of the records and the datatype will give the maximum size of the final files. The grid can be a single-cell grid : at the beginning of the consolidation, the aoi of all the datasets will be projected and merged in the given crs. The bounds of this aoi give the size of the cell. Single-cell grid can be used to consolidate a bunch of already aligned records, like Sentinel-2 granules. Be careful with Single-cell Layout as the merged aoi may be very large and caused memory errors.","title":"Layout"},{"location":"user-guide/consolidation/#consolidation_1","text":"As a GetCube request, the Consolidation request requires a list of records and an instance of a variable to select the datasets that will be consolidated. Then, the consolidater defines the containers that will be created from the layout : each cell of the grid of the layout intersecting at least one dataset will result in a new file/container. Finally, the Consolidation Parameters that are linked to the variable is used to define how to create the container from the datasets (dataformat, overviews...). The Consolidate() function will create an asynchronous consolidation job . Below are described all the state of the consolidation.","title":"Consolidation"},{"location":"user-guide/consolidation/#new","text":"During this step, the datasets are selected, regarding the records and the instance , and locked to prevent an other job from modifying them. The job is created in the database. Action Effect NewStatus ForceRetry NEW Cancel Only if job is waiting (STEP_BY_STEP_ALL) ABORTED ForceCancel Rollback ABORTED","title":"NEW"},{"location":"user-guide/consolidation/#created","text":"For each cell of the grid of the layout, the list of the datasets intersecting the cell is retrieved. This list is inspected to determine whether or not a consolidation is required (the dataset might already be consolidated on the same layout). If a consolidation is needed, a consolidation task is created. Action Effect NewStatus ForceRetry CREATED Cancel If job is waiting (STEP_BY_STEP_ALL) ABORTED ForceCancel Rollback ABORTED","title":"CREATED"},{"location":"user-guide/consolidation/#consolidation-in-progress","text":"The consolidation tasks are sent to the consolidation workers, that get the cube of data intersecting the cell and format it in a new MuCOG. Action Effect NewStatus Retry Retry the FAILED tasks (the tasks that finish during the retrying may be ignored. Use with cautious. CONSOLIDATIONRETRYING ForceRetry Retry the FAILED and the PENDING tasks (should only be used if the processing is stuck and consolidation workers are deleted) CONSOLIDATIONFORCERETRYING Cancel, ForceCancel Cancel the PENDING tasks and the job (the workers will automatically stop as soon as possible) CONSOLIDATIONCANCELLING","title":"CONSOLIDATION IN PROGRESS"},{"location":"user-guide/consolidation/#consolidation-done","text":"The new datasets are indexed in the Geocube, but there are still inactive (they cannot be retrieved by the user) Action Effect NewStatus ForceRetry CONSOLIDATIONDONE Cancel If job is waiting ABORTED ForceCancel Rollback ABORTED","title":"CONSOLIDATION DONE"},{"location":"user-guide/consolidation/#consolidation-indexed","text":"The old datasets are inactivated and the new datasets are activated (they can be retrieved by the user with no loss of service). Action Effect NewStatus ForceRetry CONSOLIDATIONINDEXED Cancel If job is waiting ABORTED ForceCancel Rollback ABORTED","title":"CONSOLIDATION INDEXED"},{"location":"user-guide/consolidation/#consolidation-effective","text":"The old datasets are removed from the database and if necessary, a deletion job is created to physically delete the containers. Action Effect NewStatus ForceRetry CONSOLIDATIONEFFECTIVE","title":"CONSOLIDATION EFFECTIVE"},{"location":"user-guide/consolidation/#done","text":"Consolidation is finished with success.","title":"DONE"},{"location":"user-guide/consolidation/#initialisation-failed","text":"Something goes wrong with the initialisation. The job is aborted Action Effect NewStatus Retry, ForceRetry CREATED Cancel, ForceCancel Rollback ABORTED","title":"INITIALISATION FAILED"},{"location":"user-guide/consolidation/#consolidation-failed","text":"At least one consolidation tasks failed. The job can be retried or cancelled. Action Effect NewStatus Retry, ForceRetry Restart FAILED tasks CONSOLIDATINRETRYING Cancel, ForceCancel Rollback ABORTED","title":"CONSOLIDATION FAILED"},{"location":"user-guide/deletion/","text":"Deletion Datasets, and underlying files (if they are marked as \"managed\" by the Geocube), can be deleted from the Geocube. Datasets to be deleted are selected by records , instances and/or container uri/filename. The DeleteDatasets() function will create a deletion job that can be asynchronous or synchronous. Below are described all the state of the deletion. NEW During this step, the datasets are selected, regarding the records and the instances , and locked to prevent an other job from modifying them. The job is created in the database. Action Effect NewStatus ForceRetry NEW Cancel Only if job is waiting (STEP_BY_STEP_ALL) ABORTED ForceCancel Rollback ABORTED CREATED The datasets are inactivated: they cannot be returned by GetCube/GetMetadata calls. Action Effect NewStatus ForceRetry CREATED Cancel If job is waiting (STEP_BY_STEP_ALL) ABORTED ForceCancel Rollback ABORTED DELETION IN PROGRESS The dataset are removed from the database. The containers that are not referenced by at least one dataset in the Geocube are also removed and if they were managed by the Geocube (meaning: the Geocube is responsible for their lifecycle), a deletion task is created. Action Effect NewStatus ForceRetry DELETIONINPROGRESS Cancel If job is waiting (STEP_BY_STEP_ALL) ABORTED ForceCancel Rollback ABORTED DELETION EFFECTIVE The deletion tasks are processed : the Geocube deletes the file referenced by the container for each deletion task. It may be a long process and has many reasons to fail. The tasks that are successful are removed from the job, remaining the failed tasks, so that they can be retried. Action Effect NewStatus ForceRetry Retry the failed tasks DELETIONEFFECTIVE ForceCancel Mark the job as cancelled, but do not stop the pending deletion tasks. The files already deleted are not restored and some files can be deleted after the job is cancelled ABORTED DONE Deletion is finished with success. DONE BUT UNTIDY Something goes wrong with the deletion (usually, at least one task has failed). The Geocube is clean, but there are some files that the Geocube is responsible for that still exist, but should be deleted. Action Effect NewStatus ForceRetry Retry all the deletion tasks that are not sucessful DELETIONEFFECTIVE","title":"Deletion"},{"location":"user-guide/deletion/#deletion","text":"Datasets, and underlying files (if they are marked as \"managed\" by the Geocube), can be deleted from the Geocube. Datasets to be deleted are selected by records , instances and/or container uri/filename. The DeleteDatasets() function will create a deletion job that can be asynchronous or synchronous. Below are described all the state of the deletion.","title":"Deletion"},{"location":"user-guide/deletion/#new","text":"During this step, the datasets are selected, regarding the records and the instances , and locked to prevent an other job from modifying them. The job is created in the database. Action Effect NewStatus ForceRetry NEW Cancel Only if job is waiting (STEP_BY_STEP_ALL) ABORTED ForceCancel Rollback ABORTED","title":"NEW"},{"location":"user-guide/deletion/#created","text":"The datasets are inactivated: they cannot be returned by GetCube/GetMetadata calls. Action Effect NewStatus ForceRetry CREATED Cancel If job is waiting (STEP_BY_STEP_ALL) ABORTED ForceCancel Rollback ABORTED","title":"CREATED"},{"location":"user-guide/deletion/#deletion-in-progress","text":"The dataset are removed from the database. The containers that are not referenced by at least one dataset in the Geocube are also removed and if they were managed by the Geocube (meaning: the Geocube is responsible for their lifecycle), a deletion task is created. Action Effect NewStatus ForceRetry DELETIONINPROGRESS Cancel If job is waiting (STEP_BY_STEP_ALL) ABORTED ForceCancel Rollback ABORTED","title":"DELETION IN PROGRESS"},{"location":"user-guide/deletion/#deletion-effective","text":"The deletion tasks are processed : the Geocube deletes the file referenced by the container for each deletion task. It may be a long process and has many reasons to fail. The tasks that are successful are removed from the job, remaining the failed tasks, so that they can be retried. Action Effect NewStatus ForceRetry Retry the failed tasks DELETIONEFFECTIVE ForceCancel Mark the job as cancelled, but do not stop the pending deletion tasks. The files already deleted are not restored and some files can be deleted after the job is cancelled ABORTED","title":"DELETION EFFECTIVE"},{"location":"user-guide/deletion/#done","text":"Deletion is finished with success.","title":"DONE"},{"location":"user-guide/deletion/#done-but-untidy","text":"Something goes wrong with the deletion (usually, at least one task has failed). The Geocube is clean, but there are some files that the Geocube is responsible for that still exist, but should be deleted. Action Effect NewStatus ForceRetry Retry all the deletion tasks that are not sucessful DELETIONEFFECTIVE","title":"DONE BUT UNTIDY"},{"location":"user-guide/entities/","text":"Entities In the Geocube, an image (a dataset ) is indexed by a record and an instance of a variable . In short, a record defines the data-take and the variable defines the kind of data. Record A record defines a data-take by its geometry, its sensing time and user-defined tags that describe the context in more detail. A record is usually linked to an image of a satellite. For example, the image taken by Sentinel-2A over the 31TDL MGRS tile on the 1st of April 2018 is described by the record: Name : S2A_MSIL1C_20180401T105031_N0206_R051_T31TDL_20180401T144530 AOI : 31TDL tile (POLYGON ((2.6859855651855 45.680294036865, 2.811126708984324 45.680294036865, 2.811126708984324 45.59909820556617, 2.6859855651855 45.59909820556617, 2.6859855651855 45.680294036865))) DateTime : 2018-04-01 10:50:31 Tags : [ CONSTELLATION : \"SENTINEL2\"] [...] But a record can describe any product like a mosaic over a country, a classification map... : Name : Mosaic of France January 2020 AOI : France DateTime : 2020-01-31 00:00:00 Tags : [ PROCESSOR : Software used to process the mosaic] [...] Records are uniquely defined by : its name its tags : user-defined depending on the project (currently, no standard is implemented). its date So, it's possible to have two records with the same name, but with different tags. AOI A record is linked to an AOI in geographic coordinates. The AOI can be shared between several records. It is used to filter records by localisation. Variable A variable describes the kind of data stored in a product, for example a spectral band, NDVI, RGB, backscatter, classification ... It stores the information needed to describe , process and visualize the product. In particular, the variable describes a data format with: a data-type ( int , float , complex , byte ...): what type of data this variable describes ? a minimum value, or if the variable is unbound, a lower reference (see ) a maximum value, or if the variable is unbound, a upper reference a no-data value This dataformat is used to render images. A request of images will cast the data according to this format (see Dataset dataformat ) Instance An instance is a declination of a variable with different processing parameters. For example, an RGB variable can be defined with different spectral bands (RGB bands of Sentinel-2 are not the same as LANDSAT's), a Label variable can have a different mapping, the SAR products can be processed with different processing graphs or software, but, in each example, the declination belongs to the same variable. Instances have user-defined metadata that can be used to describe them. For example, the metadata can describe: The processing parameters used to compute the images indexed with this instance A description of the classes or the number of the classes for a Label variable. The resolution ... client.variable(\"RGB\").instantiate(name=\"Sentinel2-Raw-Bands\", metadata={\"R\":\"664.6\", \"G\":...}) client.variable(\"LandUseLabels\").instantiate(name=\"v1\", metadata={\"0\":\"Undefined\",\"1\":\"Urban\", \"2\":...}) client.variable(\"Sigma0VV\").instantiate(name=\"terrain-corrected\", metadata={\"snap_graph_name\":\"mygraph.xml\", ...}) Palette For color rendering, a variable can defined a palette. A palette is described by a set of values in [0, 255] and its corresponding RGB-points. All the values that are not declared are linearly interpolated. Container A container describes a file image, with its full path, its storage class (when the image is stored in an object storage) and a flag to tell whether the Geocube is responsible for the life-cycle of this file (in other words, should the Geocube delete the file or not if it's dereferenced ?). A container can contain one or several dataset . Dataset A dataset is defined by the following information: which band(s) of the container are indexed (usually all the bands, but it can be a subset of the bands of the image, e.g. only one band of a 3-bands images) the dataformat of the pixels (datatype, min, max and no_data value) how to map its pixel values to the dataformat of the variable (min->minOut, max->maxOut and exponent) A dataset is indexed by a record (to find it semantically, geographically and temporally) and the instance of a variable (to find it by type of data). Dataformat and mapping In the Geocube Database, the (internal) data format of an dataset can be different from the data format of the variable that describes the image (for example: in order to optimize storage costs, the image may be stored in byte whereas the variable describes a float value between 0 and 1). When the data is retrieved, the Geocube maps the internal format of the dataset to the data format of the variable. This process may map the data below the minimum or above the maximum value. In that case, no crop is performed. Usually, the min value of the dataformat of the dataset will be mapped to the min value of the dataformat of the variable, and the same for max (in that case, minout and maxout are equals to min and max of the dataformat of the variable). In some cases, it could be useful to map the min/max values of the dataformat of the dataset to other values than the min/max of the variable. In that case, the minout/maxout are defined differently from the min/max of the variable, and min/max value of the dataformat of the dataset will be mapped to the minout/maxout . The generic case is described here : NB: dataset.min and dataset.max are NOT necessarily the minimum and maximum values of the pixels but the values that map the min and max of the variable. DataFormat is provided for the interpretation of the image. There is no check during indexation that the min, max and no_data value are the right ones. Job A modification of the life-cycle of the datasets and the underlying images is a process described by a state-machine, that can be run step-by-step, cancelled, retried, ... A job is an entity that describes this running process, and especially the current state in the whole state machine. A job has, in particular : a name a type of action (Consolidation, Deletion, Moving...) a status to decribe the progress, the current state (Initialisation, In progress, Done, Effective, Failed, Cancelled, Aborted...) a payload to describe what to do a list of datasets that are locked during the process a list of subtasks logs and other information to monitore the processus A job can be executed in many ways: Step-by-step: the state-machine will stop after the state changes and before executing the actions linked to this state. It's useful to monitore the job. Three levels exist: stop at all states, only major states or only critical states. Synchronous: the state-machine will process all the states in a row (the execution returns when the job is finished) Asynchronous: same as Synchronous but in background (the execution returns immediately, the job continue in background) Layout A layout describes how the data is spatially organised. It is composed of an external layout (typically a grid of cells) and an internal layout describing the data in each cell (size of the blocks of data and depth). A layout is used: to optimize the storage of the datasets (see Consolidation ). Each cell of the grid is a file containing several datasets (max depth) that are tiled according to the block size. to tile an AOI and divide a processing into multiple subtasks Grid A grid is a set of cells with a name and a description. Each cell is a polygon in a given CRS with a unique ID in a given grid. For example, Three type of grids are currently available in the Geocube: Regular grid: all the cells have the same rectangular size and pave the AOI in a unique CRS. Single cell grid: a grid of one rectangular cell in a given CRS. The bounds of this cell is defined on the fly, depending on the usage. Customer-defined grid: each cell is defined independently from the others, with a LinearRing and a CRS, and eventually, an internal regular grid. The MGRS can be described by a grid .","title":"Entities"},{"location":"user-guide/entities/#entities","text":"In the Geocube, an image (a dataset ) is indexed by a record and an instance of a variable . In short, a record defines the data-take and the variable defines the kind of data.","title":"Entities"},{"location":"user-guide/entities/#record","text":"A record defines a data-take by its geometry, its sensing time and user-defined tags that describe the context in more detail. A record is usually linked to an image of a satellite. For example, the image taken by Sentinel-2A over the 31TDL MGRS tile on the 1st of April 2018 is described by the record: Name : S2A_MSIL1C_20180401T105031_N0206_R051_T31TDL_20180401T144530 AOI : 31TDL tile (POLYGON ((2.6859855651855 45.680294036865, 2.811126708984324 45.680294036865, 2.811126708984324 45.59909820556617, 2.6859855651855 45.59909820556617, 2.6859855651855 45.680294036865))) DateTime : 2018-04-01 10:50:31 Tags : [ CONSTELLATION : \"SENTINEL2\"] [...] But a record can describe any product like a mosaic over a country, a classification map... : Name : Mosaic of France January 2020 AOI : France DateTime : 2020-01-31 00:00:00 Tags : [ PROCESSOR : Software used to process the mosaic] [...] Records are uniquely defined by : its name its tags : user-defined depending on the project (currently, no standard is implemented). its date So, it's possible to have two records with the same name, but with different tags.","title":"Record"},{"location":"user-guide/entities/#aoi","text":"A record is linked to an AOI in geographic coordinates. The AOI can be shared between several records. It is used to filter records by localisation.","title":"AOI"},{"location":"user-guide/entities/#variable","text":"A variable describes the kind of data stored in a product, for example a spectral band, NDVI, RGB, backscatter, classification ... It stores the information needed to describe , process and visualize the product. In particular, the variable describes a data format with: a data-type ( int , float , complex , byte ...): what type of data this variable describes ? a minimum value, or if the variable is unbound, a lower reference (see ) a maximum value, or if the variable is unbound, a upper reference a no-data value This dataformat is used to render images. A request of images will cast the data according to this format (see Dataset dataformat )","title":"Variable"},{"location":"user-guide/entities/#instance","text":"An instance is a declination of a variable with different processing parameters. For example, an RGB variable can be defined with different spectral bands (RGB bands of Sentinel-2 are not the same as LANDSAT's), a Label variable can have a different mapping, the SAR products can be processed with different processing graphs or software, but, in each example, the declination belongs to the same variable. Instances have user-defined metadata that can be used to describe them. For example, the metadata can describe: The processing parameters used to compute the images indexed with this instance A description of the classes or the number of the classes for a Label variable. The resolution ... client.variable(\"RGB\").instantiate(name=\"Sentinel2-Raw-Bands\", metadata={\"R\":\"664.6\", \"G\":...}) client.variable(\"LandUseLabels\").instantiate(name=\"v1\", metadata={\"0\":\"Undefined\",\"1\":\"Urban\", \"2\":...}) client.variable(\"Sigma0VV\").instantiate(name=\"terrain-corrected\", metadata={\"snap_graph_name\":\"mygraph.xml\", ...})","title":"Instance"},{"location":"user-guide/entities/#palette","text":"For color rendering, a variable can defined a palette. A palette is described by a set of values in [0, 255] and its corresponding RGB-points. All the values that are not declared are linearly interpolated.","title":"Palette"},{"location":"user-guide/entities/#container","text":"A container describes a file image, with its full path, its storage class (when the image is stored in an object storage) and a flag to tell whether the Geocube is responsible for the life-cycle of this file (in other words, should the Geocube delete the file or not if it's dereferenced ?). A container can contain one or several dataset .","title":"Container"},{"location":"user-guide/entities/#dataset","text":"A dataset is defined by the following information: which band(s) of the container are indexed (usually all the bands, but it can be a subset of the bands of the image, e.g. only one band of a 3-bands images) the dataformat of the pixels (datatype, min, max and no_data value) how to map its pixel values to the dataformat of the variable (min->minOut, max->maxOut and exponent) A dataset is indexed by a record (to find it semantically, geographically and temporally) and the instance of a variable (to find it by type of data).","title":"Dataset"},{"location":"user-guide/entities/#dataformat-and-mapping","text":"In the Geocube Database, the (internal) data format of an dataset can be different from the data format of the variable that describes the image (for example: in order to optimize storage costs, the image may be stored in byte whereas the variable describes a float value between 0 and 1). When the data is retrieved, the Geocube maps the internal format of the dataset to the data format of the variable. This process may map the data below the minimum or above the maximum value. In that case, no crop is performed. Usually, the min value of the dataformat of the dataset will be mapped to the min value of the dataformat of the variable, and the same for max (in that case, minout and maxout are equals to min and max of the dataformat of the variable). In some cases, it could be useful to map the min/max values of the dataformat of the dataset to other values than the min/max of the variable. In that case, the minout/maxout are defined differently from the min/max of the variable, and min/max value of the dataformat of the dataset will be mapped to the minout/maxout . The generic case is described here : NB: dataset.min and dataset.max are NOT necessarily the minimum and maximum values of the pixels but the values that map the min and max of the variable. DataFormat is provided for the interpretation of the image. There is no check during indexation that the min, max and no_data value are the right ones.","title":"Dataformat and mapping"},{"location":"user-guide/entities/#job","text":"A modification of the life-cycle of the datasets and the underlying images is a process described by a state-machine, that can be run step-by-step, cancelled, retried, ... A job is an entity that describes this running process, and especially the current state in the whole state machine. A job has, in particular : a name a type of action (Consolidation, Deletion, Moving...) a status to decribe the progress, the current state (Initialisation, In progress, Done, Effective, Failed, Cancelled, Aborted...) a payload to describe what to do a list of datasets that are locked during the process a list of subtasks logs and other information to monitore the processus A job can be executed in many ways: Step-by-step: the state-machine will stop after the state changes and before executing the actions linked to this state. It's useful to monitore the job. Three levels exist: stop at all states, only major states or only critical states. Synchronous: the state-machine will process all the states in a row (the execution returns when the job is finished) Asynchronous: same as Synchronous but in background (the execution returns immediately, the job continue in background)","title":"Job"},{"location":"user-guide/entities/#layout","text":"A layout describes how the data is spatially organised. It is composed of an external layout (typically a grid of cells) and an internal layout describing the data in each cell (size of the blocks of data and depth). A layout is used: to optimize the storage of the datasets (see Consolidation ). Each cell of the grid is a file containing several datasets (max depth) that are tiled according to the block size. to tile an AOI and divide a processing into multiple subtasks","title":"Layout"},{"location":"user-guide/entities/#grid","text":"A grid is a set of cells with a name and a description. Each cell is a polygon in a given CRS with a unique ID in a given grid. For example, Three type of grids are currently available in the Geocube: Regular grid: all the cells have the same rectangular size and pave the AOI in a unique CRS. Single cell grid: a grid of one rectangular cell in a given CRS. The bounds of this cell is defined on the fly, depending on the usage. Customer-defined grid: each cell is defined independently from the others, with a LinearRing and a CRS, and eventually, an internal regular grid. The MGRS can be described by a grid .","title":"Grid"},{"location":"user-guide/grpc/","text":"Protocol Documentation Table of Contents pb/geocube.proto Geocube pb/geocubeDownloader.proto GeocubeDownloader pb/admin.proto TidyDBRequest TidyDBResponse UpdateDatasetsRequest UpdateDatasetsResponse UpdateDatasetsResponse.ResultsEntry Admin pb/records.proto AOI AddRecordsTagsRequest AddRecordsTagsRequest.TagsEntry AddRecordsTagsResponse Coord CreateAOIRequest CreateAOIResponse CreateRecordsRequest CreateRecordsResponse DeleteRecordsRequest DeleteRecordsResponse GetAOIRequest GetAOIResponse GetRecordsRequest GetRecordsResponseItem GroupedRecordIds GroupedRecordIdsList GroupedRecords LinearRing ListRecordsRequest ListRecordsRequest.TagsEntry ListRecordsResponseItem NewRecord NewRecord.TagsEntry Polygon Record Record.TagsEntry RecordFilters RecordFilters.TagsEntry RecordFiltersWithAOI RecordIdList RemoveRecordsTagsRequest RemoveRecordsTagsResponse pb/variables.proto CreatePaletteRequest CreatePaletteResponse CreateVariableRequest CreateVariableResponse DeleteInstanceRequest DeleteInstanceResponse DeleteVariableRequest DeleteVariableResponse GetVariableRequest GetVariableResponse Instance Instance.MetadataEntry InstantiateVariableRequest InstantiateVariableRequest.InstanceMetadataEntry InstantiateVariableResponse ListVariablesRequest ListVariablesResponseItem Palette UpdateInstanceRequest UpdateInstanceRequest.AddMetadataEntry UpdateInstanceResponse UpdateVariableRequest UpdateVariableResponse Variable colorPoint Resampling pb/dataformat.proto DataFormat DataFormat.Dtype pb/catalog.proto GetCubeMetadataRequest GetCubeMetadataResponse GetCubeRequest GetCubeResponse GetCubeResponseHeader GetTileRequest GetTileResponse ImageChunk ImageFile ImageHeader ListDatasetsRequest ListDatasetsResponse Shape ByteOrder FileFormat pb/layouts.proto Cell CreateGridRequest CreateGridResponse CreateLayoutRequest CreateLayoutResponse DeleteGridRequest DeleteGridResponse DeleteLayoutRequest DeleteLayoutResponse FindContainerLayoutsRequest FindContainerLayoutsResponse GeoTransform Grid Layout Layout.GridParametersEntry ListGridsRequest ListGridsResponse ListLayoutsRequest ListLayoutsResponse Size Tile TileAOIRequest TileAOIResponse pb/operations.proto CancelJobRequest CancelJobResponse CleanJobsRequest CleanJobsResponse ConfigConsolidationRequest ConfigConsolidationResponse ConsolidateRequest ConsolidateResponse ConsolidationParams ConsolidationParams.CreationParamsEntry Container ContinueJobRequest ContinueJobResponse Dataset DeleteDatasetsRequest DeleteDatasetsResponse GetConsolidationParamsRequest GetConsolidationParamsResponse GetContainersRequest GetContainersResponse GetJobRequest GetJobResponse IndexDatasetsRequest IndexDatasetsResponse Job ListJobsRequest ListJobsResponse RetryJobRequest RetryJobResponse ConsolidationParams.Compression ExecutionLevel StorageClass pb/datasetMeta.proto DatasetMeta InternalMeta pb/version.proto GetVersionRequest GetVersionResponse Scalar Value Types Top pb/geocube.proto Geocube API Documentation may be detailed in Request/Response sections. Method Name Request Type Response Type Description CreateRecords CreateRecordsRequest CreateRecordsResponse Create one or a list of records GetRecords GetRecordsRequest GetRecordsResponseItem stream Get records from their ID ListRecords ListRecordsRequest ListRecordsResponseItem stream List records given criterias AddRecordsTags AddRecordsTagsRequest AddRecordsTagsResponse Update records, adding or updating tags RemoveRecordsTags RemoveRecordsTagsRequest RemoveRecordsTagsResponse Update records, removing tags DeleteRecords DeleteRecordsRequest DeleteRecordsResponse Delete records CreateAOI CreateAOIRequest CreateAOIResponse Create an AOI if not exists or returns the aoi id of the aoi. GetAOI GetAOIRequest GetAOIResponse Get an AOI from its ID CreateVariable CreateVariableRequest CreateVariableResponse Create a variable GetVariable GetVariableRequest GetVariableResponse Get a variable given its id, name or one of its instance id UpdateVariable UpdateVariableRequest UpdateVariableResponse Update some fields of a variable DeleteVariable DeleteVariableRequest DeleteVariableResponse Delete a variable iif no dataset has a reference on ListVariables ListVariablesRequest ListVariablesResponseItem stream List variables given a name pattern InstantiateVariable InstantiateVariableRequest InstantiateVariableResponse Instantiate a variable UpdateInstance UpdateInstanceRequest UpdateInstanceResponse Update metadata of an instance DeleteInstance DeleteInstanceRequest DeleteInstanceResponse Delete an instance iif no dataset has a reference on CreatePalette CreatePaletteRequest CreatePaletteResponse Create or update a palette that can be used to create a display of a dataset GetContainers GetContainersRequest GetContainersResponse GetInfo on containers IndexDatasets IndexDatasetsRequest IndexDatasetsResponse Index new datasets in the Geocube ListDatasets ListDatasetsRequest ListDatasetsResponse List datasets from the Geocube DeleteDatasets DeleteDatasetsRequest DeleteDatasetsResponse Delete datasets using records, instances and/or filepath ConfigConsolidation ConfigConsolidationRequest ConfigConsolidationResponse Configurate a consolidation process GetConsolidationParams GetConsolidationParamsRequest GetConsolidationParamsResponse Get the configuration of a consolidation Consolidate ConsolidateRequest ConsolidateResponse Start a consolidation job ListJobs ListJobsRequest ListJobsResponse List the jobs given a name pattern GetJob GetJobRequest GetJobResponse Get a job given its name CleanJobs CleanJobsRequest CleanJobsResponse Delete jobs given their status RetryJob RetryJobRequest RetryJobResponse Retry a job CancelJob CancelJobRequest CancelJobResponse Cancel a job ContinueJob ContinueJobRequest ContinueJobResponse Continue a job that is in waiting state GetCube GetCubeRequest GetCubeResponse stream Get a cube of data given a CubeParams GetXYZTile GetTileRequest GetTileResponse Get a XYZTile (can be used with a TileServer, provided a GRPCGateway is up) CreateLayout CreateLayoutRequest CreateLayoutResponse Create a layout to be used for tiling or consolidation DeleteLayout DeleteLayoutRequest DeleteLayoutResponse Delete a layout given its name ListLayouts ListLayoutsRequest ListLayoutsResponse List layouts given a name pattern FindContainerLayouts FindContainerLayoutsRequest FindContainerLayoutsResponse stream Find all the layouts known for a set of containers TileAOI TileAOIRequest TileAOIResponse stream Tile an AOI given a layout CreateGrid CreateGridRequest stream CreateGridResponse Create a grid that can be used to tile an AOI DeleteGrid DeleteGridRequest DeleteGridResponse Delete a grid ListGrids ListGridsRequest ListGridsResponse List grids given a name pattern Version GetVersionRequest GetVersionResponse Version of the GeocubeServer Top pb/geocubeDownloader.proto GeocubeDownloader API GeocubeDownloader to download a cube from metadata Method Name Request Type Response Type Description DownloadCube GetCubeMetadataRequest GetCubeMetadataResponse stream Request cube using metadatas returned by a call to Geocube.GetCube() Version GetVersionRequest GetVersionResponse Version of the GeocubeDownloader Top pb/admin.proto TidyDBRequest Request to remove from the database all the pending entities (entities that are not linked to any dataset) Field Type Label Description Simulate bool If true, a simulation is done, nothing is actually deleted PendingAOIs bool Remove AOIs that are not linked to any Records PendingRecords bool Remove Records that do not reference any Datasets PendingVariables bool Remove Variables that have not any instances PendingInstances bool Remove Instances that do not reference any Datasets PendingContainers bool Remove Containers that do not contain any Datasets PendingParams bool Remove ConsolidationParams that are not linked to any Variable or Job TidyDBResponse Return the number of entities that were deleted (or should have been deleted if Simulate=True) Field Type Label Description NbAOIs int64 NbRecords int64 NbVariables int64 NbInstances int64 NbContainers int64 NbParams int64 UpdateDatasetsRequest Update fields of datasets that can be tricky Field Type Label Description simulate bool If true, a simulation is done, nothing is actually updated instance_id string Instance id that references the datasets to be updated record_ids string repeated Record ids that reference the datasets to be updated dformat DataFormat Internal data format (DType can be Undefined) real_min_value double Real min value (dformat.min_value maps to real_min_value) real_max_value double Real max value (dformat.max_value maps to real_max_value) exponent double 1: linear scaling (RealMax - RealMin) * pow( (Value - Min) / (Max - Min), Exponent) + RealMin UpdateDatasetsResponse Return the number of modifications per kind of modification Field Type Label Description results UpdateDatasetsResponse.ResultsEntry repeated UpdateDatasetsResponse.ResultsEntry Field Type Label Description key string value int64 Admin Service providing some functions to update or clean the database Must be used cautiously because there is no control neither possible rollback Method Name Request Type Response Type Description TidyDB TidyDBRequest TidyDBResponse UpdateDatasets UpdateDatasetsRequest UpdateDatasetsResponse DeleteDatasets DeleteDatasetsRequest DeleteDatasetsResponse Top pb/records.proto AOI Geographic AOI Field Type Label Description polygons Polygon repeated AddRecordsTagsRequest Add the given tags to a set of records Field Type Label Description ids string repeated tags AddRecordsTagsRequest.TagsEntry repeated AddRecordsTagsRequest.TagsEntry Field Type Label Description key string value string AddRecordsTagsResponse Returns the number of records impacted by the addition Field Type Label Description nb int64 Coord Geographic coordinates (4326) Field Type Label Description lon float lat float CreateAOIRequest Create a new AOI Field Type Label Description aoi AOI CreateAOIResponse Returns the ID of the AOI Field Type Label Description id string CreateRecordsRequest Create new records Field Type Label Description records NewRecord repeated CreateRecordsResponse Returns the ID of the created records Field Type Label Description ids string repeated DeleteRecordsRequest Delete records by ID Field Type Label Description ids string repeated no_fail bool If true, do not fail if some records still have datasets that refer to them and delete the others. DeleteRecordsResponse Return the number of deleted records Field Type Label Description nb int64 GetAOIRequest Request the AOI given its ID Field Type Label Description id string GetAOIResponse Returns a geometric AOI Field Type Label Description aoi AOI GetRecordsRequest Get record from its id Field Type Label Description ids string repeated GetRecordsResponseItem Return a record Field Type Label Description record Record GroupedRecordIds Record ids that are considered as a unique, merged record (e.g. all records of a given date, whatever the time of the day) Field Type Label Description ids string repeated GroupedRecordIdsList List of groupedRecordIds Field Type Label Description records GroupedRecordIds repeated GroupedRecords Records that are considered as a unique, merged record (e.g. all records of a given date, whatever the time of the day) Field Type Label Description records Record repeated LinearRing Geographic linear ring Field Type Label Description points Coord repeated ListRecordsRequest Request to find the list of records corresponding to multiple filters (inclusive) Field Type Label Description name string Name pattern (support * and ? for all or any characters and trailing (?i) for case-insensitiveness) tags ListRecordsRequest.TagsEntry repeated cf RecordFilters from_time google.protobuf.Timestamp cf RecordFilters to_time google.protobuf.Timestamp cf RecordFilters aoi AOI cf RecordFiltersWithAOI limit int32 page int32 with_aoi bool Also returns the AOI (may be big) ListRecordsRequest.TagsEntry Field Type Label Description key string value string ListRecordsResponseItem Field Type Label Description record Record NewRecord Structure to create a new record Field Type Label Description name string time google.protobuf.Timestamp tags NewRecord.TagsEntry repeated aoi_id string NewRecord.TagsEntry Field Type Label Description key string value string Polygon Geographic polygon Field Type Label Description linearrings LinearRing repeated Record Record Field Type Label Description id string name string time google.protobuf.Timestamp tags Record.TagsEntry repeated aoi_id string aoi AOI optional Record.TagsEntry Field Type Label Description key string value string RecordFilters RecordFilters defines some filters to identify records Field Type Label Description tags RecordFilters.TagsEntry repeated Tags of the records from_time google.protobuf.Timestamp Minimum date of the records to_time google.protobuf.Timestamp Maximum date of the records RecordFilters.TagsEntry Field Type Label Description key string value string RecordFiltersWithAOI RecordFiltersWithAOI defines some filters to identify records, including an AOI in geometric coordinates Field Type Label Description filters RecordFilters aoi AOI Geometric coordinates of an AOI that intersects the AOI of the records RecordIdList List of record ids that are considered separately Field Type Label Description ids string repeated RemoveRecordsTagsRequest Remove the given tags for a set of records Field Type Label Description ids string repeated tagsKey string repeated RemoveRecordsTagsResponse Returns the number of records impacted by the removal Field Type Label Description nb int64 Top pb/variables.proto CreatePaletteRequest Create a new palette or update it if already exists (provided replace=True) Field Type Label Description palette Palette Palette to be created replace bool Replace the current existing palette if exists CreatePaletteResponse Return nothing. CreateVariableRequest Define a new variable. Return an error if the name already exists. Field Type Label Description variable Variable CreateVariableResponse Return the id of the new variable. Field Type Label Description id string DeleteInstanceRequest Delete an instance Return an error if the instance is linked to datasets. Field Type Label Description id string UUID-4 of the instance to delete DeleteInstanceResponse Return nothing DeleteVariableRequest Delete a variable Return an error if the variable has still instances Field Type Label Description id string UUID-4 of the variable to delete DeleteVariableResponse Return nothing GetVariableRequest Read a variable given either its id, its name or the id of one of its instance Field Type Label Description id string UUID-4 of the variable name string Name of the variable instance_id string UUID-4 of an instance GetVariableResponse Return the variable and its instances Field Type Label Description variable Variable Instance Field Type Label Description id string Null at creation name string metadata Instance.MetadataEntry repeated Instance.MetadataEntry Field Type Label Description key string value string InstantiateVariableRequest Instantiate a variable. Return an error if the instance_name already exists for this variable. Field Type Label Description variable_id string instance_name string instance_metadata InstantiateVariableRequest.InstanceMetadataEntry repeated InstantiateVariableRequest.InstanceMetadataEntry Field Type Label Description key string value string InstantiateVariableResponse Return the new instance (its id, name and metadata) Field Type Label Description instance Instance ListVariablesRequest List variables given a name pattern Field Type Label Description name string Pattern of the name of the variable (support * and ? for all or any characters, (?i) suffix for case-insensitiveness) limit int32 Limit the number of variables returned page int32 Navigate through results (start at 0) ListVariablesResponseItem Return a stream of variables Field Type Label Description variable Variable Palette Define a palette with a name and a set of colorPoint. Maps all values in [0,1] to an RGBA value, using piecewise curve defined by colorPoints. All intermediate values are linearly interpolated. Field Type Label Description name string Name of the palette (Alpha-numerics characters, dots, dashes and underscores are supported) colors colorPoint repeated Set of colorPoints. At least two points must be defined, corresponding to value=0 and value=1. UpdateInstanceRequest Update an instance Return an error if the name is to be updated but the new name already exists. Field Type Label Description id string UUID-4 of the instance to update name google.protobuf.StringValue [Optional] New name of the variable. Empty to ignore add_metadata UpdateInstanceRequest.AddMetadataEntry repeated Pairs of metadata (key, values) to be inserted or updated del_metadata_keys string repeated Metadata keys to be deleted UpdateInstanceRequest.AddMetadataEntry Field Type Label Description key string value string UpdateInstanceResponse Return nothing UpdateVariableRequest Update the non-critical fields of a variable Return an error if the name is to be updated but the new name already exists. Field Type Label Description id string UUID-4 of the variable to update name google.protobuf.StringValue [Optional] New name of the variable. Empty to ignore unit google.protobuf.StringValue [Optional] New unit of the variable. Empty to ignore description google.protobuf.StringValue [Optional] New description of the variable. Empty to ignore palette google.protobuf.StringValue [Optional] New default palette of the variable. Empty to ignore resampling_alg Resampling [Optional] New default resampling algorithm of the variable. UNDEFINED to ignore UpdateVariableResponse Return nothing Variable Variable Field Type Label Description id string Internal UUID-4 of the variable (ignored at creation) name string Name of the variable (Alpha-numerics characters, dashs, dots and underscores) unit string Unit of the variable (for user information only) description string Description of the variable (for user information only) dformat DataFormat Format of the data. Range.Min and Range.Max are used for data mapping from internal data format of a dataset (See IndexDatasets for more details), DType and NoData are used for the outputs of GetCube. bands string repeated Name of each band. Can be empty when the variable refers to only one band, must be unique otherwise. palette string Name of the default palette for color rendering. resampling_alg Resampling Default resampling algorithm in case of reprojection. instances Instance repeated List of instances of the variable (ignored at creation) colorPoint Define a color mapping from a value [0-1] to a RGBA value. Field Type Label Description value float r uint32 g uint32 b uint32 a uint32 Resampling Resampling algorithms (supported by GDAL) Name Number Description UNDEFINED 0 NEAR 1 BILINEAR 2 CUBIC 3 CUBICSPLINE 4 LANCZOS 5 AVERAGE 6 MODE 7 MAX 8 MIN 9 MED 10 Q1 11 Q3 12 Top pb/dataformat.proto DataFormat Format of the data of a dataset. Format is defined by the type of the data, its no-data value and the range of values (its interpretation depends on the use) Field Type Label Description dtype DataFormat.Dtype Type of the data no_data double No-data value (supports any float values, including NaN) min_value double Min value (usually used to map from one min value to another) max_value double Max value (usually used to map from one min value to another) DataFormat.Dtype Type of data supported by the Geocube & GDAL Name Number Description UNDEFINED 0 UInt8 1 UInt16 2 UInt32 3 Int8 4 Int16 5 Int32 6 Float32 7 Float64 8 Complex64 9 Pair of float32 Top pb/catalog.proto GetCubeMetadataRequest Request a cube from metadatas (provided by Geocube.GetCube()) Field Type Label Description datasets_meta DatasetMeta repeated List of Metadatas needed to download and generate the slices of the cube grouped_records GroupedRecords repeated List of GroupedRecords describing the slices of the cube ref_dformat DataFormat Output dataformat resampling_alg Resampling Resampling algorithm to use for reprojection pix_to_crs GeoTransform crs string size Size format FileFormat Format of the output data predownload bool Predownload the datasets before merging them. When the dataset is remote and all the dataset is required, it is more efficient to predownload it. protocol_v11x bool For compatibility with older clients. Clients with version above 1.1.0 must set this field to true. GetCubeMetadataResponse Return either information on the cube, information on an image or a chunk of an image Field Type Label Description global_header GetCubeResponseHeader header ImageHeader chunk ImageChunk GetCubeRequest Request a cube of data Field Type Label Description records RecordIdList List of record ids requested. At least one. One image will be returned by record (if not empty) filters RecordFilters Filters to list the records that will be used to create the cube grouped_records GroupedRecordIdsList List of groups of record ids requested. At least one. One image will be returned by group of records (if not empty). All the datasets of a group of records will be merged together using the latest first. instances_id string repeated Instances of a variable defining the kind of images requested. At least one, and all must be instance of the same variable. Only one is actually supported crs string Coordinates Reference System of the output images (images will be reprojected on the fly if necessary) pix_to_crs GeoTransform GeoTransform of the requested cube (images will be rescaled on the fly if necessary) size Size Shape of the output images compression_level int32 Define a level of compression to speed up the transfer, values: -3 to 9 (-2: Huffman only, -1:default, 0->9: level of compression from the fastest to the best compression, -3: disable the compression). The data is compressed by the server and decompressed by the Client. Use -3 or -2 if the bandwidth is not limited. 0 is level 0 of DEFLATE (thus, it must be decompressed by DEFLATE even though the data is not compressed). If the client can support -3, 0 is useless. headers_only bool Only returns headers (including all metadatas on datasets) format FileFormat Format of the output images resampling_alg Resampling Resampling algorithm used for reprojecion. If undefined, the default resampling algorithm associated to the variable is used. protocol_v11x bool For compatibility with older clients. Clients with version above 1.1.0 must set this field to true. GetCubeResponse Return either information on the cube, information on an image or a chunk of an image Field Type Label Description global_header GetCubeResponseHeader header ImageHeader chunk ImageChunk GetCubeResponseHeader Return global information on the requested cube Field Type Label Description count int64 nb_datasets int64 ref_dformat DataFormat Output dataformat resampling_alg Resampling Resampling algorithm to use for reprojection geotransform GeoTransform Geotransform used for mapping crs string GetTileRequest Request a web-mercator tile, given a variable and a group of records Field Type Label Description instance_id string x int32 y int32 z int32 min float max float records GroupedRecordIds Group of record ids. At least one. All the datasets of the group of records will be merged together using the latest first. filters RecordFilters All the datasets whose records have RecordTags and time between from_time and to_time GetTileResponse Return a 256x256 png image Field Type Label Description image ImageFile ImageChunk Chunk of the full image, to handle the GRPC limit of 4Mbytes/message Field Type Label Description part int32 Index of the chunk (from 1 to ImageHeader.nb_parts-1). The first part (=0) is ImageHeader.data data bytes Chunk of the full array of bytes ImageFile ByteArray of a PNG image 256x256pixels Field Type Label Description data bytes ImageHeader Header of an image (slice of the cube) It describes the image, the underlying datasets and the way to recreate it from the array of byte : 1. Append ImageHeader.data and ImageChunk.data from part=0 to part=nb_parts-1 2. If compression=True, decompress the array of bytes using deflate 3. Cast the result to the dtype using byteOrder 4. Reshape the result Field Type Label Description shape Shape Shape of the image (widthxheight) dtype DataFormat.Dtype Type of the data (to interprete \"ImageHeader.data + ImageChunk.data\") nb_parts int32 Number of parts the image is splitted into data bytes First part of the image as an array of bytes size int64 Size of the full array of bytes order ByteOrder ByteOrder of the datatype compression bool Deflate compressed data format, described in RFC 1951 grouped_records GroupedRecords Group of records used to generate this image dataset_meta DatasetMeta All information on the underlying datasets that composed the image error string If not empty, an error occured and the image was not retrieved. ListDatasetsRequest List Datasets Field Type Label Description instance_id string Instance of a variable defining the kind of datasets requested. records RecordIdList List of record ids requested. filters RecordFilters Filters to list the records that will be used to create the cube ListDatasetsResponse Returns metadata on datasets that match records x instance Field Type Label Description records Record repeated List of records dataset_metas DatasetMeta repeated For each record, list of the datasets Shape Shape of an image width x height x channels Field Type Label Description dim1 int32 dim2 int32 dim3 int32 ByteOrder ByteOrder for the conversion between data type and byte. Name Number Description LittleEndian 0 BigEndian 1 FileFormat Available file formats Name Number Description Raw 0 raw bitmap GTiff 1 Top pb/layouts.proto Cell Define a cell of a grid Field Type Label Description id string Cell identifier crs string Coordinate reference system used in the cell coordinates LinearRing Geographic coordinates CreateGridRequest Create a new grid. Field Type Label Description grid Grid CreateGridResponse CreateLayoutRequest Create a new layout Return an error if the name already exists Field Type Label Description layout Layout CreateLayoutResponse DeleteGridRequest Delete a grid Field Type Label Description name string DeleteGridResponse DeleteLayoutRequest Delete a layout by name Field Type Label Description name string DeleteLayoutResponse FindContainerLayoutsRequest Find all the layouts used by the datasets on an AOI or a set of records It can be used to tile the AOI with an optimal layout. Field Type Label Description instance_id string records RecordIdList List of record ids filters RecordFiltersWithAOI Filters to select records FindContainerLayoutsResponse Stream the name of the layout and the associated containers Field Type Label Description layout_name string Name of the layout container_uris string repeated List of containers having the layout GeoTransform GDAL GeoTransform Field Type Label Description a double x offset b double x resolution c double d double y offset e double f double y resolution Grid Define a grid Field Type Label Description name string Unique name of the grid description string Description of the grid cells Cell repeated Cells of the grid Layout Define a layout for consolidation. A layout is composed of an external and an internal layout. External layout is a grid that is used to cover any area with tiles. TODO Internal layout defines the internal structure of a dataset Interlacing_pattern defines how to interlace the [R]ecords, the [B]ands, the [Z]ooms level/overview and the [T]iles (geotiff blocks). The four levels of interlacing must be prioritized in the following way L1>L2>L3>L4 where each L is in [R, B, Z, T]. This order should be understood as: for each L1: for each L2: for each L3: for each L4: addBlock(L1, L2, L3, L4) In other words, all L4 for a given (L1, L2, L3) will be contiguous in memory. For example: - To optimize the access to geographical information of all the bands (such as in COG) : R>Z>T>B => For a given record, zoom level and block, all the bands will be contiguous. - To optimize the access to geographical information of one band at a time : B>R>Z>T => For a given band, record and zoom, all the blocks will be contiguous. - To optimize the access to timeseries of all the bands (such as in MUCOG): Z>T>R>B => For a given zoom level and block, all the records will be contiguous. Interlacing pattern can be specialized to only select a list or a range for each level (except Tile level). - By values: L=0,2,3 will only select the value 0, 2 and 3 of the level L. For example B=0,2,3 to select the corresponding band level. - By range: L=0:3 will only select the values from 0 to 3 (not included) of the level L. For example B=0:3 to select the three firsts bands. First and last values of the range can be omitted to define 0 or last element of the level. e.g B=2: means all the bands from the second. Z=0 is the full resolution, Z=1 is the overview with zoom factor 2, Z=2 is the zoom factor 4, and so on. To chain interlacing patterns, use \";\" separator. For example: - MUCOG optimizes access to timeseries for full resolution (Z=0), but geographic for overviews (Z=1:). Z=0>T>R>B;Z=1:>R>T>B - Same example, but the bands are separated: B>Z=0>T>R;B>Z=1:>R>T - To optimize access to geographic information of the three first bands together, but timeseries of the others: Z>T>R>B=0:3;B=3:>Z>R>T Field Type Label Description name string grid_flags string repeated External layout: Grid:Cell (CRS) grid_parameters Layout.GridParametersEntry repeated block_x_size int64 Internal layout: Cell, Tile block_y_size int64 max_records int64 overviews_min_size int64 Maximum width or height of the smallest overview level. 0: No overview, -1: default=256. interlacing_pattern string Define how to interlace the [R]ecords, the [B]ands, the [Z]ooms level/overview and the [T]iles (geotiff blocks). Layout.GridParametersEntry Field Type Label Description key string value string ListGridsRequest List all the grids given a name pattern (does not retrieve the cells) Field Type Label Description name_like string Name pattern (support * and ? for all or any characters and trailing (?i) for case-insensitiveness) ListGridsResponse Return a list of grids Field Type Label Description grids Grid repeated ListLayoutsRequest List all the layouts given a name pattern Field Type Label Description name_like string Name pattern (support * and ? for all or any characters and trailing (?i) for case-insensitiveness) ListLayoutsResponse Return a list of layouts Field Type Label Description layouts Layout repeated Size Define a size Field Type Label Description width int32 height int32 Tile Define a rectangular tile in a given coordinate system (CRS). Field Type Label Description transform GeoTransform Transform to map from pixel coordinates to CRS size_px Size Size of the tile in pixel crs string Coordinate reference system TileAOIRequest Tile an AOI, covering it with cells defined by a grid. In the future, it will be able to find the best tiling given the internal layout of datasets. Field Type Label Description aoi AOI layout_name string Name of an existing layout layout Layout User-defined layout TileAOIResponse Return tiles, thousand by thousand. Field Type Label Description tiles Tile repeated Top pb/operations.proto CancelJobRequest Cancel a job (e.g. during consolidation) Field Type Label Description id string force_any_state bool Force cancel even when the job is not in a failed state or consolidation step (could corrupt the data) CancelJobResponse CleanJobsRequest Clean terminated jobs Field Type Label Description name_like string Filter by name (support *, ? and (?i)-suffix for case-insensitivity) state string Filter by terminated state (DONE, FAILED) CleanJobsResponse Return the number of jobs that have been deleted Field Type Label Description count int32 ConfigConsolidationRequest Configure the parameters of the consolidation attached to the variable Field Type Label Description variable_id string consolidation_params ConsolidationParams ConfigConsolidationResponse ConsolidateRequest Create and start a consolidation job given a list of records and an instance_id to be consolidated on a layout Optionnaly, the job can be done step by step, pausing and waiting for user action, with three levels: - 1: after each critical steps - 2: after each major steps - 3: after all steps Field Type Label Description job_name string instance_id string layout_name string execution_level ExecutionLevel Execution level of a job. A consolidation job cannot be executed synchronously collapse_on_record_id string [Optional] Collapse all records on this record (in this case only, original datasets are kept, data is duplicated) records RecordIdList At least one filters RecordFilters ConsolidateResponse Return the id of the job created Field Type Label Description job_id string ConsolidationParams Parameters of consolidation that are linked to a variable, to define: - how to resample the data during consolidation - how to store the data: - Compression - CreationParams (supported: see GDAL drivers: PHOTOMETRIC, COMPRESS, PREDICTOR, ZLEVEL, ZSTDLEVEL, MAX_Z_ERROR, JPEGTABLESMODE and with _OVERVIEW suffix if exists) Field Type Label Description dformat DataFormat dataformat of the data. See exponent for the mapping formula. exponent double 1: linear scaling (RealMax - RealMin) * pow( (Value - Min) / (Max - Min), Exponent) + RealMin create_overviews bool Deprecated. Use Layout.overviews_min_size instead resampling_alg Resampling Define how to resample the data during the consolidation (if a reprojection is needed or if the overviews are created) compression ConsolidationParams.Compression Define how the data is compressed at block level creation_params ConsolidationParams.CreationParamsEntry repeated map of params:value to configure the creation of the file. See Compression to list the supported params bands_interleave bool Deprecated. If the variable is multibands, define whether the bands are interleaved. Use Layout.interlacing_pattern instead storage_class StorageClass Define the storage class of the created file (support only GCS) ConsolidationParams.CreationParamsEntry Field Type Label Description key string value string Container Define a container of datasets. Usually a container is a file containing one dataset. But after a consolidation or if the container has several bands, it can contain several datasets. Field Type Label Description uri string URI of the file managed bool True if the Geocube is responsible for the lifecycle of this file datasets Dataset repeated List of datasets of the container ContinueJobRequest Proceed the next step of a step-by-step job Field Type Label Description id string ContinueJobResponse Dataset Define a dataset. A dataset is the metadata to retrieve an image from a file. It is defined by a record and the instance of a variable. A dataset defines: - Which band(s) are indexed (usually all the bands, but it can be a subset) - How to map the value of its pixels to the dataformat of the variable. In more details: . the dataformat of the dataset (dformat.[no_data, min, max]) that describes the pixel of the image . the mapping from each pixel to the data format of the variable (variable.dformat). This mapping is defined as [MinOut, MaxOut, Exponent]. Field Type Label Description record_id string instance_id string container_subdir string bands int64 repeated dformat DataFormat Internal data format (DType can be Undefined) real_min_value double Real min value (dformat.min_value maps to real_min_value) real_max_value double Real max value (dformat.max_value maps to real_max_value) exponent double 1: linear scaling (RealMax - RealMin) * pow( (Value - Min) / (Max - Min), Exponent) + RealMin DeleteDatasetsRequest Remove the datasets referenced by instances and records without any control The containers (if empty) are not deleted Field Type Label Description record_ids string repeated Instance id that references the datasets to be deleted instance_ids string repeated Record ids that reference the datasets to be deleted dataset_patterns string repeated Dataset file patterns (support * and ? for all or any characters and trailing (?i) for case-insensitiveness) (or empty to ignore) execution_level ExecutionLevel Execution level (see enum) job_name string Name of the job (if empty, a name will be generated) DeleteDatasetsResponse Return the number of deleted datasets Field Type Label Description job Job GetConsolidationParamsRequest Retrieve the consolidation parameters attached to the given variable Field Type Label Description variable_id string GetConsolidationParamsResponse Return consolidation parameters Field Type Label Description consolidation_params ConsolidationParams GetContainersRequest Request info on containers Field Type Label Description uris string repeated List of container uris GetContainersResponse Field Type Label Description containers Container repeated GetJobRequest Retrieve a job given its id Field Type Label Description id string log_page int32 log_limit int32 GetJobResponse Return a job with the requested id Field Type Label Description job Job IndexDatasetsRequest Request to index all the datasets of a container Field Type Label Description container Container TODO Index several containers: repeated ? IndexDatasetsResponse Job Job to modify datasets (consolidation, deletion, ingestion...) Some lifecycle operations are required to be done cautiously, in order to garantee the integrity of the database. Such operations are defined by a job and are done asynchronously. A job is a state-machine that can be rollbacked anytime during the operation until it ends. Field Type Label Description id string Id of the job name string Name of the job (must be unique) type string Type of the job (consolidation, deletion...) state string Current state of the state machine creation_time google.protobuf.Timestamp Time of creation of the job last_update_time google.protobuf.Timestamp Time of the last update logs string repeated Job logs: if logs are too big to fit in a grpc response, logs will only be a subset (by default, the latest) active_tasks int32 If the job is divided into sub tasks, number of pending tasks failed_tasks int32 If the job is divided into sub tasks, number of failed tasks execution_level ExecutionLevel Execution level of a job (see ExecutionLevel) waiting bool If true, the job is waiting for user to continue ListJobsRequest List jobs given a name pattern Field Type Label Description name_like string page int32 limit int32 ListJobsResponse Return a list of the job whose name matchs the pattern Field Type Label Description jobs Job repeated RetryJobRequest Retry a job that failed or is stuck (e.g. during consolidation) Field Type Label Description id string force_any_state bool Force retry even when the job is not in a failed state (could corrupt the data) RetryJobResponse ConsolidationParams.Compression Name Number Description NO 0 LOSSLESS 1 LOSSY 2 CUSTOM 3 configured by creation_params ExecutionLevel Execution level of a job Name Number Description ExecutionSynchronous 0 Job is done synchronously ExecutionAsynchronous 1 Job is done asynchronously, but without any pause StepByStepCritical 2 Job is done asynchronously, step-by-step, pausing at every critical steps StepByStepMajor 3 Job is done asynchronously, step-by-step, pausing at every major steps StepByStepAll 4 Job is done asynchronously, step-by-step, pausing at every steps StorageClass Storage class of a container. Depends on the storage Name Number Description STANDARD 0 INFREQUENT 1 ARCHIVE 2 DEEPARCHIVE 3 Top pb/datasetMeta.proto DatasetMeta DatasetMeta contains all the metadata on files and fileformats to download and generate a slice of a cube Field Type Label Description internalsMeta InternalMeta repeated Information on the images composing the slice InternalMeta InternalMeta contains all the metadata on a file to download it and to map its internal values to the external range. Field Type Label Description container_uri string URI of the file storing the data container_subdir string Subdir of the file storing the data bands int64 repeated Subbands of the file requested dformat DataFormat Internal dataformat of the data range_min double dformat.RangeMin will be mapped to this value range_max double dformat.RangeMax will be mapped to this value exponent double Exponent used to map the value from dformat to [RangeMin, RangeMax] Top pb/version.proto GetVersionRequest Request the version of the Geocube GetVersionResponse Return the version of the Geocube Field Type Label Description Version string Scalar Value Types .proto Type Notes C++ Java Python Go C# PHP Ruby double double double float float64 double float Float float float float float float32 float float Float int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int32 int integer Bignum or Fixnum (as required) int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long int64 long integer/string Bignum uint32 Uses variable-length encoding. uint32 int int/long uint32 uint integer Bignum or Fixnum (as required) uint64 Uses variable-length encoding. uint64 long int/long uint64 ulong integer/string Bignum or Fixnum (as required) sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int int32 int integer Bignum or Fixnum (as required) sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long int64 long integer/string Bignum fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int uint32 uint integer Bignum or Fixnum (as required) fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long uint64 ulong integer/string Bignum sfixed32 Always four bytes. int32 int int int32 int integer Bignum or Fixnum (as required) sfixed64 Always eight bytes. int64 long int/long int64 long integer/string Bignum bool bool boolean boolean bool bool boolean TrueClass/FalseClass string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode string string string String (UTF-8) bytes May contain any arbitrary sequence of bytes. string ByteString str []byte ByteString string String (ASCII-8BIT)","title":"GRPC documentation"},{"location":"user-guide/grpc/#protocol-documentation","text":"","title":"Protocol Documentation"},{"location":"user-guide/grpc/#table-of-contents","text":"pb/geocube.proto Geocube pb/geocubeDownloader.proto GeocubeDownloader pb/admin.proto TidyDBRequest TidyDBResponse UpdateDatasetsRequest UpdateDatasetsResponse UpdateDatasetsResponse.ResultsEntry Admin pb/records.proto AOI AddRecordsTagsRequest AddRecordsTagsRequest.TagsEntry AddRecordsTagsResponse Coord CreateAOIRequest CreateAOIResponse CreateRecordsRequest CreateRecordsResponse DeleteRecordsRequest DeleteRecordsResponse GetAOIRequest GetAOIResponse GetRecordsRequest GetRecordsResponseItem GroupedRecordIds GroupedRecordIdsList GroupedRecords LinearRing ListRecordsRequest ListRecordsRequest.TagsEntry ListRecordsResponseItem NewRecord NewRecord.TagsEntry Polygon Record Record.TagsEntry RecordFilters RecordFilters.TagsEntry RecordFiltersWithAOI RecordIdList RemoveRecordsTagsRequest RemoveRecordsTagsResponse pb/variables.proto CreatePaletteRequest CreatePaletteResponse CreateVariableRequest CreateVariableResponse DeleteInstanceRequest DeleteInstanceResponse DeleteVariableRequest DeleteVariableResponse GetVariableRequest GetVariableResponse Instance Instance.MetadataEntry InstantiateVariableRequest InstantiateVariableRequest.InstanceMetadataEntry InstantiateVariableResponse ListVariablesRequest ListVariablesResponseItem Palette UpdateInstanceRequest UpdateInstanceRequest.AddMetadataEntry UpdateInstanceResponse UpdateVariableRequest UpdateVariableResponse Variable colorPoint Resampling pb/dataformat.proto DataFormat DataFormat.Dtype pb/catalog.proto GetCubeMetadataRequest GetCubeMetadataResponse GetCubeRequest GetCubeResponse GetCubeResponseHeader GetTileRequest GetTileResponse ImageChunk ImageFile ImageHeader ListDatasetsRequest ListDatasetsResponse Shape ByteOrder FileFormat pb/layouts.proto Cell CreateGridRequest CreateGridResponse CreateLayoutRequest CreateLayoutResponse DeleteGridRequest DeleteGridResponse DeleteLayoutRequest DeleteLayoutResponse FindContainerLayoutsRequest FindContainerLayoutsResponse GeoTransform Grid Layout Layout.GridParametersEntry ListGridsRequest ListGridsResponse ListLayoutsRequest ListLayoutsResponse Size Tile TileAOIRequest TileAOIResponse pb/operations.proto CancelJobRequest CancelJobResponse CleanJobsRequest CleanJobsResponse ConfigConsolidationRequest ConfigConsolidationResponse ConsolidateRequest ConsolidateResponse ConsolidationParams ConsolidationParams.CreationParamsEntry Container ContinueJobRequest ContinueJobResponse Dataset DeleteDatasetsRequest DeleteDatasetsResponse GetConsolidationParamsRequest GetConsolidationParamsResponse GetContainersRequest GetContainersResponse GetJobRequest GetJobResponse IndexDatasetsRequest IndexDatasetsResponse Job ListJobsRequest ListJobsResponse RetryJobRequest RetryJobResponse ConsolidationParams.Compression ExecutionLevel StorageClass pb/datasetMeta.proto DatasetMeta InternalMeta pb/version.proto GetVersionRequest GetVersionResponse Scalar Value Types Top","title":"Table of Contents"},{"location":"user-guide/grpc/#pbgeocubeproto","text":"","title":"pb/geocube.proto"},{"location":"user-guide/grpc/#geocube","text":"API Documentation may be detailed in Request/Response sections. Method Name Request Type Response Type Description CreateRecords CreateRecordsRequest CreateRecordsResponse Create one or a list of records GetRecords GetRecordsRequest GetRecordsResponseItem stream Get records from their ID ListRecords ListRecordsRequest ListRecordsResponseItem stream List records given criterias AddRecordsTags AddRecordsTagsRequest AddRecordsTagsResponse Update records, adding or updating tags RemoveRecordsTags RemoveRecordsTagsRequest RemoveRecordsTagsResponse Update records, removing tags DeleteRecords DeleteRecordsRequest DeleteRecordsResponse Delete records CreateAOI CreateAOIRequest CreateAOIResponse Create an AOI if not exists or returns the aoi id of the aoi. GetAOI GetAOIRequest GetAOIResponse Get an AOI from its ID CreateVariable CreateVariableRequest CreateVariableResponse Create a variable GetVariable GetVariableRequest GetVariableResponse Get a variable given its id, name or one of its instance id UpdateVariable UpdateVariableRequest UpdateVariableResponse Update some fields of a variable DeleteVariable DeleteVariableRequest DeleteVariableResponse Delete a variable iif no dataset has a reference on ListVariables ListVariablesRequest ListVariablesResponseItem stream List variables given a name pattern InstantiateVariable InstantiateVariableRequest InstantiateVariableResponse Instantiate a variable UpdateInstance UpdateInstanceRequest UpdateInstanceResponse Update metadata of an instance DeleteInstance DeleteInstanceRequest DeleteInstanceResponse Delete an instance iif no dataset has a reference on CreatePalette CreatePaletteRequest CreatePaletteResponse Create or update a palette that can be used to create a display of a dataset GetContainers GetContainersRequest GetContainersResponse GetInfo on containers IndexDatasets IndexDatasetsRequest IndexDatasetsResponse Index new datasets in the Geocube ListDatasets ListDatasetsRequest ListDatasetsResponse List datasets from the Geocube DeleteDatasets DeleteDatasetsRequest DeleteDatasetsResponse Delete datasets using records, instances and/or filepath ConfigConsolidation ConfigConsolidationRequest ConfigConsolidationResponse Configurate a consolidation process GetConsolidationParams GetConsolidationParamsRequest GetConsolidationParamsResponse Get the configuration of a consolidation Consolidate ConsolidateRequest ConsolidateResponse Start a consolidation job ListJobs ListJobsRequest ListJobsResponse List the jobs given a name pattern GetJob GetJobRequest GetJobResponse Get a job given its name CleanJobs CleanJobsRequest CleanJobsResponse Delete jobs given their status RetryJob RetryJobRequest RetryJobResponse Retry a job CancelJob CancelJobRequest CancelJobResponse Cancel a job ContinueJob ContinueJobRequest ContinueJobResponse Continue a job that is in waiting state GetCube GetCubeRequest GetCubeResponse stream Get a cube of data given a CubeParams GetXYZTile GetTileRequest GetTileResponse Get a XYZTile (can be used with a TileServer, provided a GRPCGateway is up) CreateLayout CreateLayoutRequest CreateLayoutResponse Create a layout to be used for tiling or consolidation DeleteLayout DeleteLayoutRequest DeleteLayoutResponse Delete a layout given its name ListLayouts ListLayoutsRequest ListLayoutsResponse List layouts given a name pattern FindContainerLayouts FindContainerLayoutsRequest FindContainerLayoutsResponse stream Find all the layouts known for a set of containers TileAOI TileAOIRequest TileAOIResponse stream Tile an AOI given a layout CreateGrid CreateGridRequest stream CreateGridResponse Create a grid that can be used to tile an AOI DeleteGrid DeleteGridRequest DeleteGridResponse Delete a grid ListGrids ListGridsRequest ListGridsResponse List grids given a name pattern Version GetVersionRequest GetVersionResponse Version of the GeocubeServer Top","title":"Geocube"},{"location":"user-guide/grpc/#pbgeocubedownloaderproto","text":"","title":"pb/geocubeDownloader.proto"},{"location":"user-guide/grpc/#geocubedownloader","text":"API GeocubeDownloader to download a cube from metadata Method Name Request Type Response Type Description DownloadCube GetCubeMetadataRequest GetCubeMetadataResponse stream Request cube using metadatas returned by a call to Geocube.GetCube() Version GetVersionRequest GetVersionResponse Version of the GeocubeDownloader Top","title":"GeocubeDownloader"},{"location":"user-guide/grpc/#pbadminproto","text":"","title":"pb/admin.proto"},{"location":"user-guide/grpc/#tidydbrequest","text":"Request to remove from the database all the pending entities (entities that are not linked to any dataset) Field Type Label Description Simulate bool If true, a simulation is done, nothing is actually deleted PendingAOIs bool Remove AOIs that are not linked to any Records PendingRecords bool Remove Records that do not reference any Datasets PendingVariables bool Remove Variables that have not any instances PendingInstances bool Remove Instances that do not reference any Datasets PendingContainers bool Remove Containers that do not contain any Datasets PendingParams bool Remove ConsolidationParams that are not linked to any Variable or Job","title":"TidyDBRequest"},{"location":"user-guide/grpc/#tidydbresponse","text":"Return the number of entities that were deleted (or should have been deleted if Simulate=True) Field Type Label Description NbAOIs int64 NbRecords int64 NbVariables int64 NbInstances int64 NbContainers int64 NbParams int64","title":"TidyDBResponse"},{"location":"user-guide/grpc/#updatedatasetsrequest","text":"Update fields of datasets that can be tricky Field Type Label Description simulate bool If true, a simulation is done, nothing is actually updated instance_id string Instance id that references the datasets to be updated record_ids string repeated Record ids that reference the datasets to be updated dformat DataFormat Internal data format (DType can be Undefined) real_min_value double Real min value (dformat.min_value maps to real_min_value) real_max_value double Real max value (dformat.max_value maps to real_max_value) exponent double 1: linear scaling (RealMax - RealMin) * pow( (Value - Min) / (Max - Min), Exponent) + RealMin","title":"UpdateDatasetsRequest"},{"location":"user-guide/grpc/#updatedatasetsresponse","text":"Return the number of modifications per kind of modification Field Type Label Description results UpdateDatasetsResponse.ResultsEntry repeated","title":"UpdateDatasetsResponse"},{"location":"user-guide/grpc/#updatedatasetsresponseresultsentry","text":"Field Type Label Description key string value int64","title":"UpdateDatasetsResponse.ResultsEntry"},{"location":"user-guide/grpc/#admin","text":"Service providing some functions to update or clean the database Must be used cautiously because there is no control neither possible rollback Method Name Request Type Response Type Description TidyDB TidyDBRequest TidyDBResponse UpdateDatasets UpdateDatasetsRequest UpdateDatasetsResponse DeleteDatasets DeleteDatasetsRequest DeleteDatasetsResponse Top","title":"Admin"},{"location":"user-guide/grpc/#pbrecordsproto","text":"","title":"pb/records.proto"},{"location":"user-guide/grpc/#aoi","text":"Geographic AOI Field Type Label Description polygons Polygon repeated","title":"AOI"},{"location":"user-guide/grpc/#addrecordstagsrequest","text":"Add the given tags to a set of records Field Type Label Description ids string repeated tags AddRecordsTagsRequest.TagsEntry repeated","title":"AddRecordsTagsRequest"},{"location":"user-guide/grpc/#addrecordstagsrequesttagsentry","text":"Field Type Label Description key string value string","title":"AddRecordsTagsRequest.TagsEntry"},{"location":"user-guide/grpc/#addrecordstagsresponse","text":"Returns the number of records impacted by the addition Field Type Label Description nb int64","title":"AddRecordsTagsResponse"},{"location":"user-guide/grpc/#coord","text":"Geographic coordinates (4326) Field Type Label Description lon float lat float","title":"Coord"},{"location":"user-guide/grpc/#createaoirequest","text":"Create a new AOI Field Type Label Description aoi AOI","title":"CreateAOIRequest"},{"location":"user-guide/grpc/#createaoiresponse","text":"Returns the ID of the AOI Field Type Label Description id string","title":"CreateAOIResponse"},{"location":"user-guide/grpc/#createrecordsrequest","text":"Create new records Field Type Label Description records NewRecord repeated","title":"CreateRecordsRequest"},{"location":"user-guide/grpc/#createrecordsresponse","text":"Returns the ID of the created records Field Type Label Description ids string repeated","title":"CreateRecordsResponse"},{"location":"user-guide/grpc/#deleterecordsrequest","text":"Delete records by ID Field Type Label Description ids string repeated no_fail bool If true, do not fail if some records still have datasets that refer to them and delete the others.","title":"DeleteRecordsRequest"},{"location":"user-guide/grpc/#deleterecordsresponse","text":"Return the number of deleted records Field Type Label Description nb int64","title":"DeleteRecordsResponse"},{"location":"user-guide/grpc/#getaoirequest","text":"Request the AOI given its ID Field Type Label Description id string","title":"GetAOIRequest"},{"location":"user-guide/grpc/#getaoiresponse","text":"Returns a geometric AOI Field Type Label Description aoi AOI","title":"GetAOIResponse"},{"location":"user-guide/grpc/#getrecordsrequest","text":"Get record from its id Field Type Label Description ids string repeated","title":"GetRecordsRequest"},{"location":"user-guide/grpc/#getrecordsresponseitem","text":"Return a record Field Type Label Description record Record","title":"GetRecordsResponseItem"},{"location":"user-guide/grpc/#groupedrecordids","text":"Record ids that are considered as a unique, merged record (e.g. all records of a given date, whatever the time of the day) Field Type Label Description ids string repeated","title":"GroupedRecordIds"},{"location":"user-guide/grpc/#groupedrecordidslist","text":"List of groupedRecordIds Field Type Label Description records GroupedRecordIds repeated","title":"GroupedRecordIdsList"},{"location":"user-guide/grpc/#groupedrecords","text":"Records that are considered as a unique, merged record (e.g. all records of a given date, whatever the time of the day) Field Type Label Description records Record repeated","title":"GroupedRecords"},{"location":"user-guide/grpc/#linearring","text":"Geographic linear ring Field Type Label Description points Coord repeated","title":"LinearRing"},{"location":"user-guide/grpc/#listrecordsrequest","text":"Request to find the list of records corresponding to multiple filters (inclusive) Field Type Label Description name string Name pattern (support * and ? for all or any characters and trailing (?i) for case-insensitiveness) tags ListRecordsRequest.TagsEntry repeated cf RecordFilters from_time google.protobuf.Timestamp cf RecordFilters to_time google.protobuf.Timestamp cf RecordFilters aoi AOI cf RecordFiltersWithAOI limit int32 page int32 with_aoi bool Also returns the AOI (may be big)","title":"ListRecordsRequest"},{"location":"user-guide/grpc/#listrecordsrequesttagsentry","text":"Field Type Label Description key string value string","title":"ListRecordsRequest.TagsEntry"},{"location":"user-guide/grpc/#listrecordsresponseitem","text":"Field Type Label Description record Record","title":"ListRecordsResponseItem"},{"location":"user-guide/grpc/#newrecord","text":"Structure to create a new record Field Type Label Description name string time google.protobuf.Timestamp tags NewRecord.TagsEntry repeated aoi_id string","title":"NewRecord"},{"location":"user-guide/grpc/#newrecordtagsentry","text":"Field Type Label Description key string value string","title":"NewRecord.TagsEntry"},{"location":"user-guide/grpc/#polygon","text":"Geographic polygon Field Type Label Description linearrings LinearRing repeated","title":"Polygon"},{"location":"user-guide/grpc/#record","text":"Record Field Type Label Description id string name string time google.protobuf.Timestamp tags Record.TagsEntry repeated aoi_id string aoi AOI optional","title":"Record"},{"location":"user-guide/grpc/#recordtagsentry","text":"Field Type Label Description key string value string","title":"Record.TagsEntry"},{"location":"user-guide/grpc/#recordfilters","text":"RecordFilters defines some filters to identify records Field Type Label Description tags RecordFilters.TagsEntry repeated Tags of the records from_time google.protobuf.Timestamp Minimum date of the records to_time google.protobuf.Timestamp Maximum date of the records","title":"RecordFilters"},{"location":"user-guide/grpc/#recordfilterstagsentry","text":"Field Type Label Description key string value string","title":"RecordFilters.TagsEntry"},{"location":"user-guide/grpc/#recordfilterswithaoi","text":"RecordFiltersWithAOI defines some filters to identify records, including an AOI in geometric coordinates Field Type Label Description filters RecordFilters aoi AOI Geometric coordinates of an AOI that intersects the AOI of the records","title":"RecordFiltersWithAOI"},{"location":"user-guide/grpc/#recordidlist","text":"List of record ids that are considered separately Field Type Label Description ids string repeated","title":"RecordIdList"},{"location":"user-guide/grpc/#removerecordstagsrequest","text":"Remove the given tags for a set of records Field Type Label Description ids string repeated tagsKey string repeated","title":"RemoveRecordsTagsRequest"},{"location":"user-guide/grpc/#removerecordstagsresponse","text":"Returns the number of records impacted by the removal Field Type Label Description nb int64 Top","title":"RemoveRecordsTagsResponse"},{"location":"user-guide/grpc/#pbvariablesproto","text":"","title":"pb/variables.proto"},{"location":"user-guide/grpc/#createpaletterequest","text":"Create a new palette or update it if already exists (provided replace=True) Field Type Label Description palette Palette Palette to be created replace bool Replace the current existing palette if exists","title":"CreatePaletteRequest"},{"location":"user-guide/grpc/#createpaletteresponse","text":"Return nothing.","title":"CreatePaletteResponse"},{"location":"user-guide/grpc/#createvariablerequest","text":"Define a new variable. Return an error if the name already exists. Field Type Label Description variable Variable","title":"CreateVariableRequest"},{"location":"user-guide/grpc/#createvariableresponse","text":"Return the id of the new variable. Field Type Label Description id string","title":"CreateVariableResponse"},{"location":"user-guide/grpc/#deleteinstancerequest","text":"Delete an instance Return an error if the instance is linked to datasets. Field Type Label Description id string UUID-4 of the instance to delete","title":"DeleteInstanceRequest"},{"location":"user-guide/grpc/#deleteinstanceresponse","text":"Return nothing","title":"DeleteInstanceResponse"},{"location":"user-guide/grpc/#deletevariablerequest","text":"Delete a variable Return an error if the variable has still instances Field Type Label Description id string UUID-4 of the variable to delete","title":"DeleteVariableRequest"},{"location":"user-guide/grpc/#deletevariableresponse","text":"Return nothing","title":"DeleteVariableResponse"},{"location":"user-guide/grpc/#getvariablerequest","text":"Read a variable given either its id, its name or the id of one of its instance Field Type Label Description id string UUID-4 of the variable name string Name of the variable instance_id string UUID-4 of an instance","title":"GetVariableRequest"},{"location":"user-guide/grpc/#getvariableresponse","text":"Return the variable and its instances Field Type Label Description variable Variable","title":"GetVariableResponse"},{"location":"user-guide/grpc/#instance","text":"Field Type Label Description id string Null at creation name string metadata Instance.MetadataEntry repeated","title":"Instance"},{"location":"user-guide/grpc/#instancemetadataentry","text":"Field Type Label Description key string value string","title":"Instance.MetadataEntry"},{"location":"user-guide/grpc/#instantiatevariablerequest","text":"Instantiate a variable. Return an error if the instance_name already exists for this variable. Field Type Label Description variable_id string instance_name string instance_metadata InstantiateVariableRequest.InstanceMetadataEntry repeated","title":"InstantiateVariableRequest"},{"location":"user-guide/grpc/#instantiatevariablerequestinstancemetadataentry","text":"Field Type Label Description key string value string","title":"InstantiateVariableRequest.InstanceMetadataEntry"},{"location":"user-guide/grpc/#instantiatevariableresponse","text":"Return the new instance (its id, name and metadata) Field Type Label Description instance Instance","title":"InstantiateVariableResponse"},{"location":"user-guide/grpc/#listvariablesrequest","text":"List variables given a name pattern Field Type Label Description name string Pattern of the name of the variable (support * and ? for all or any characters, (?i) suffix for case-insensitiveness) limit int32 Limit the number of variables returned page int32 Navigate through results (start at 0)","title":"ListVariablesRequest"},{"location":"user-guide/grpc/#listvariablesresponseitem","text":"Return a stream of variables Field Type Label Description variable Variable","title":"ListVariablesResponseItem"},{"location":"user-guide/grpc/#palette","text":"Define a palette with a name and a set of colorPoint. Maps all values in [0,1] to an RGBA value, using piecewise curve defined by colorPoints. All intermediate values are linearly interpolated. Field Type Label Description name string Name of the palette (Alpha-numerics characters, dots, dashes and underscores are supported) colors colorPoint repeated Set of colorPoints. At least two points must be defined, corresponding to value=0 and value=1.","title":"Palette"},{"location":"user-guide/grpc/#updateinstancerequest","text":"Update an instance Return an error if the name is to be updated but the new name already exists. Field Type Label Description id string UUID-4 of the instance to update name google.protobuf.StringValue [Optional] New name of the variable. Empty to ignore add_metadata UpdateInstanceRequest.AddMetadataEntry repeated Pairs of metadata (key, values) to be inserted or updated del_metadata_keys string repeated Metadata keys to be deleted","title":"UpdateInstanceRequest"},{"location":"user-guide/grpc/#updateinstancerequestaddmetadataentry","text":"Field Type Label Description key string value string","title":"UpdateInstanceRequest.AddMetadataEntry"},{"location":"user-guide/grpc/#updateinstanceresponse","text":"Return nothing","title":"UpdateInstanceResponse"},{"location":"user-guide/grpc/#updatevariablerequest","text":"Update the non-critical fields of a variable Return an error if the name is to be updated but the new name already exists. Field Type Label Description id string UUID-4 of the variable to update name google.protobuf.StringValue [Optional] New name of the variable. Empty to ignore unit google.protobuf.StringValue [Optional] New unit of the variable. Empty to ignore description google.protobuf.StringValue [Optional] New description of the variable. Empty to ignore palette google.protobuf.StringValue [Optional] New default palette of the variable. Empty to ignore resampling_alg Resampling [Optional] New default resampling algorithm of the variable. UNDEFINED to ignore","title":"UpdateVariableRequest"},{"location":"user-guide/grpc/#updatevariableresponse","text":"Return nothing","title":"UpdateVariableResponse"},{"location":"user-guide/grpc/#variable","text":"Variable Field Type Label Description id string Internal UUID-4 of the variable (ignored at creation) name string Name of the variable (Alpha-numerics characters, dashs, dots and underscores) unit string Unit of the variable (for user information only) description string Description of the variable (for user information only) dformat DataFormat Format of the data. Range.Min and Range.Max are used for data mapping from internal data format of a dataset (See IndexDatasets for more details), DType and NoData are used for the outputs of GetCube. bands string repeated Name of each band. Can be empty when the variable refers to only one band, must be unique otherwise. palette string Name of the default palette for color rendering. resampling_alg Resampling Default resampling algorithm in case of reprojection. instances Instance repeated List of instances of the variable (ignored at creation)","title":"Variable"},{"location":"user-guide/grpc/#colorpoint","text":"Define a color mapping from a value [0-1] to a RGBA value. Field Type Label Description value float r uint32 g uint32 b uint32 a uint32","title":"colorPoint"},{"location":"user-guide/grpc/#resampling","text":"Resampling algorithms (supported by GDAL) Name Number Description UNDEFINED 0 NEAR 1 BILINEAR 2 CUBIC 3 CUBICSPLINE 4 LANCZOS 5 AVERAGE 6 MODE 7 MAX 8 MIN 9 MED 10 Q1 11 Q3 12 Top","title":"Resampling"},{"location":"user-guide/grpc/#pbdataformatproto","text":"","title":"pb/dataformat.proto"},{"location":"user-guide/grpc/#dataformat","text":"Format of the data of a dataset. Format is defined by the type of the data, its no-data value and the range of values (its interpretation depends on the use) Field Type Label Description dtype DataFormat.Dtype Type of the data no_data double No-data value (supports any float values, including NaN) min_value double Min value (usually used to map from one min value to another) max_value double Max value (usually used to map from one min value to another)","title":"DataFormat"},{"location":"user-guide/grpc/#dataformatdtype","text":"Type of data supported by the Geocube & GDAL Name Number Description UNDEFINED 0 UInt8 1 UInt16 2 UInt32 3 Int8 4 Int16 5 Int32 6 Float32 7 Float64 8 Complex64 9 Pair of float32 Top","title":"DataFormat.Dtype"},{"location":"user-guide/grpc/#pbcatalogproto","text":"","title":"pb/catalog.proto"},{"location":"user-guide/grpc/#getcubemetadatarequest","text":"Request a cube from metadatas (provided by Geocube.GetCube()) Field Type Label Description datasets_meta DatasetMeta repeated List of Metadatas needed to download and generate the slices of the cube grouped_records GroupedRecords repeated List of GroupedRecords describing the slices of the cube ref_dformat DataFormat Output dataformat resampling_alg Resampling Resampling algorithm to use for reprojection pix_to_crs GeoTransform crs string size Size format FileFormat Format of the output data predownload bool Predownload the datasets before merging them. When the dataset is remote and all the dataset is required, it is more efficient to predownload it. protocol_v11x bool For compatibility with older clients. Clients with version above 1.1.0 must set this field to true.","title":"GetCubeMetadataRequest"},{"location":"user-guide/grpc/#getcubemetadataresponse","text":"Return either information on the cube, information on an image or a chunk of an image Field Type Label Description global_header GetCubeResponseHeader header ImageHeader chunk ImageChunk","title":"GetCubeMetadataResponse"},{"location":"user-guide/grpc/#getcuberequest","text":"Request a cube of data Field Type Label Description records RecordIdList List of record ids requested. At least one. One image will be returned by record (if not empty) filters RecordFilters Filters to list the records that will be used to create the cube grouped_records GroupedRecordIdsList List of groups of record ids requested. At least one. One image will be returned by group of records (if not empty). All the datasets of a group of records will be merged together using the latest first. instances_id string repeated Instances of a variable defining the kind of images requested. At least one, and all must be instance of the same variable. Only one is actually supported crs string Coordinates Reference System of the output images (images will be reprojected on the fly if necessary) pix_to_crs GeoTransform GeoTransform of the requested cube (images will be rescaled on the fly if necessary) size Size Shape of the output images compression_level int32 Define a level of compression to speed up the transfer, values: -3 to 9 (-2: Huffman only, -1:default, 0->9: level of compression from the fastest to the best compression, -3: disable the compression). The data is compressed by the server and decompressed by the Client. Use -3 or -2 if the bandwidth is not limited. 0 is level 0 of DEFLATE (thus, it must be decompressed by DEFLATE even though the data is not compressed). If the client can support -3, 0 is useless. headers_only bool Only returns headers (including all metadatas on datasets) format FileFormat Format of the output images resampling_alg Resampling Resampling algorithm used for reprojecion. If undefined, the default resampling algorithm associated to the variable is used. protocol_v11x bool For compatibility with older clients. Clients with version above 1.1.0 must set this field to true.","title":"GetCubeRequest"},{"location":"user-guide/grpc/#getcuberesponse","text":"Return either information on the cube, information on an image or a chunk of an image Field Type Label Description global_header GetCubeResponseHeader header ImageHeader chunk ImageChunk","title":"GetCubeResponse"},{"location":"user-guide/grpc/#getcuberesponseheader","text":"Return global information on the requested cube Field Type Label Description count int64 nb_datasets int64 ref_dformat DataFormat Output dataformat resampling_alg Resampling Resampling algorithm to use for reprojection geotransform GeoTransform Geotransform used for mapping crs string","title":"GetCubeResponseHeader"},{"location":"user-guide/grpc/#gettilerequest","text":"Request a web-mercator tile, given a variable and a group of records Field Type Label Description instance_id string x int32 y int32 z int32 min float max float records GroupedRecordIds Group of record ids. At least one. All the datasets of the group of records will be merged together using the latest first. filters RecordFilters All the datasets whose records have RecordTags and time between from_time and to_time","title":"GetTileRequest"},{"location":"user-guide/grpc/#gettileresponse","text":"Return a 256x256 png image Field Type Label Description image ImageFile","title":"GetTileResponse"},{"location":"user-guide/grpc/#imagechunk","text":"Chunk of the full image, to handle the GRPC limit of 4Mbytes/message Field Type Label Description part int32 Index of the chunk (from 1 to ImageHeader.nb_parts-1). The first part (=0) is ImageHeader.data data bytes Chunk of the full array of bytes","title":"ImageChunk"},{"location":"user-guide/grpc/#imagefile","text":"ByteArray of a PNG image 256x256pixels Field Type Label Description data bytes","title":"ImageFile"},{"location":"user-guide/grpc/#imageheader","text":"Header of an image (slice of the cube) It describes the image, the underlying datasets and the way to recreate it from the array of byte : 1. Append ImageHeader.data and ImageChunk.data from part=0 to part=nb_parts-1 2. If compression=True, decompress the array of bytes using deflate 3. Cast the result to the dtype using byteOrder 4. Reshape the result Field Type Label Description shape Shape Shape of the image (widthxheight) dtype DataFormat.Dtype Type of the data (to interprete \"ImageHeader.data + ImageChunk.data\") nb_parts int32 Number of parts the image is splitted into data bytes First part of the image as an array of bytes size int64 Size of the full array of bytes order ByteOrder ByteOrder of the datatype compression bool Deflate compressed data format, described in RFC 1951 grouped_records GroupedRecords Group of records used to generate this image dataset_meta DatasetMeta All information on the underlying datasets that composed the image error string If not empty, an error occured and the image was not retrieved.","title":"ImageHeader"},{"location":"user-guide/grpc/#listdatasetsrequest","text":"List Datasets Field Type Label Description instance_id string Instance of a variable defining the kind of datasets requested. records RecordIdList List of record ids requested. filters RecordFilters Filters to list the records that will be used to create the cube","title":"ListDatasetsRequest"},{"location":"user-guide/grpc/#listdatasetsresponse","text":"Returns metadata on datasets that match records x instance Field Type Label Description records Record repeated List of records dataset_metas DatasetMeta repeated For each record, list of the datasets","title":"ListDatasetsResponse"},{"location":"user-guide/grpc/#shape","text":"Shape of an image width x height x channels Field Type Label Description dim1 int32 dim2 int32 dim3 int32","title":"Shape"},{"location":"user-guide/grpc/#byteorder","text":"ByteOrder for the conversion between data type and byte. Name Number Description LittleEndian 0 BigEndian 1","title":"ByteOrder"},{"location":"user-guide/grpc/#fileformat","text":"Available file formats Name Number Description Raw 0 raw bitmap GTiff 1 Top","title":"FileFormat"},{"location":"user-guide/grpc/#pblayoutsproto","text":"","title":"pb/layouts.proto"},{"location":"user-guide/grpc/#cell","text":"Define a cell of a grid Field Type Label Description id string Cell identifier crs string Coordinate reference system used in the cell coordinates LinearRing Geographic coordinates","title":"Cell"},{"location":"user-guide/grpc/#creategridrequest","text":"Create a new grid. Field Type Label Description grid Grid","title":"CreateGridRequest"},{"location":"user-guide/grpc/#creategridresponse","text":"","title":"CreateGridResponse"},{"location":"user-guide/grpc/#createlayoutrequest","text":"Create a new layout Return an error if the name already exists Field Type Label Description layout Layout","title":"CreateLayoutRequest"},{"location":"user-guide/grpc/#createlayoutresponse","text":"","title":"CreateLayoutResponse"},{"location":"user-guide/grpc/#deletegridrequest","text":"Delete a grid Field Type Label Description name string","title":"DeleteGridRequest"},{"location":"user-guide/grpc/#deletegridresponse","text":"","title":"DeleteGridResponse"},{"location":"user-guide/grpc/#deletelayoutrequest","text":"Delete a layout by name Field Type Label Description name string","title":"DeleteLayoutRequest"},{"location":"user-guide/grpc/#deletelayoutresponse","text":"","title":"DeleteLayoutResponse"},{"location":"user-guide/grpc/#findcontainerlayoutsrequest","text":"Find all the layouts used by the datasets on an AOI or a set of records It can be used to tile the AOI with an optimal layout. Field Type Label Description instance_id string records RecordIdList List of record ids filters RecordFiltersWithAOI Filters to select records","title":"FindContainerLayoutsRequest"},{"location":"user-guide/grpc/#findcontainerlayoutsresponse","text":"Stream the name of the layout and the associated containers Field Type Label Description layout_name string Name of the layout container_uris string repeated List of containers having the layout","title":"FindContainerLayoutsResponse"},{"location":"user-guide/grpc/#geotransform","text":"GDAL GeoTransform Field Type Label Description a double x offset b double x resolution c double d double y offset e double f double y resolution","title":"GeoTransform"},{"location":"user-guide/grpc/#grid","text":"Define a grid Field Type Label Description name string Unique name of the grid description string Description of the grid cells Cell repeated Cells of the grid","title":"Grid"},{"location":"user-guide/grpc/#layout","text":"Define a layout for consolidation. A layout is composed of an external and an internal layout. External layout is a grid that is used to cover any area with tiles. TODO Internal layout defines the internal structure of a dataset Interlacing_pattern defines how to interlace the [R]ecords, the [B]ands, the [Z]ooms level/overview and the [T]iles (geotiff blocks). The four levels of interlacing must be prioritized in the following way L1>L2>L3>L4 where each L is in [R, B, Z, T]. This order should be understood as: for each L1: for each L2: for each L3: for each L4: addBlock(L1, L2, L3, L4) In other words, all L4 for a given (L1, L2, L3) will be contiguous in memory. For example: - To optimize the access to geographical information of all the bands (such as in COG) : R>Z>T>B => For a given record, zoom level and block, all the bands will be contiguous. - To optimize the access to geographical information of one band at a time : B>R>Z>T => For a given band, record and zoom, all the blocks will be contiguous. - To optimize the access to timeseries of all the bands (such as in MUCOG): Z>T>R>B => For a given zoom level and block, all the records will be contiguous. Interlacing pattern can be specialized to only select a list or a range for each level (except Tile level). - By values: L=0,2,3 will only select the value 0, 2 and 3 of the level L. For example B=0,2,3 to select the corresponding band level. - By range: L=0:3 will only select the values from 0 to 3 (not included) of the level L. For example B=0:3 to select the three firsts bands. First and last values of the range can be omitted to define 0 or last element of the level. e.g B=2: means all the bands from the second. Z=0 is the full resolution, Z=1 is the overview with zoom factor 2, Z=2 is the zoom factor 4, and so on. To chain interlacing patterns, use \";\" separator. For example: - MUCOG optimizes access to timeseries for full resolution (Z=0), but geographic for overviews (Z=1:). Z=0>T>R>B;Z=1:>R>T>B - Same example, but the bands are separated: B>Z=0>T>R;B>Z=1:>R>T - To optimize access to geographic information of the three first bands together, but timeseries of the others: Z>T>R>B=0:3;B=3:>Z>R>T Field Type Label Description name string grid_flags string repeated External layout: Grid:Cell (CRS) grid_parameters Layout.GridParametersEntry repeated block_x_size int64 Internal layout: Cell, Tile block_y_size int64 max_records int64 overviews_min_size int64 Maximum width or height of the smallest overview level. 0: No overview, -1: default=256. interlacing_pattern string Define how to interlace the [R]ecords, the [B]ands, the [Z]ooms level/overview and the [T]iles (geotiff blocks).","title":"Layout"},{"location":"user-guide/grpc/#layoutgridparametersentry","text":"Field Type Label Description key string value string","title":"Layout.GridParametersEntry"},{"location":"user-guide/grpc/#listgridsrequest","text":"List all the grids given a name pattern (does not retrieve the cells) Field Type Label Description name_like string Name pattern (support * and ? for all or any characters and trailing (?i) for case-insensitiveness)","title":"ListGridsRequest"},{"location":"user-guide/grpc/#listgridsresponse","text":"Return a list of grids Field Type Label Description grids Grid repeated","title":"ListGridsResponse"},{"location":"user-guide/grpc/#listlayoutsrequest","text":"List all the layouts given a name pattern Field Type Label Description name_like string Name pattern (support * and ? for all or any characters and trailing (?i) for case-insensitiveness)","title":"ListLayoutsRequest"},{"location":"user-guide/grpc/#listlayoutsresponse","text":"Return a list of layouts Field Type Label Description layouts Layout repeated","title":"ListLayoutsResponse"},{"location":"user-guide/grpc/#size","text":"Define a size Field Type Label Description width int32 height int32","title":"Size"},{"location":"user-guide/grpc/#tile","text":"Define a rectangular tile in a given coordinate system (CRS). Field Type Label Description transform GeoTransform Transform to map from pixel coordinates to CRS size_px Size Size of the tile in pixel crs string Coordinate reference system","title":"Tile"},{"location":"user-guide/grpc/#tileaoirequest","text":"Tile an AOI, covering it with cells defined by a grid. In the future, it will be able to find the best tiling given the internal layout of datasets. Field Type Label Description aoi AOI layout_name string Name of an existing layout layout Layout User-defined layout","title":"TileAOIRequest"},{"location":"user-guide/grpc/#tileaoiresponse","text":"Return tiles, thousand by thousand. Field Type Label Description tiles Tile repeated Top","title":"TileAOIResponse"},{"location":"user-guide/grpc/#pboperationsproto","text":"","title":"pb/operations.proto"},{"location":"user-guide/grpc/#canceljobrequest","text":"Cancel a job (e.g. during consolidation) Field Type Label Description id string force_any_state bool Force cancel even when the job is not in a failed state or consolidation step (could corrupt the data)","title":"CancelJobRequest"},{"location":"user-guide/grpc/#canceljobresponse","text":"","title":"CancelJobResponse"},{"location":"user-guide/grpc/#cleanjobsrequest","text":"Clean terminated jobs Field Type Label Description name_like string Filter by name (support *, ? and (?i)-suffix for case-insensitivity) state string Filter by terminated state (DONE, FAILED)","title":"CleanJobsRequest"},{"location":"user-guide/grpc/#cleanjobsresponse","text":"Return the number of jobs that have been deleted Field Type Label Description count int32","title":"CleanJobsResponse"},{"location":"user-guide/grpc/#configconsolidationrequest","text":"Configure the parameters of the consolidation attached to the variable Field Type Label Description variable_id string consolidation_params ConsolidationParams","title":"ConfigConsolidationRequest"},{"location":"user-guide/grpc/#configconsolidationresponse","text":"","title":"ConfigConsolidationResponse"},{"location":"user-guide/grpc/#consolidaterequest","text":"Create and start a consolidation job given a list of records and an instance_id to be consolidated on a layout Optionnaly, the job can be done step by step, pausing and waiting for user action, with three levels: - 1: after each critical steps - 2: after each major steps - 3: after all steps Field Type Label Description job_name string instance_id string layout_name string execution_level ExecutionLevel Execution level of a job. A consolidation job cannot be executed synchronously collapse_on_record_id string [Optional] Collapse all records on this record (in this case only, original datasets are kept, data is duplicated) records RecordIdList At least one filters RecordFilters","title":"ConsolidateRequest"},{"location":"user-guide/grpc/#consolidateresponse","text":"Return the id of the job created Field Type Label Description job_id string","title":"ConsolidateResponse"},{"location":"user-guide/grpc/#consolidationparams","text":"Parameters of consolidation that are linked to a variable, to define: - how to resample the data during consolidation - how to store the data: - Compression - CreationParams (supported: see GDAL drivers: PHOTOMETRIC, COMPRESS, PREDICTOR, ZLEVEL, ZSTDLEVEL, MAX_Z_ERROR, JPEGTABLESMODE and with _OVERVIEW suffix if exists) Field Type Label Description dformat DataFormat dataformat of the data. See exponent for the mapping formula. exponent double 1: linear scaling (RealMax - RealMin) * pow( (Value - Min) / (Max - Min), Exponent) + RealMin create_overviews bool Deprecated. Use Layout.overviews_min_size instead resampling_alg Resampling Define how to resample the data during the consolidation (if a reprojection is needed or if the overviews are created) compression ConsolidationParams.Compression Define how the data is compressed at block level creation_params ConsolidationParams.CreationParamsEntry repeated map of params:value to configure the creation of the file. See Compression to list the supported params bands_interleave bool Deprecated. If the variable is multibands, define whether the bands are interleaved. Use Layout.interlacing_pattern instead storage_class StorageClass Define the storage class of the created file (support only GCS)","title":"ConsolidationParams"},{"location":"user-guide/grpc/#consolidationparamscreationparamsentry","text":"Field Type Label Description key string value string","title":"ConsolidationParams.CreationParamsEntry"},{"location":"user-guide/grpc/#container","text":"Define a container of datasets. Usually a container is a file containing one dataset. But after a consolidation or if the container has several bands, it can contain several datasets. Field Type Label Description uri string URI of the file managed bool True if the Geocube is responsible for the lifecycle of this file datasets Dataset repeated List of datasets of the container","title":"Container"},{"location":"user-guide/grpc/#continuejobrequest","text":"Proceed the next step of a step-by-step job Field Type Label Description id string","title":"ContinueJobRequest"},{"location":"user-guide/grpc/#continuejobresponse","text":"","title":"ContinueJobResponse"},{"location":"user-guide/grpc/#dataset","text":"Define a dataset. A dataset is the metadata to retrieve an image from a file. It is defined by a record and the instance of a variable. A dataset defines: - Which band(s) are indexed (usually all the bands, but it can be a subset) - How to map the value of its pixels to the dataformat of the variable. In more details: . the dataformat of the dataset (dformat.[no_data, min, max]) that describes the pixel of the image . the mapping from each pixel to the data format of the variable (variable.dformat). This mapping is defined as [MinOut, MaxOut, Exponent]. Field Type Label Description record_id string instance_id string container_subdir string bands int64 repeated dformat DataFormat Internal data format (DType can be Undefined) real_min_value double Real min value (dformat.min_value maps to real_min_value) real_max_value double Real max value (dformat.max_value maps to real_max_value) exponent double 1: linear scaling (RealMax - RealMin) * pow( (Value - Min) / (Max - Min), Exponent) + RealMin","title":"Dataset"},{"location":"user-guide/grpc/#deletedatasetsrequest","text":"Remove the datasets referenced by instances and records without any control The containers (if empty) are not deleted Field Type Label Description record_ids string repeated Instance id that references the datasets to be deleted instance_ids string repeated Record ids that reference the datasets to be deleted dataset_patterns string repeated Dataset file patterns (support * and ? for all or any characters and trailing (?i) for case-insensitiveness) (or empty to ignore) execution_level ExecutionLevel Execution level (see enum) job_name string Name of the job (if empty, a name will be generated)","title":"DeleteDatasetsRequest"},{"location":"user-guide/grpc/#deletedatasetsresponse","text":"Return the number of deleted datasets Field Type Label Description job Job","title":"DeleteDatasetsResponse"},{"location":"user-guide/grpc/#getconsolidationparamsrequest","text":"Retrieve the consolidation parameters attached to the given variable Field Type Label Description variable_id string","title":"GetConsolidationParamsRequest"},{"location":"user-guide/grpc/#getconsolidationparamsresponse","text":"Return consolidation parameters Field Type Label Description consolidation_params ConsolidationParams","title":"GetConsolidationParamsResponse"},{"location":"user-guide/grpc/#getcontainersrequest","text":"Request info on containers Field Type Label Description uris string repeated List of container uris","title":"GetContainersRequest"},{"location":"user-guide/grpc/#getcontainersresponse","text":"Field Type Label Description containers Container repeated","title":"GetContainersResponse"},{"location":"user-guide/grpc/#getjobrequest","text":"Retrieve a job given its id Field Type Label Description id string log_page int32 log_limit int32","title":"GetJobRequest"},{"location":"user-guide/grpc/#getjobresponse","text":"Return a job with the requested id Field Type Label Description job Job","title":"GetJobResponse"},{"location":"user-guide/grpc/#indexdatasetsrequest","text":"Request to index all the datasets of a container Field Type Label Description container Container TODO Index several containers: repeated ?","title":"IndexDatasetsRequest"},{"location":"user-guide/grpc/#indexdatasetsresponse","text":"","title":"IndexDatasetsResponse"},{"location":"user-guide/grpc/#job","text":"Job to modify datasets (consolidation, deletion, ingestion...) Some lifecycle operations are required to be done cautiously, in order to garantee the integrity of the database. Such operations are defined by a job and are done asynchronously. A job is a state-machine that can be rollbacked anytime during the operation until it ends. Field Type Label Description id string Id of the job name string Name of the job (must be unique) type string Type of the job (consolidation, deletion...) state string Current state of the state machine creation_time google.protobuf.Timestamp Time of creation of the job last_update_time google.protobuf.Timestamp Time of the last update logs string repeated Job logs: if logs are too big to fit in a grpc response, logs will only be a subset (by default, the latest) active_tasks int32 If the job is divided into sub tasks, number of pending tasks failed_tasks int32 If the job is divided into sub tasks, number of failed tasks execution_level ExecutionLevel Execution level of a job (see ExecutionLevel) waiting bool If true, the job is waiting for user to continue","title":"Job"},{"location":"user-guide/grpc/#listjobsrequest","text":"List jobs given a name pattern Field Type Label Description name_like string page int32 limit int32","title":"ListJobsRequest"},{"location":"user-guide/grpc/#listjobsresponse","text":"Return a list of the job whose name matchs the pattern Field Type Label Description jobs Job repeated","title":"ListJobsResponse"},{"location":"user-guide/grpc/#retryjobrequest","text":"Retry a job that failed or is stuck (e.g. during consolidation) Field Type Label Description id string force_any_state bool Force retry even when the job is not in a failed state (could corrupt the data)","title":"RetryJobRequest"},{"location":"user-guide/grpc/#retryjobresponse","text":"","title":"RetryJobResponse"},{"location":"user-guide/grpc/#consolidationparamscompression","text":"Name Number Description NO 0 LOSSLESS 1 LOSSY 2 CUSTOM 3 configured by creation_params","title":"ConsolidationParams.Compression"},{"location":"user-guide/grpc/#executionlevel","text":"Execution level of a job Name Number Description ExecutionSynchronous 0 Job is done synchronously ExecutionAsynchronous 1 Job is done asynchronously, but without any pause StepByStepCritical 2 Job is done asynchronously, step-by-step, pausing at every critical steps StepByStepMajor 3 Job is done asynchronously, step-by-step, pausing at every major steps StepByStepAll 4 Job is done asynchronously, step-by-step, pausing at every steps","title":"ExecutionLevel"},{"location":"user-guide/grpc/#storageclass","text":"Storage class of a container. Depends on the storage Name Number Description STANDARD 0 INFREQUENT 1 ARCHIVE 2 DEEPARCHIVE 3 Top","title":"StorageClass"},{"location":"user-guide/grpc/#pbdatasetmetaproto","text":"","title":"pb/datasetMeta.proto"},{"location":"user-guide/grpc/#datasetmeta","text":"DatasetMeta contains all the metadata on files and fileformats to download and generate a slice of a cube Field Type Label Description internalsMeta InternalMeta repeated Information on the images composing the slice","title":"DatasetMeta"},{"location":"user-guide/grpc/#internalmeta","text":"InternalMeta contains all the metadata on a file to download it and to map its internal values to the external range. Field Type Label Description container_uri string URI of the file storing the data container_subdir string Subdir of the file storing the data bands int64 repeated Subbands of the file requested dformat DataFormat Internal dataformat of the data range_min double dformat.RangeMin will be mapped to this value range_max double dformat.RangeMax will be mapped to this value exponent double Exponent used to map the value from dformat to [RangeMin, RangeMax] Top","title":"InternalMeta"},{"location":"user-guide/grpc/#pbversionproto","text":"","title":"pb/version.proto"},{"location":"user-guide/grpc/#getversionrequest","text":"Request the version of the Geocube","title":"GetVersionRequest"},{"location":"user-guide/grpc/#getversionresponse","text":"Return the version of the Geocube Field Type Label Description Version string","title":"GetVersionResponse"},{"location":"user-guide/grpc/#scalar-value-types","text":".proto Type Notes C++ Java Python Go C# PHP Ruby double double double float float64 double float Float float float float float float32 float float Float int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int32 int integer Bignum or Fixnum (as required) int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long int64 long integer/string Bignum uint32 Uses variable-length encoding. uint32 int int/long uint32 uint integer Bignum or Fixnum (as required) uint64 Uses variable-length encoding. uint64 long int/long uint64 ulong integer/string Bignum or Fixnum (as required) sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int int32 int integer Bignum or Fixnum (as required) sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long int64 long integer/string Bignum fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int uint32 uint integer Bignum or Fixnum (as required) fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long uint64 ulong integer/string Bignum sfixed32 Always four bytes. int32 int int int32 int integer Bignum or Fixnum (as required) sfixed64 Always eight bytes. int64 long int/long int64 long integer/string Bignum bool bool boolean boolean bool bool boolean TrueClass/FalseClass string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode string string string String (UTF-8) bytes May contain any arbitrary sequence of bytes. string ByteString str []byte ByteString string String (ASCII-8BIT)","title":"Scalar Value Types"},{"location":"user-guide/indexation/","text":"Indexation Description Referencing a new image in the Geocube is a process called indexation . The Geocube does not store images (it's the purpose of the object storage), but their URI and some metadata on them. The indexation ( IndexDatasets() ) defines : how the geocube will read an image: Which layer of the image ? (in case of multiple layers are stored in the image) Which band(s) ? How to interpret pixel values ? (see dataformat ) how the client will find the dataset: geographically, temporally and semantically, using the record by the type of data, using the instance and the variable NB: Geocube must have the rights to access the images. During the indexation, the file containing the image is not modified in any way. It's only the way the geocube will interpret the image that is defined. All the information provided during indexation are for the interpretation of the image. There is no (or limited) check during indexation that the user provides the right values. Storage optimisation In order to optimize the storage of a large volume of data, it can be decided to reduce the size of the data type (for example from float32 to int16) and/or scale the data. So, the dataformat of the dataset can be different from the variable in some ways: For compression purpose : the data type is smaller. For example data is encoded in byte [0, 255] that maps to float [0, 1] in the variable. To optimize accuracy : the range of values is smaller than the one of the variable. Two examples : Given a variable between -1 and 1, the data in a given image is known to be in [0, 1] instead of [-1, 1]. To optimize accuracy, the data is encoded between 0 and 255 and min/max_out are [0, 1]. Given a variable between 0 and 100, 90% of the data is known to be between 0 and 10. To optimize accuracy, the data is encoded between 0 and 255, using a non-linear mapping to [0, 100] using an exponent=2. Data is scaled according to the non-linear scaling in the diagram :","title":"Indexation"},{"location":"user-guide/indexation/#indexation","text":"","title":"Indexation"},{"location":"user-guide/indexation/#description","text":"Referencing a new image in the Geocube is a process called indexation . The Geocube does not store images (it's the purpose of the object storage), but their URI and some metadata on them. The indexation ( IndexDatasets() ) defines : how the geocube will read an image: Which layer of the image ? (in case of multiple layers are stored in the image) Which band(s) ? How to interpret pixel values ? (see dataformat ) how the client will find the dataset: geographically, temporally and semantically, using the record by the type of data, using the instance and the variable NB: Geocube must have the rights to access the images. During the indexation, the file containing the image is not modified in any way. It's only the way the geocube will interpret the image that is defined. All the information provided during indexation are for the interpretation of the image. There is no (or limited) check during indexation that the user provides the right values.","title":"Description"},{"location":"user-guide/indexation/#storage-optimisation","text":"In order to optimize the storage of a large volume of data, it can be decided to reduce the size of the data type (for example from float32 to int16) and/or scale the data. So, the dataformat of the dataset can be different from the variable in some ways: For compression purpose : the data type is smaller. For example data is encoded in byte [0, 255] that maps to float [0, 1] in the variable. To optimize accuracy : the range of values is smaller than the one of the variable. Two examples : Given a variable between -1 and 1, the data in a given image is known to be in [0, 1] instead of [-1, 1]. To optimize accuracy, the data is encoded between 0 and 255 and min/max_out are [0, 1]. Given a variable between 0 and 100, 90% of the data is known to be between 0 and 10. To optimize accuracy, the data is encoded between 0 and 255, using a non-linear mapping to [0, 100] using an exponent=2. Data is scaled according to the non-linear scaling in the diagram :","title":"Storage optimisation"},{"location":"user-guide/scaleup/","text":"Scaling-up The Geocube is designed to handle very large processing flow. Tile an AOI Usually, it's not possible to process all images of a large area in a row. The area has to be divided into smaller tiles for the memory to fit into the processing machine. The Geocube provides a convenient way to tile an aoi, thanks to the TileAOI() . It takes an AOI and a layout and streams a list of tiles. Using the downloader With tiling, the process can be parallelized on processing workers, but a bottleneck can occur when it's time to get cubes of data from the Geocube Server. To prevent from that, either more Geocube Server machines must be provisioned or the processing workers can use a local or dockerized downloader service . For the latter case, the local machines should have an efficient access to the object storages (they should be on the same network) or the datasets should be consolidated with the same layout to limit the volume of transfered data. Using Dask Dask is a flexible open-source Python library for parallel computing. The geocube-client-python provides several examples and a docker to use Dask with the Geocube and the Downloader services.","title":"Scaling-up"},{"location":"user-guide/scaleup/#scaling-up","text":"The Geocube is designed to handle very large processing flow.","title":"Scaling-up"},{"location":"user-guide/scaleup/#tile-an-aoi","text":"Usually, it's not possible to process all images of a large area in a row. The area has to be divided into smaller tiles for the memory to fit into the processing machine. The Geocube provides a convenient way to tile an aoi, thanks to the TileAOI() . It takes an AOI and a layout and streams a list of tiles.","title":"Tile an AOI"},{"location":"user-guide/scaleup/#using-the-downloader","text":"With tiling, the process can be parallelized on processing workers, but a bottleneck can occur when it's time to get cubes of data from the Geocube Server. To prevent from that, either more Geocube Server machines must be provisioned or the processing workers can use a local or dockerized downloader service . For the latter case, the local machines should have an efficient access to the object storages (they should be on the same network) or the datasets should be consolidated with the same layout to limit the volume of transfered data.","title":"Using the downloader"},{"location":"user-guide/scaleup/#using-dask","text":"Dask is a flexible open-source Python library for parallel computing. The geocube-client-python provides several examples and a docker to use Dask with the Geocube and the Downloader services.","title":"Using Dask"},{"location":"user-guide/tutorials/","text":"Tutorials Five tutorials explain the basics of Geocube using the Geocube Python Client Library : Data Indexation Jupyter notebook Data Access Jupyter notebook Data Consolidation Jupyter notebook SDK-1 Jupyter notebook SDK-2 Jupyter notebook It is recommended to follow these tutorials that are stand-alone (at least the first four) and can be executed in a local machine, using docker-compose for example.","title":"Tutorials"},{"location":"user-guide/tutorials/#tutorials","text":"Five tutorials explain the basics of Geocube using the Geocube Python Client Library : Data Indexation Jupyter notebook Data Access Jupyter notebook Data Consolidation Jupyter notebook SDK-1 Jupyter notebook SDK-2 Jupyter notebook It is recommended to follow these tutorials that are stand-alone (at least the first four) and can be executed in a local machine, using docker-compose for example.","title":"Tutorials"}]}